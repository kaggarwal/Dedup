A CLASSICAL

INTRODUCTION TO

CRYPTOGRAPHY

Applications for

Communications Security

A CLASSICAL

INTRODUCTION TO

CRYPTOGRAPHY

Applications for

Communications Security

by

Serge Vaudenay

Swiss Federal Institute of Technologies (EPFL)

Serge Vaudenay 
Ch. de Riant-Mont 4 
CH-1023 Crissier 
Switzerland 

Library of Congress Cataloging-in-Publication Data 

A C.I.P. Catalogue record for this book is available 
from the Library of Congress. 

A CLASSICAL INTRODUCTION TO MODERN CRYPTOGRAPHY 
Applications for Communications Security 
by Serge Vaudenay 
Swiss Fédéralel Institute of Technologies (EPFL) 

ISBN-10:  0-387-25464-1   
ISBN-13:   978-0-387-25464-7 

e-ISBN-10:  0-387-25880-9 
e-ISBN-13:  978-0-387-25880-5 

Printed on acid-free paper. 

(cid:164) 2006 Springer Science+Business Media, Inc. 
All rights reserved. This work may not be translated or copied in whole or 
in  part  without  the  written  permission  of  the  publisher  (Springer 
Science+Business Media, Inc., 233 Spring Street, New York, NY 10013, 
USA),  except  for  brief  excerpts  in  connection  with  reviews  or  scholarly 
analysis.  Use  in  connection  with  any  form  of  information  storage  and 
retrieval,  electronic  adaptation,  computer  software,  or  by  similar  or 
dissimilar methodology now know or hereafter developed is forbidden. 
The use in this publication of trade names, trademarks, service marks and 
similar terms, even if the are not identified as such, is not to be taken as 
an  expression  of  opinion  as  to  whether  or  not  they  are  subject  to 
proprietary rights. 

Printed in the United States of America.   

 

9  8  7  6  5  4  3  2  1                   SPIN  11357582, 11426141 

springeronline.com

To Christine and Emilien

Contents

Preamble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

xv

1 Prehistory of Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1 Foundations of Conventional Cryptography . . . . . . . . . . . . . . . . . . . .
1.1.1 The Origins of Cryptography . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.2 Key Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.3 Transpositions, Substitutions, and Secret Keys . . . . . . . . . . . .
1.1.4 Vernam Cipher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.5 Enigma: Toward Industrial Cryptography . . . . . . . . . . . . . . . .
1.2 Roots of Modern Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.1 Cryptographic Problems: The Fundamental Trilogy . . . . . . . .
1.2.2 Assumptions of Modern Cryptography . . . . . . . . . . . . . . . . . .
1.2.3 Adversarial Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.4 Cryptography from Various Perspectives . . . . . . . . . . . . . . . .
1.2.5 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 (cid:1)The Shannon Theory of Secrecy . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.1 (cid:1)Secrecy of Communication . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.2 (cid:1)Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.3 (cid:1)Perfect Secrecy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.4 (cid:1)Product Ciphers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Conventional Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1 The Data Encryption Standard (DES) . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 DES Modes of Operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Electronic Code Book (ECB) . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Cipher Block Chaining (CBC) . . . . . . . . . . . . . . . . . . . . . . . .
2.2.3 Output Feedback (OFB) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.4 Cipher Feedback (CFB) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.5 Counter Mode (CTR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Multiple Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 Double Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.2 Triple Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 An Application of DES: UNIX Passwords . . . . . . . . . . . . . . . . . . . . .

1
1
1
2
4
7
8
10
10
11
12
13
15
15
15
17
18
19
19

21
22
25
25
26
27
29
30
30
30
31
31

viii

Contents

2.5 Classical Cipher Skeletons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.1 Feistel Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.2 Lai–Massey Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.3 Substitution–Permutation Network . . . . . . . . . . . . . . . . . . . . .
2.6 Other Block Cipher Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6.1 (cid:1)FOX: A Lai–Massey Scheme . . . . . . . . . . . . . . . . . . . . . . . .
2.6.2 (cid:1)CS-CIPHER: A Substitution–Permutation Network . . . . . . .
2.7 The Advanced Encryption Standard (AES) . . . . . . . . . . . . . . . . . . . .
2.8 Stream Ciphers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8.1 Stream Ciphers versus Block Ciphers . . . . . . . . . . . . . . . . . . .
2.8.2 RC4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8.3 A5/1: GSM Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8.4 E0: Bluetooth Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9 Brute Force Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9.1 Exhaustive Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9.2 Dictionary Attack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9.3 Codebook Attack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9.4 (cid:1)Time–Memory Tradeoffs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9.5 Meet-in-the-Middle Attack . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Dedicated Conventional Cryptographic Primitives . . . . . . . . . . . . . .
3.1 Cryptographic Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.2 Threat Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.3 From Compression to Hashing . . . . . . . . . . . . . . . . . . . . . . . .
3.1.4 Example of MD5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.5 Examples of SHA and SHA-1 . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 The Birthday Paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 (cid:1)A Dedicated Attack on MD4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Message Authentication Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.1 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.2 Threat Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.3 MAC from Block Ciphers: CBC-MAC . . . . . . . . . . . . . . . . . .
3.4.4 (cid:1)Analysis of CBC-MAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.5 (cid:1)MAC from Stream Ciphers . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.6 MAC from Hash Functions: HMAC . . . . . . . . . . . . . . . . . . . .
3.4.7 An Authenticated Mode of Operation . . . . . . . . . . . . . . . . . . .
3.5 Cryptographic Pseudorandom Generators . . . . . . . . . . . . . . . . . . . . .
3.5.1 Usage and Threat Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.2 (cid:1)Congruential Pseudorandom Generator . . . . . . . . . . . . . . . .
3.5.3 Practical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32
32
33
36
37
37
40
42
46
46
46
48
50
51
52
53
54
54
59
60

63
63
63
64
65
66
67
70
74
78
78
79
80
82
86
88
90
92
92
92
93
95

Contents

ix

4 (cid:1)Conventional Security Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
4.1 (cid:1)Differential Cryptanalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
4.2 (cid:1)Linear Cryptanalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4.3 (cid:1)Classical Security Strengthening . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.3.1 (cid:1)Nonlinearities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.3.2 (cid:1)Characteristics and Markov Ciphers . . . . . . . . . . . . . . . . . . . 112
4.3.3 (cid:1)Theoretical Differential and Linear Cryptanalysis . . . . . . . . 114
4.3.4 (cid:1)Ad hoc Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.4 (cid:1)Modern Security Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
4.4.1 (cid:1)Distinguishability Security Model . . . . . . . . . . . . . . . . . . . . . 123
4.4.2 (cid:1)The Luby–Rackoff Result . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.4.3 (cid:1)Decorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
4.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

5 Security Protocols with Conventional Cryptography . . . . . . . . . . . . . 135
5.1 Password Access Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
5.1.1 UNIX Passwords . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.1.2 Basic Access Control in HTTP . . . . . . . . . . . . . . . . . . . . . . . . 136
5.1.3 PAP Access Control in PPP . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.2 Challenge–Response Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.2.1 Digest Access Control in HTTP . . . . . . . . . . . . . . . . . . . . . . . 138
5.2.2 CHAP Access Control in PPP . . . . . . . . . . . . . . . . . . . . . . . . . 140
5.3 One-Time Password . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
5.3.1 Lamport Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
5.3.2 S/Key and OTP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
5.4 Key Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
5.4.1 The Needham–Schroeder Authentication Protocol . . . . . . . . . 142
5.4.2 Kerberos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.4.3 (cid:1)Merkle Puzzles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.5 (cid:1)Authentication Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.5.1 (cid:1)Merkle Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.5.2 (cid:1)Timestamps and Notary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.6 Wireless Communication: Two Case Studies . . . . . . . . . . . . . . . . . . . 148
5.6.1 The GSM Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
5.6.2 The Bluetooth Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
5.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153

6 Algorithmic Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.1 Basic Group Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.1.1 Basic Set Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.1.2 Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
6.1.3 Generating a Group, Comparing Groups . . . . . . . . . . . . . . . . 158
6.1.4 Building New Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
6.1.5 Fundamentals on Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

x

Contents

6.2 The Ring Zn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.2.1 Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.2.2 Deﬁnition of Zn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
6.2.3 Additions, Multiplications, Inversion . . . . . . . . . . . . . . . . . . . 162
∗
6.2.4 The Multiplicative Group Z
n . . . . . . . . . . . . . . . . . . . . . . . . . 166
6.2.5 Exponentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.2.6 Zmn: The Chinese Remainder Theorem . . . . . . . . . . . . . . . . . 167
6.3 The Finite Field Zp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
6.3.1 Basic Properties of Z p . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
6.3.2 (cid:1)Quadratic Residues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
6.4 Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
6.5 (cid:1)Elliptic Curves over Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
6.5.1 (cid:1)Characteristic p > 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
6.5.2 (cid:1)Characteristic Two . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
6.5.3 (cid:1)General Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
6.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178

7 Algorithmic Number Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
7.1 Primality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
7.1.1 Fermat Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
7.1.2 (cid:1) Carmichael Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
7.1.3 (cid:1)Solovay–Strassen Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
7.1.4 Miller-Rabin Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
7.1.5 (cid:1)Analysis of the Miller-Rabin Test . . . . . . . . . . . . . . . . . . . . . 189
7.1.6 Prime Number Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
7.2 (cid:1)Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
7.2.1 (cid:1)Pollard Rho Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
7.2.2 (cid:1)Pollard p − 1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
7.2.3 (cid:1)The Elliptic Curves Method (ECM) . . . . . . . . . . . . . . . . . . . 194
7.2.4 (cid:1)Fermat Factorization and Factor Bases . . . . . . . . . . . . . . . . . 196
7.2.5 (cid:1)The Quadratic Sieve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
7.2.6 (cid:1)Factorization Nowadays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
7.2.7 (cid:1)Factorization Tomorrow . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
7.3 Computing Orders in Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
7.3.1 Finding the Group Exponent . . . . . . . . . . . . . . . . . . . . . . . . . . 201
7.3.2 Computing Element Orders in Groups . . . . . . . . . . . . . . . . . . 202
7.4 (cid:1)Discrete Logarithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
7.4.1 (cid:1)Pollard Rho Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
7.4.2 (cid:1)Shanks Baby Steps – Giant Steps Algorithm . . . . . . . . . . . . 204
7.4.3 (cid:1)Pohlig–Hellman Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 205
7.4.4 (cid:1)Factor Base and Index Calculus Algorithm . . . . . . . . . . . . . . 210
7.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

Contents

xi

8 (cid:1)Elements of Complexity Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 215
8.1 (cid:1)Formal Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
8.1.1 (cid:1)Formal Languages and Regular Expressions . . . . . . . . . . . . 215
8.1.2 (cid:1)Finite Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
8.1.3 (cid:1)Beyond Finite Automata Capabilities . . . . . . . . . . . . . . . . . . 218
8.1.4 (cid:1)Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
8.2 (cid:1)Ability Frontiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
8.2.1 (cid:1)Standard Computational Models . . . . . . . . . . . . . . . . . . . . . . 220
8.2.2 (cid:1)Beyond Computability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
8.2.3 (cid:1)Decisional Problems and Decidability . . . . . . . . . . . . . . . . . 221
8.3 (cid:1)Complexity Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
8.3.1 (cid:1)Asymptotic Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . 222
8.3.2 (cid:1)Complexity Classes P, NP, co-NP . . . . . . . . . . . . . . . . . . . . . 223
8.3.3 (cid:1)Intractability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
8.3.4 (cid:1)Oracles and Turing Reduction . . . . . . . . . . . . . . . . . . . . . . . . 225
8.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

9 Public-Key Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
9.1 Difﬁe–Hellman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
9.1.1 Public-Key Cryptosystems . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
9.1.2 The Difﬁe–Hellman Key Agreement Protocol . . . . . . . . . . . . 231
9.2 (cid:1)Experiment with NP-Completeness . . . . . . . . . . . . . . . . . . . . . . . . . 234
9.2.1 (cid:1)Knapsack Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
9.2.2 (cid:1)The Merkle–Hellman Cryptosystem . . . . . . . . . . . . . . . . . . . 235
9.3 Rivest–Shamir–Adleman (RSA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
9.3.1 Plain RSA Cryptosystem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
9.3.2 RSA Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
9.3.3 Attacks on Broadcast Encryption with Low Exponent . . . . . . 241
9.3.4 Attacks on Low Exponent . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
9.3.5 Side Channel Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
9.3.6 (cid:1)Bit Security of RSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
9.3.7 (cid:1)Back to the Encryption Security Assumptions . . . . . . . . . . . 244
9.3.8 RSA–OAEP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
9.4 ElGamal Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
9.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250

10 Digital Signature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
10.1 Digital Signature Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
10.2 RSA Signature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
10.2.1 From Public-Key Cryptosystem to Digital Signature . . . . . . 255
10.2.2 On the Plain RSA Signature . . . . . . . . . . . . . . . . . . . . . . . . . 256

xii

Contents

10.2.3 ISO/IEC 9796 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
10.2.4 (cid:1)Attack on the ISO/IEC 9796 Signature Scheme . . . . . . . . . 259
10.2.5 PKCS#1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
10.3 ElGamal Signature Family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
10.3.1 ElGamal Signature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
10.3.2 (cid:1)The Bleichenbacher Attack against the ElGamal Signature 262
10.3.3 Schnorr Signature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
10.3.4 The Digital Signature Standard (DSS) . . . . . . . . . . . . . . . . . 264
10.3.5 (cid:1)ECDSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
10.3.6 Pointcheval–Vaudenay Signature . . . . . . . . . . . . . . . . . . . . . 266
10.4 (cid:1)Toward Provable Security for Digital Signatures . . . . . . . . . . . . . . . 266
10.4.1 (cid:1)From Interactive Proofs to Signatures . . . . . . . . . . . . . . . . . 266
10.4.2 (cid:1)Security in the Random Oracle Model . . . . . . . . . . . . . . . . 270
10.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274

11 (cid:1)Cryptographic Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
11.1 (cid:1)Zero-Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
11.1.1 (cid:1)Notion of Zero-Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . 277
11.1.2 (cid:1)The Basic Fiat–Shamir Protocol . . . . . . . . . . . . . . . . . . . . . 278
11.1.3 (cid:1)The Feige–Fiat–Shamir Protocol . . . . . . . . . . . . . . . . . . . . . 280
11.2 (cid:1)Secret Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
11.2.1 (cid:1)The Shamir Threshold Scheme . . . . . . . . . . . . . . . . . . . . . . 283
11.2.2 (cid:1)Perfect Secret Sharing Schemes . . . . . . . . . . . . . . . . . . . . . 284
11.2.3 (cid:1)Access Structure of Perfect Secret Sharing Schemes . . . . . 285
11.2.4 (cid:1)The Benaloh–Leichter Secret Sharing Scheme . . . . . . . . . . 286
11.3 (cid:1)Special Purpose Digital Signatures . . . . . . . . . . . . . . . . . . . . . . . . . . 287
11.3.1 (cid:1)Undeniable Signature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
11.3.2 (cid:1)Other Special Purpose Digital Signatures . . . . . . . . . . . . . . 291
11.4 (cid:1)Other Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
11.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293

12 From Cryptography to Communication Security . . . . . . . . . . . . . . . 295
12.1 Certiﬁcates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
12.2 SSH: Secure Shell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
12.2.1 Principles of SSH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
12.2.2 SSH2 Key Exchange and Authentication . . . . . . . . . . . . . . . 299
12.3 SSL: Secure Socket Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
12.3.1 Handshake . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
12.3.2 Cipher Suites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
12.3.3 Record Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
12.3.4 Stream Cipher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
12.3.5 Block Cipher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
12.3.6 Master Key Exchange . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
12.3.7 Key Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306

Contents

xiii

12.4 PGP: Pretty Good Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
12.4.1 Security for Individuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
12.4.2 Public-Key Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
12.4.3 Security Weaknesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
12.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311

Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315

Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329

Preamble

Cryptography is the science of information and communication security. It entered in
mass product markets quite recently and every citizen from developed countries uses
it daily. It is used for authentication and encryption (bank cards, wireless telephone,
e-commerce, pay-TV), access control (car lock systems, ski lifts), payment (prepaid
telephone cards, e-cash), and may become the fundamental instrument of democracy
with the advent of e-voting systems. To master cryptographic tools becomes a require-
ment for most engineers. The present book aims at presenting fundamentals on modern
cryptography.

Cryptography is a puzzling kaleidoscope where we have to face and use malice at
the same time, to rationalize irrational behaviors, to prove unprovability, and to twist
mathematics to make it ﬁt applications. We easily switch from theoretical questions
on elliptic curves to standardization problems such as how to represent a number
in a communication protocol. One needs to juggle with assembly code on chips and
distributed systems over the Internet. We are concerned about commercial aspects such
as patents or mass products, as well as communication aspects since cryptography is
more and more exposed in media, novels, and movies. Since I ﬁrst started to work
on cryptography, I always summarized how fun it was by the slogan La crypto c’est
rigolo!1 My wish is that the reader appreciates the malicious beauty of this puzzling
science throughout this textbook.

When writing the preamble of textbooks, authors usually try to justify themselves
for introducing yet-another-book. Actually, I had to set up a new course on cryptography.
When looking around in the library, I realized that I had to write a new one myself. My
motivation came from the following observations.

Education problem. I realized that there is a substantial gap between state-of-the-
art research on cryptography and standard applications. Most of daily used
standards are quite old (more than 5 years indeed!), new ﬂaws are regularly
discovered. At the time I am wrapping up the present textbook, the MD5 hash
function (a 14-year-old standard which is used in most Internet protocols) has
been broken, the stronger SHA-1 hash function (its small 10-year-old brother
which is basically used in every other protocol) is in a very bad shape. Stan-
dards are being updated or completely replaced and have very short life span.
Weaknesses are discovered in well-established standards such as PKCS#1v1.5
(12 years old), TLS 1.0 (6 years old), or IPSEC (7 years old). Furthermore,
they are almost never built on strong notions of security. Even very young
standards such as Bluetooth 1.2 (2 years old) rely on ad hoc and pragmatic
security instead of protocols with (pretty well) proven security. Although we

1 Crypto is fun!

xvi

Preamble

know some strongly secure encryption schemes such as the Cramer-Shoup
cryptosystem (7 years old), we still daily use PKCS#1v1.5. Although we
know some efﬁcient and provably secure message authentication codes in the
perfect cipher model like OMAC (2 years old), we still use HMAC (8 years
old) with heuristic security.

It is virtually impossible to teach standards which will be used in 5 years
from now. Even techniques which look like the Graal of cryptographers at this
time may become completely obsolete in a short time frame. Nevertheless,
engineers who will be able to understand future standards and to assess the
security impacts of cryptographic products must be trained. For this, I tried
to present cryptography like an evolutionary process. I tried to suggest more
questions than answers and to provide a critical taste. If the reader can ask the
right questions after reading this book, it will have reached its goal.

A place for conventional cryptography. Most textbooks concentrate on public-key
cryptography. It is actually conceptually more elegant and more adapted to for-
mal treatments since the mathematics behind (number theory) are well studied
and understood. Conventional cryptography uses more exotic math, . . . and ac-
tually more mess than math. It is actually quite hard to construct a mess theory
in conventional cryptography since the mess is assembled in a more artistic
than scientiﬁc way by experts in this ﬁeld. Paradoxically, most mass products
do not use public-key cryptography at all. Wireless communications such as
GSM or Bluetooth use no prime numbers and actually require no such tech-
niques. Once a secret key is established, applications such as SSL/TLS, IPSEC,
or PGP no longer use public-key cryptography. So it seems that most textbooks
favor the part of cryptography that has tiny representativity in practice. The
present textbook is more balanced between conventional cryptography and
public-key cryptography.

The two sides of cryptanalysis. Most authors present cryptanalysis as a kind of
cryptographic hooliganism: cryptanalysis is for breaking nice cryptographic
toys by any means. There is actually no rules for outlaws and one can even
try to break a cryptographic scheme with dirty math and unchecked assump-
tions as long as experiment validates the attack. Actually, because of the gap
between research and practice, most of the daily used standards are not built
on solid foundations and are more likely to be broken by experts. However,
cryptanalysis has a constructive side on which the expert tries to prove the
robustness of cryptography. Unfortunately, an absolute proof is often impossi-
ble, and one must satisfy from “adversary models” and “proofs-by-reduction.”
Adversary modelism and proof reductionism are the two main paradigms of
security validation at this time. This book tries to present a rigorous approach
on cryptanalysis.

Cryptography for communication security. Most textbooks hide the communica-
tion models and objectives. Following Shannon, cryptography is a way to

Preamble

xvii

establish secure communications over insecure channels by using an extra
hypothesis: a channel which already provides security. This channel is basi-
cally used to set up a conﬁdential and authenticated symmetric key. Once set
up, symmetric keys can be used to communicate securely (i.e., conﬁdentially
and in an authenticated way) over insecure channels by using conventional
cryptography. Merkle reduces the extra channel hypothesis to the problem of
communicating in an authenticated way: the extra channel no longer requires
to protect conﬁdentiality but only authentication. The reduction is further im-
proved by using the Difﬁe–Hellman protocol. At last, following the Rivest–
Shamir–Adleman (RSA) scheme, asymmetric keys can be set up once for all
using this channel so that the extra channel is no longer required. In this book I
tried to present secure communication as the ultimate goal of cryptography and
to emphasize the usage of multiple channels with speciﬁc security properties.

This textbook presents cryptography in a classical way by following a chronological or-
der. As a consequence, recent (strong) notions of security such as resistance to adaptive
chosen ciphertext attacks (or “nonmalleability”) are lately presented.

The science of cryptographic analysis requires an amazingly wide spectrum of
mathematical background. One can be surprised to see that we extensively use com-
plexity theory or number theory. I have tried to provide reminders and abstracts of the
required knowledge. I have to apologize for those who will think that there is a too strong
“Bourbaki taste” formalism here. Formalism is indeed useful. Sometimes, inability to
provide a formal description is due to misconception, so having a formal picture brings
some kind of conﬁdence: it is a necessary (but unfortunately not sufﬁcient) condition
for catching the right concepts.

Using this Document

This document is the transcript of a course given at the Communication Systems Di-
vision of EPFL since 2001. In that course I tried to make a distinction between the
material that students must know from the material covering more advanced topics in
cryptography. The lectures are thus split into basic and advanced courses. Titles of
sections which belong to the advanced material are preceded by the (cid:1) symbol. At a ﬁrst
reading, the reader can thus focus on nonadvanced parts. Every chapter begins with
a synthetic table of contents and ends with a few exercises. References are put in the
Bibliography at the end of this document.

People who look for information about standards for implementation purposes may
ﬁnd some here. They are however strongly encouraged to refer to the source documents
of the standards.

A companion exercise book with solutions is also available. In addition, some

information (e.g. errata) is available on the Internet on.

http://www.vaudenay.ch/crypto/

xviii

Preamble

As this manuscript may contain errors, readers who ﬁnd some are encouraged to

check the errata list and submit a report if necessary.

Additional Readings

Besides the present textbook, we recommend the following books. Further readings are suggested
at the end of this book.

B. Schneier. Applied Cryptography. John Wiley & Sons, NY, 1996. (Ref. [159].)

French version: Cryptographie Appliqu´ee. Vuibert, Paris, 1996.
This is an easy survey book. Since most of the technical details are omitted, this must
not be used as a reference for technical concepts. It is also becoming a little outdated.

N. Ferguson, B. Schneier. Practical Cryptography. John Wiley & Sons, NY, 2003. (Ref. [67].)
This book may be used as a “cryptography for dummies”: it may be a good starting
point (or a deﬁnite ending point) for dummies.

D.R. Stinson. Cryptography, Theory and Practice (2nd Edition). CRC, NY, 2002. (Ref. [177].)

French version: Cryptographie, Th´eorie et Pratique. Vuibert, Paris, 2003.
An excellent introduction to cryptography (lecture notes of Douglas Stinson at the
University of Waterloo).

N. Koblitz. A Course in Number Theory and Cryptography. Springer-Verlag, NY, 1994.

(Ref. [102].)
A nice introduction to algorithmic number theory for cryptography.

V. Shoup. A Computational Introduction to Number Theory and Algebra. Online textbook, 2004.

(Ref. [170].)
An outstanding ABC on number theory for beginner cryptographers. It really goes
step by step.

Acknowledgments

I would like to thank people who helped prepare this document. In particular, I heartily
thank my colleagues from LASEC, the Security and Cryptography Laboratory, espe-
cially Pascal Junod who was responsible for the exercises in the beginning of this course,
and also Gildas Avoine, Thomas Baign`eres, Martine Corval, Matthieu Finiasz, Yi Lu,
Jean Monnerat, as well as Claude Barral, Julien Brouchier, and Simon K¨unzli. I am
also grateful to the EPFL students for working hard on the course and their extensive
comments, especially Ramun Berger and Nicolas Dunais. I owe my gratitude to my
peers for highly valuable comments: Eli Biham, Colin Boyd, Matt Franklin, Dieter Goll-
mann, Marc Joye, Willi Meier, David Naccache, Phong Nguyen, Raphael Phan, David
Pointcheval, and Jacques Stern. Finally, I would like to thank Christine and Emilien for
their permanent encouragement and love. This textbook is dedicated to them.

Serge Vaudenay

1
Prehistory of Cryptography

Content

Foundation: history, vocabulary, transpositions, substitutions
Basic ciphers: simple substitution, Vigen`ere, Vernam
Modern settings: digital communications, Kerckhoffs principles
(cid:1)The Shannon Theory of secrecy: entropy, encryption model, perfect secrecy

Cryptography distinguishes itself from coding theory in the sense that the presence
of random noise in the latter is replaced by malicious adversaries in the former. It uses
a speciﬁc vocabulary and relies on some fundamental principles which are presented
in this chapter. We also expose historical cases which are relevant for our analysis, and
survey the Shannon Theory of secrecy which was an attempt to make cryptography rely
on information theory. Unfortunately, the goals of achieving “perfect” secrecy were too
ambitious and impossible to achieve in practice at a reasonable cost.

1.1 Foundations of Conventional Cryptography

1.1.1 The Origins of Cryptography

As an easy introduction, we can say that, strictly speaking, cryptography begins with
history: with the origin of language writing. As an example, the Egyptians were able
to communicate by written messages with hieroglyph. This code was the secret of a
selected category of people: the scribes. Scribes used to transmit the secret of writing
hieroglyph from father to son until the society collapsed. It was only several millenia
later that this secret code was broken by Champollion.

Although Egyptian codes are quite anecdotal, history includes many other cryp-

tographic usages.1 Communication with secret codes was commonly required

for diplomacy: governments had to communicate with their remote embassies in

suspicious environments,

during war: army headquarters had to communicate through hostile environments,
for individual or corporate privacy: some people wanted to be protected against
their neighborhood (against jealous spouses, against dictatorships, etc.), com-
panies wanted to protect their assets against competitors.

1 See Ref. [173] for other examples, or Ref. [98] for references on history.

2

Chapter 1

Most of these stories, however, used cryptography in a pedestrian way. This was the
prehistory of cryptography. Most of the secret codes had a security based on obscurity:
secret codes were dedicated to applications, and people who wanted to communicate
securely had to choose their own secret code. Thus, all communication users had to
be cryptographers. Modern cryptography history began with electrical communication
technology to which this model was clearly not well suited.

Knowledge and science used to be limited to a small category of privileged peo-
ple in ancient civilizations until education became accessible to everyone. Hopefully,
education brings communication to people: most people in developed countries know
(more or less) how to read and write in a common language (or even several ones) and
most people have access to communication systems. It is furthermore impractical to
invent a new language, or a new communication system, in order to provide secrecy.
So we cannot use common communication systems in order to provide secrecy.

Language makes it feasible to encode any information into a standard message
which consists of a character stream. Following the Shannon Separation Principle
paradigm, we should deﬁnitely use an additional code in order to encode the standard
message into a secret code. This process, called encryption, must be invertible, and the
inversion should require a secret information.

1.1.2 Key Words

We list here a few key words. The reader may also use the Internet Security Glossary
which was published by the Internet Society as the RFC 2828 informational standard
(Ref. [168]).

by unauthorized parties.

r Conﬁdentiality, secrecy: insurance that a given information cannot be accessed
r Privacy: ability for a person to control how his personal information spreads in
r Code: a system of symbols (formally, a set of “words” called codewords) which
r Coding theory: science of code transformation which enables to send informa-

represent information. Note that codes are not related to secrecy from this deﬁ-
nition.

a community. This is often (improperly) used as a synonym of “secrecy.”

tion through a communication channel in a reliable way. Usually, this theory
focuses on noisy channels and tries to make the information recovery feasible
to anyone (as opposed to cryptography which tries to make the information
recovery feasible for authorized parties only).

r Encode, Decode: basic processes of coding theory: action to transform an infor-
r Cryptography: (originally) the science of secret codes, enabling the conﬁdential-

mation into a codeword, or to recover the information from a codeword.

ity of communication through an insecure channel. As opposed to coding the-
ory which faces random noises, cryptography faces malicious adversaries. Now

Prehistory of Cryptography

3

“cryptography” has a wider sense, being deﬁned as the science of information
protection against unauthorized parties by preventing unauthorized alteration of
use. Cryptographic algorithms are the mathematical algorithms which enforce
protection.

r Cipher: (formally) secret code, enabling the expression of a public code by a

secret one by making the related information conﬁdential. This deﬁnition is
different from the one in RFC 2828, namely, “a cryptographic algorithm for
encryption and decryption.”

r Cryptographic system: set of cryptographic algorithms which include ciphers
r Cryptosystem: an abbreviation of “cryptographic system” which is not recom-

and other cryptographic schemes.

mended in RFC 2828. It is mostly used for “public-key cryptosystem”2 in which
case it is a set of cryptographic algorithms that include algorithms for key pair
generation, encryption, and decryption. It is also (improperly) used as a synonym
of “cipher.” Cipher is mostly used for symmetric key techniques.3

r Cleartext: information encoded by using a public code, i.e. available “in clear.”
r Plaintext: input of an encryption algorithm (usually, a cleartext).
r Ciphertext, cryptogram: information encoded by a cryptographic system.
r Encryption, encipherment, decryption, decipherment: basic cryptographic pro-

cesses: action to transform a plaintext into a ciphertext or the opposite. Note
that purists make a subtle difference between decryption and decipherment as
detailed below.

r Decryption (for purists): action to transform a ciphertext into a plaintext by an
r Decipherment (for purists): action to transform a ciphertext into a plaintext by
r Cryptanalysis, cryptographic analysis, cryptoanalysis: theory of security anal-

an authorized party.

unauthorized party.

ysis of cryptographic systems. Usually, this term is used in a negative way: for
the insecurity analysis (by breaking the security of systems). This is a little mis-
leading since this can also be used in a positive way: for security certiﬁcation
(by formal proof or reduction to problems which are known to be hard but not
yet proven to be so).4

r Breaking a cryptosystem: proving the insecurity of a cryptosystem, for instance
r Cryptology: science of cryptography and cryptanalysis (sometimes also
r Steganography: science of information hiding. Here we do not want to protect

steganography). Purists thus distinguish “cryptography” from its superset “cryp-
tology.” (The title of the present textbook may thus be a little misleading.)

by exhibiting how to decrypt a message.

the secrecy of an information only, we also want to make sure that any unau-
thorized party has no evidence that the information even exists (for instance, by
watermarking).

2 See Chapter 9.
3 See Chapter 2.
4 See, e.g., Chapter 4.

4

Chapter 1

1.1.3 Transpositions, Substitutions, and Secret Keys

In the antiquity, Spartan warriors used to encrypt messages by using scytales. These
were cylinders around which they wrapped a leather belt. Encryption was performed
by writing the message on a leather belt along the axis of the cylinder and unwrapping
the belt. Decryption was performed by wrapping the belt around a cylinder of the same
diameter and reading along the axis.

Obviously, this encryption consists of changing the order of the characters in the

message, according to a secret permutation called transposition.

Later in Rome, Caesar used another cryptographic system which consisted of
replacing every character by the character which comes three positions later in
the alphabet. Following the Latin alphabet of Caesar5, the substitution was as fol-
lows.

a b c d e f g h i k l m n o p q r s t v x
D E F G H I K L M N O P Q R S T V X A B C

For instance, the plaintext caesar was encrypted into FDHXDV.6 The Caesar Cipher
generalizes into the simple substitution cipher: we encrypt by replacing every character
by another one obtained by a secret permutation of the alphabet. We decrypt by replacing
every character by another one obtained by the inverse permutation of the alphabet.
The permutation is called a substitution.

The UNIX community is already familiar with the ROT13 substitution which shifts

the alphabet by 13 positions as follows.

a b c d e f g h i j k l m n o p q r s t u v w x y z
N O P Q R S T U V W X Y Z A B C D E F G H I J K L M

This makes it an involution: rotating twice by 13 positions consists of rotating by 26
positions. Since 26 is the exact size of the alphabet, this rotation is a complete rotation
which leads to no transform at all.

It is quite easy to break a simple substitution cipher by using frequencies of char-
acter in human language. We can, for instance, use the frequencies of characters in
English texts as given in Fig. 1.1. We can also use frequent digrams or trigrams. Here

5 Most textbooks on cryptography describe the Caesar cipher with a 26-character alphabet, which is a little
anachronic. There were only 21 characters at this time. Characters Y and Z were foreign characters, used
in order to transcript Greek words. Characters I and J were the same one, I. Characters U and V were
also the same (V). Character W did not exist.

6 Writing ciphertext with capital letters and plaintext with small letters is a common convention.

Prehistory of Cryptography

5

Letter Probability Letter Probability Letter Probability

A
B
C
D
E
F
G
H
I

0.082
0.015
0.028
0.043
0.127
0.022
0.020
0.061
0.070

J
K
L
M
N
O
P
Q
R

0.002
0.008
0.040
0.024
0.067
0.075
0.019
0.001
0.060

S
T
U
V
W
X
Y
Z

0.063
0.091
0.028
0.010
0.023
0.001
0.020
0.001

Figure 1.1. Frequencies of characters in english texts.

are the 30 most frequent digrams in decreasing order of likelihood:

TH, HE, IN,
TO, NT, HA,
IT, AR, TE,

ER,
ND,
SE,

AN,
OU,
HI,

ED,
RE,
EA,
NG,
and OF.

ON,
AS,

ES,
OR,

ST,
TI,

EN,
IS,

AT,
ET,

Here are the 12 most frequent trigrams:

THE, ING,
FOR,

and

AND,

HER,

ERE,

ENT, THA,

NTH,

WAS,

ETH,

DTH.

Transposition and substitution are the two elementary operations which can be

used to build up a cipher. Another important concept is the notion of key.

The Caesar Cipher was improved in the sixteenth century by Blaise de Vigen`ere.
Here we consider every character as an integral residue modulo the size of the alphabet.
This way, the Caesar Cipher can be considered as adding 3 to every characters. The
Vigen`ere Cipher consists of using a word as a secret key K , splitting the messages into
blocks of the the same length of the key, and adding characterwise the key onto every
block.

As an example we encrypt this is a dummy message with the key ABC.

Here we need to compute

thi
+ ABC
= TIK

sis
ABC

adu
ABC

mmy
ABC

mes
ABC

sag
ABC

SJU

AEW

MNA

MFU

SBI

e
A

E

and we obtain TIKSJUAEWMNAMFUSBIE. Note that adding A, B, and C corresponds
to a translation by 0, 1, and 2 positions respectively in the alphabet. Translations are
cyclic, e.g. y + C = A.

6

Chapter 1

We notice important particular cases:

r when the key is of length 1, we obtain a simple substitution;
r when the key is as long as the message and the alphabet is binary, we obtained

the Vernam cipher (one-time pad). (see Section 1.1.4.)
Given a sequence x = x1x2 ··· xn of characters xi in an alphabet Z, we deﬁne the

number nc of indices i for which xi = c, and the index of coincidence by

(cid:1)

c∈Z

nc(nc − 1)
n(n − 1)

Index = Pr

I,J

[x I = x J|I < J ] =

where I and J are two independent uniformly distributed elements of {1, . . . , n}. The
index of coincidence is invariant under substitution: if we encrypt x into another se-
quence by simple substitution, this will not change the index of coincidence. When n
tends toward inﬁnity, the index of coincidence only depends on the sequence distribu-
tion. For a sequence randomly taken from an English text, the index of coincidence is
approximately 0.065. For sequences generated from a truly random distribution, this is
approximately 0.038.

This index of coincidence can thus be used in order to break the Vigen`ere Cipher.

put the ciphertext in an array with m columns
check that the index of coincidence of each column is high

1: for all guesses for the length m of the secret key do
2:
3:
4: end for
5: check the key shifts between columns with mutual indices of coincidence

(Note that the guess for the length of the key can be speeded up by using the Kasiski
Test.) The mutual index of coincidence between two sequences x = x1x2 ··· xn and
y = y1 y2 ··· yn is PrI,J [x I = yJ ]. It should be high when the two sequences come
from the same substitution.

With the example TIKSJUAEWMNAMFUSBIE, if we guess that m = 3 we can

write

I
J
E
N
F
B

K
U
W
A
U
I

T
S
A
M
M
S
E

and so we can compute the index of coincidence of TSAMMSE, IJENFB, and KUWAUI.
(Note that this example is not so relevant because the ciphertext is too short.)

Prehistory of Cryptography

1.1.4 Vernam Cipher

7

With the industrial revolution, communications became automatic (with telegraph,
radio, etc.). Encryption and decryption had to be performed by a cryptographic device.
The telewriter used the Baudot code for which all characters are encoded into 5 bits.
The Vernam cipher (1926) is deﬁned by

r the plaintext is a bitstring: an element of {0, 1}n
r the secret key is a uniformly distributed element of {0, 1}n
r the ciphertext is CK (X) = X ⊕ K where ⊕ is the bitwise XOR

The key is aimed at being used for only one plaintext. For this reason this cipher is also
known as one-time pad. It was published in 1926 by Gilbert Vernam, from AT&T. (It
was actually invented at the end of the First World War, but since the war was over,
people did not get any more interest in it until 1926.) The security was formally proven
by Shannon, by using the notion of perfect secrecy. Later on, the Vernam cipher was
used for the red telephone between Moscow and Washington.

Note that the Vernam cipher uses a keyed substitution. It is a kind of Vigen`ere

cipher with a binary alphabet and a one-time key.

The drawbacks of this cipher are that

r the key must be at least as long as the message,
r it becomes insecure if a key is used twice,
r the security result makes sense only when the key source is truly random,
r some pseudorandom keys may lead to insecure implementations,
r randomness is expensive.

The security of the Vernam cipher is indeed critically sensitive to the freshness require-
ment on the secret key. In the forties, the Soviet intelligence agency KGB was using
the Vernam cipher, but was reusing some fragments of keys several times. This led the
American counterpart NSA to decrypt messages in the famous VERONA project.

This cipher notably illustrates in a very nice visual way as shown by Moni Naor
and Adi Shamir in Ref. [137]. Let us assume that the plaintext, the ciphertext, and the
key are black-and-white images. These are indeed sequences of bits: a white pixel is
a 0 bit and a black pixel is a 1 bit. We assume that we perform the encryption on a
computer and that we next print the ciphertext and the key by representing bits with
special square black-and-white patterns. We use a balanced black-and-white pattern,
e.g. a 2 × 2 array with two white cells and two black cells. This pattern is used in order
to represent 0 bit. The complement pattern is used in order to represent 1 bit. This step
is called “pixel coding” (see Fig. 1.2). Now we can overlay the key onto the ciphertext.
Nonoverlapping patterns will be complement patterns and will look completely black.
As a matter of fact, they correspond to complement bits whose XOR is 1. Overlapping
patterns will look gray from far away (see Fig. 1.3). They correspond to equal bits

8

Chapter 1

→

→

0

1

Figure 1.2. Pixel coding for visual cryptography.

whose XOR is 0. Hence we can visually decrypt. As an example, Fig. 1.4 shows a
ciphertext, a key, and what we see from the overlay.

1.1.5 Enigma: Toward Industrial Cryptography

A famous example of an industrial encryption scheme is the Enigma machine which
was used by the Germans during World War II.

We let π be a ﬁxed permutation over the 26-character alphabet with the following

properties:

r it has no ﬁxed point (π(x) (cid:4)= x for any x),
r it is an involution (π(π(x)) = x for any x).

We let S be a set of ﬁve permutations over the alphabet. For α ∈ S, we let αi denote
ρ−i ◦ α ◦ ρi where ρ is the circular rotation over the alphabet by one position (ρi
thus denotes the circular rotation over the alphabet by i positions). We let σ be any
involution over the alphabet with the property that it has exactly six ﬁxed points. We
pick α, β, γ ∈ S pairwise different. We pick a number a.

0⊕ 0 −→

0⊕ 1 −→

1⊕ 0 −→

1⊕ 1 −→

≈

=

=

≈

Figure 1.3. Visual pixel XOR.

Prehistory of Cryptography

9

Figure 1.4. Visual cryptography example.

For a plaintext x = x1, . . . , xm with m < 263, we let
◦ βi2

yi = σ −1 ◦ α−1

◦ π ◦ γi3

◦ β−1

◦ γ −1

i1

i2

i3

◦ αi1

◦ σ (xi )

where i3i2i1 are the last three digits of the basis 26 numeration of i + a. Finally, we let
y = y1, . . . , ym be the ciphertext.

This is a mathematical deﬁnition of an electromechanical device. In practice, α,
β, and γ are rotors that permute 26 electrical signals, αi simply consists of α turned
by i positions, S is a box of ﬁve rotors, π is a cabled permutation which sends back
electrical signals, σ is a conﬁgurable wire connection, and a is the initial position of
the rotors.

Here the secret consists of

r the speciﬁcation of manufactured permutations like π and the ﬁve permutations
r the choice of the initial offset a,
r the choice of σ .

of S,

Once again, we notice that the Enigma cipher consists of a keyed substitution. Here,
the key is rather short.

10

Chapter 1

Enigma was made to be “unbreakable,” even if the enemy had the speciﬁcations
(namely, the above description and π, α, β, γ ): the number of combination is so high
(namely 263 times the number of possible wired permutation σ ) that the enemy cannot
obtain the key by exhaustive search. Poles reconstructed the Enigma machine before
World War II and started to perform cryptanalysis. Their knowledge was given to the
Allied forces when Poland was invaded.

Cryptanalysis starts with the following observation. The permutation which maps
xi onto yi is an involution without any ﬁxed point, and any two of these permutations
which differ only on the choice of σ are isomorphic. The attack then starts by ﬁnding
known plaintexts: messages always have predictable headers or footers, and we can
rule wrong guesses out by checking that we have no ﬁxed points. Then, we describe
the observed sequence of input-output pairs and we describe it as a graph. If we guess
the initial choice of a, we must obtain an isomorphic graph. An automatic process then
exhaustively tries all possible a combinations (we have 263 of them). Handy analysis
then ends up recovering σ .

Interestingly, the ﬁrst programmable computer which was ever built (even before
the ENIAC) was constructed in order to break ciphers like Enigma. Some dedicated
machines called “bombes” (from the name of a Polish ice cream) were performing
the exhaustive search on a. Alan Turing was a member of the team which built this
machine and so this was indeed the ﬁrst Turing machine. The reason why textbooks
on computer science refer to ENIAC as the ﬁrst computer is that the existence of these
activities became to be publicly known only in the seventies.7

1.2 Roots of Modern Cryptography

1.2.1 Cryptographic Problems: The Fundamental Trilogy

In Anglo-Saxon literature, cryptography is usually illustrated by famous characters:
Alice and Bob (A and B). For biblical reasons, the malicious adversary is usually
female and called Eve. According to the (politically incorrect) folklore, Alice and Bob
want to communicate securely (for instance, conﬁdentially) and to protect against Eve.
A Canadian rock band was even inspired to write a French song about it.8

Cryptography concentrates on three fundamental paradigms.

Conﬁdentiality. The information should not leak to any unexpected party.
Integrity. The information must be protected against any malicious modiﬁcation.

(Example: integrity of communications, integrity of backups, etc.)

Authentication. The information should make clear who the author of it is. (Ex-

ample: signature authentication, access control, etc.)

7 For more references, see Ref. [91].
8 “Les Tchigaboux,” see http://www.iro.umontreal.ca/˜crepeau/CRYPTO/Alice-Bob.html.

Prehistory of Cryptography

11

Some other cryptographic notions are also considered. They are dedicated to spe-
cial problems. We make here a brief list which is far from exhaustive in order to suggest
possible original problems in modern cryptography. It illustrates the rich variety of
modern cryptographic problems.

Nonrepudiation. In the case of a dispute on the origin of the document, someone
should be able to formally prove that he is not the author. This repudiation
proof should be made impossible if he actually is the author.

Electronic payment. The notion of electronic coin should be protected against, for

instance, double spending, because it is easy to copy digital information.

Anonymity. Privacy protection may require anonymity enforcement.
Electronic votes. Democracy protection requires that ballots should be anonymous,
that a single person should not vote more than once, and that people should
not be able to prove for whom they voted afterwards (otherwise they could be
subject to threats or corruption).

Zero-knowledge. We want to make sure that no information leaks out of a security

protocol.

1.2.2 Assumptions of Modern Cryptography

Cryptography in communication systems relies on some fundamental principles.

The n2 Problem

In a network of n users, there is a number of potential pairs of users within the order of
magnitude of n2. Obviously we cannot make a dedicated secure channel between any
pair of users. This means that we cannot invent a new cryptosystem for every pair of
users.

We should better use a common cryptosystem, but enable the distinction between
pairs of users by making them choose their own secret key like in the Vigen`ere cipher
or the Enigma cipher. In addition, this paradigm beneﬁts from the fact that not every
user needs to be a mathematician in order to make a new cryptosystem.

We deduce a need for the cryptosystem to be shared among a large number of
users, and a need for the cryptosystem to depend on an easily selectable parameter
called a secret key.

The Kerckhoffs Principle

Assuming the cryptosystem is designed by a third party, from a third company, in a
third country jurisdiction, since it is furthermore implemented in n points of a network,
security should deﬁnitely not rely on the secrecy of the cryptosystem itself. The pro-
tection of the cryptosystem structure may be considered as an extra security protection,
but should not be necessary for security.

12

Chapter 1

Therefore, the security analysis of the cryptosystem must assume that the algorithm

is public.

Note that despite what cryptographers regularly claim, the Kerckhoffs Principle

does not mean that we should make the cryptosystem public.

Auguste Kerckhoffs, a French professor of grammar, stated other principles about

cryptography in the nineteenth century, but this one is the most popular one.

The Moore Law

Secret keys can be tried exhaustively. With technological improvements, comput-
ers are faster and faster. Moore stated an empirical law which says that the speed
of CPUs doubles every 18 months. If a cryptosystem is designed for long-term se-
crecy, the secret key must thus be long enough to resist exhaustive search using future
technologies.

The Murphy Law

If there is a single security hole, the exposure of a cryptosystem will make sure that
someone will eventually ﬁnd it. Even if this person is honest, this discovery may
ultimately leak to malicious parties. By extension we should keep in mind that security
does not add up: systems are as secure as their weakest part.

1.2.3 Adversarial Models

For studying the security of a cryptosystem we must consider its whole environment.
Security analysis identiﬁes several famous threat models. Here are a few attack models
against ciphers. We can, of course, consider any combination of them.

Ciphertext-only attack. The adversary tries to break the system by wiretapping the

ciphertext messages.

Known plaintext attack. The adversary obtains the ciphertext and succeeds to get
the corresponding plaintext in a way or another (for instance, if the plaintext
is a standard message). She then tries to exploit this extra information to break
the system.

Chosen plaintext attack. The adversary can play with the encryption device and
submit appropriately chosen plaintexts, and get the corresponding cipher-
texts in return. She then tries to exploit this experiment by breaking the
system.

Chosen ciphertext attack. The adversary can play with the decryption device, and
thus decrypt any chosen cryptogram. She then tries to exploit this experiment
in order to decrypt other cryptograms without using this access model.

We distinguish adversary capabilities (as above) from adversary goals. In the case of
attack models against ciphers, we can, for instance, consider key recovery attacks (in

Prehistory of Cryptography

13

which the goal is to deduce the secret key) from a decryption attack (in which the goal
is to decrypt a target ciphertext). It is thus quite important to describe the adversary
models related to every cryptographic system.

1.2.4 Cryptography from Various Perspectives

On the Government Side: The Cold War

The military success of the Allies during World War II was partly due to cryptography,
and as such, cryptography became highly estimated during the cold war. A famous
implementation was the red telephone between Moscow and Washington DC. This was
a kind of email service (not a real telephone) with encryption through a one-time pad
(see Section 1.1.4).

United States and Russia had an opposite approach of intelligence. While Russia
mainly used on-site physical agents, the United States started to develop a communi-
cation wiretapping industry. They created the NSA, the existence of which remained
secret until very recently, and developed the Echelon network.9 Echelon is aimed at
listening to all electronic communications (telephone, radio, fax, satellite, cables, etc.),
at automatically analyzing and ﬁltering them, and at developing virtual agents which
can trigger alerts.

Cryptography was considered as a war weapon and regulated as such: import–
export organizations, salesmen, developers, researchers, publishers were controlled by
government agencies in many countries (United States, France, etc.). Switzerland was
one of the only cryptographic paradise where one could freely set up mirror companies
for cryptographic products. In other countries, obscure agencies used to put vetoes
on some sensitive research projects (like studying data integrity control algorithms),
to lobby to forbid scientiﬁc publications (even the present textbook could have been
classiﬁed as a war weapon and the book holder could have been prosecuted by a
military court), and to classify some patent applications. Programs were set up in
order to facilitate government inspection of private communications by key escrow (for
instance, with the US cliper chip which has now disappeared).10 There is indeed an
equilibrium to ﬁnd between individual privacy and national security, and governments
used to clearly favor the latter. The situation has become much more liberal now and
cryptography can be taught, studied, and used in private business.

On the Industry Side: Electronic Commerce

While cryptography research, development, and usage were restricted, there was a need
for communication protection in civil environments. In the seventies, banking was using
more and more electronic transactions and had to make them secure. At this time, the

9 For more references on the NSA, see Ref. [22].
10 This situation is described by Whitﬁeld Difﬁe and Susan Landau in Ref. [60].

14

Chapter 1

US Government made an encryption standard available (DES) to protect unclassiﬁed
but sensitive data.11

Now, the rise of electronic commerce and mobile communication introduces a
new need for security. Industries also need to protect industrial secrets and trades.
Software manufacturers need to have secure and user-friendly license management
tools. Artists and major companies had a severe revenue loss because of peer-to-peer
media exchange platforms. More generally, most goods providers, not necessarily in
the information technology sector, want to implement easy tracing tools such as tags
using radio frequency identiﬁcation (RFID) technology. It should be used in order to
optimize logistic, but it is a threat to people’s privacy at the same time.

On the Academic Side: Research in Cryptology

At the same time that the US Government was pushing for an encryption standard, the
academic world in computer sciences was building complexity theory: the mathematical
notion of intractability, NP-completeness, one-way functions, etc.12 The mathematical
notion of complexity became the natural root of information protection.

The academic world naturally discovered the notion of public-key cryptography13:
we could encrypt with a public key, but decryption without a private key had to face a
large complexity problem.14

On the People Side: Towards Civil Cryptography?

At the same time that governments and industry were developing security applica-
tions for their businesses, people also wanted to protect their activities. The PGP ef-
fort (Pretty Good Privacy) by some privacy protection activists illustrates the need to
strengthen people’s privacy by email encryption against governmental requirements.15
Law enforcement, goods traceability, and license and media protection evolve in such
a way that automatic applications can implement them. This typically violates people’s
privacy.

Even democracy management may be performed by machines since electronic
voting is being developed. Obviously this must be done under very careful concern about
the people’s integrity. Since the present document is aimed at teaching cryptography,
the author would like to make the reader aware of the risks of a possible technologic
nightmare in preparation.

11 See Chapter 2.
12 See Chapter 8.
13 See Chapter 9.
14 For more details, see Ref. [175]. See also Ref. [116].
15 See Chapter 12.

Prehistory of Cryptography

1.2.5 Methodology

Communication Channels

15

Communication channels have different kinds of attributes: cost, speed, availability,
reliability, security. Here, reliability refers to resistance against random noise. We do
not consider it since it is addressed by coding theory. So we implicitly consider that all
communication channels perform a transmission in a reliable way: the sent information
is always equal to the received one unless there is a malicious attack.

As we have seen, security may relate to the ability to provide conﬁdentiality, in-
tegrity, or authentication. If we use basic telegraph through radio signal, speed is high,
cost is low, but security is void. Availability is also high since ether is (in principle) al-
ways usable. If we now use the diplomatic case to transmit information (for instance, we
give some information to an ambassador who is physically sent to the information des-
tination), we have a low speed, a high cost, but a high security. Availability also depends
on the airplane and the schedule of the ambassador. If we now use Enigma-encrypted
radio signals, the speed is high, the cost is relatively low (the development of the Enigma
machine is quickly amortized in wartime), and the security should have been high.

Note that we talk about communication channels in a broad sense: we are not only
interested in moving some information from one place of a three-dimensional space to
another. We are also interested in the fourth dimension (time): we also want to archive
some information which can be used later. So an archive system can also be considered
as a communication channel.

Reduction

Computer science is a matter of reduction: instead of solving a problem from scratch,
we try to cut it into several subproblems, and reduce one problem to another one.
Security is the same. As we will see later, it is impossible to consider that we can have
a single communication channel which is cheap, fast, available, and secure. On the
other hand, we can use expensive, slow, hardly available, but secure channels in order
to transform a cheap, fast, available, and insecure channel into a cheap, fast, available,
and secure one. In the next sections and chapters we will see how to do it by improving
the security attributes.

1.3 (cid:1)The Shannon Theory of Secrecy

1.3.1

(cid:1)Secrecy of Communication

The purpose of encryption is to ensure communication secrecy. We assume that we
want to communicate, which means to transmit information through a channel. The
channel is not assumed to be secure.

16

Chapter 1

Enemy cryptanalyst

6

Message
source

Message X

-

Encipherer

Cryptogram Y

C

6
Key K

Key
source

-

Decipherer

C −1

-X

6

Figure 1.5. The Shannon encryption model.

Following the Shannon Theory, we do not encrypt ﬁxed messages, but messages
coming from a plaintext source. The plaintext source generates random texts according
to some given probability distribution. For instance, with the distribution of plain
English texts, the probability that the plaintext is hello world is much greater
(particularly when the message is a textbook about a programming language) than the
probability that it is gbwiub oafp (except maybe in some Vogon poetry).16

Following the Shannon Theory, a cipher is given by

1. a plaintext source (with the corresponding distribution),
2. a secret key distribution,
3. a ciphertext space,
4. a rule which transforms any plaintext X and a key K by a ciphertext Y =
5. a rule which enables recovering X from K and Y = CK (X) as X = C
−1
K

CK (X),

(Y ).

(see Fig. 1.5.) A more intuitive deﬁnition of a cipher includes

1. a plaintext space, a ciphertext space, a key space,
2. a key generation algorithm,
3. an encryption algorithm,
4. a decryption algorithm.

These deﬁnitions relate to conventional cryptography.

16 See the Hitch Hiker’s Guide to the Galaxy trilogy by Douglas Adams.

Prehistory of Cryptography

17

In this setting, the secret key K must still be transmitted in a secure way, so we
need a secure channel. To summarize, in order to transmit a message securely, we ﬁrst
need to set up a key and transmit it securely. Is this a vicious circle? It is not for two
reasons. Firstly, the key can be a very short piece of information and it can be easier
to protect it than to protect the message. We can afford using an expensive channel in
order to send a short key securely. Later on, we use an inexpensive channel in order
to send long messages securely thanks to encryption. Secondly, even if the key is long
(which is the case of the Vernam cipher), it still makes sense to use an expensive
channel to transmit it securely. The secure channel may not be available all the time, or
may have longer transmission delays (for instance, if this channel consists of shipping
the key by airplane from Washington to Moscow). We can use it when available by
anticipating that we will have to transmit a conﬁdential document in the future. Later on,
we can use any available channel with fast transmission. This makes the secure channel
virtually available. We summarize this paradigm by saying that using an expensive,
slow, hardly available, but conﬁdential channel, we can transform any other channel
into a conﬁdential one by using the Shannon model of encryption.

1.3.2

(cid:1)Entropy

Given a random variable X, we deﬁne the entropy by

H(X) = −

Pr[X = x] log2 Pr[X = x].

(cid:1)

x

The joint entropy H(X, Y ) of two random variables is deﬁned as the entropy of the
joint variable Z = (X, Y ), i.e.
(cid:1)

H(X, Y ) = −

Pr[X = x, Y = y] log2 Pr[X = x, Y = y].

x,y

Note that X and Y are independent if and only if Pr[X = x, Y = y] = Pr[X = x] ×
Pr[Y = y] for any x and y. We further deﬁne the conditional entropy H(X|Y ) as
H(X|Y ) = H(X, Y ) − H(Y ), i.e.
(cid:1)

H(X|Y ) = −

Pr[X = x, Y = y] log2 Pr[X = x|Y = y].

x,y

Here are some basic facts on entropy.17

Theorem 1.1. For any distribution, we have

r H(X, Y ) ≥ H(X) with equality if and only if Y can be written f (X)
r H(X, Y ) ≤ H(X) + H(Y ) with equality if and only if X and Y are independent;

17 For more information, see the textbook by Cover and Thomas (Ref. [52]).

18

r if Pr[X = x] (cid:4)= 0 for at least n values of x then H(X) ≤ log2 n with equality if
and only if all nonzero Pr[X = x] are equal to 1
n .

Chapter 1

1.3.3

(cid:1)Perfect Secrecy

Perfect secrecy means that the a posteriori distribution of the plaintext X after we know
the ciphertext Y is equal to the a priori distribution of the plaintext: the conditional
distribution of X given Y is equal to the original distribution. Formally, for all x and y
such that Pr[Y = y] (cid:4)= 0, we have Pr[X = x|Y = y] = Pr[X = x].
Theorem 1.2. Perfect secrecy is equivalent to H(X|Y ) = H(X) and to the statistic
independence between X and Y .

Proof. We have

H(X|Y ) = H(X, Y ) − H(Y ) ≤ H(X)

with equality if and only if X and Y are independent. Thus H(X|Y ) = H(X) is equiv-
alent to the independence of X and Y .

If we have perfect secrecy, then
Pr[X = x, Y = y]

Pr[Y = y]

= Pr[X = x|Y = y] = Pr[X = x]
for any x and y, thus X and Y are independent, thus H(X|Y ) = H(X).
If now H(X|Y ) = H(X), and X and Y are independent, we have
= Pr[X = x]

Pr[X = x|Y = y] = Pr[X = x, Y = y]

Pr[Y = y]

for any x and y; thus we have perfect secrecy.
Theorem 1.3 (Shannon 1949). Perfect secrecy implies H(K ) ≥ H(X).
Proof. We ﬁrst prove the intermediate property which holds in all cases: H(Y ) ≥ H(X).
First, we have H(Y ) ≥ H(Y|K ). We notice that the knowledge of K gives the same
distribution for X and Y , thus H(Y|K ) = H(X|K ). But since X and K are independent,
we obtain H(Y|K ) = H(X). We thus have H(Y ) ≥ H(X).

(cid:1)

We now notice that when X is ﬁxed, the knowledge of K determines Y . Further-
more, K and X are independent. Thus we have H(Y, K|X) = H(K ). Then we have
H(X, Y, K ) ≥ H(X, Y ). Thus we have H(K ) ≥ H(Y|X). If we have perfect secrecy,
we have H(Y|X) = H(X|Y ) + H(Y ) − H(X) = H(Y ). Thus we have H(K ) ≥ H(Y ).
(cid:1)

Hence we obtain H(K ) ≥ H(X).

Prehistory of Cryptography

19

Corollary 1.4. If X is an m-bit string and if we want to achieve perfect secrecy for any
distribution of X, then the key must at least be represented with m bits.

Proof. If we want to achieve perfect secrecy for any a priori distribution of X, we
need to have H(K ) ≥ H(X) for any distribution of X of m-bit strings. For the uniform
distribution we obtain H(K ) ≥ m. Now if k is the key length, we know that for any
distribution of K , we have H(K ) ≤ k. Thus we have k ≥ m.
(cid:1)

The corollary and the following result show that we cannot achieve perfect secrecy

in a cheaper way than the Vernam cipher.

Theorem 1.5. The Vernam cipher provides perfect secrecy for any distribution of the
plaintext.
Proof. Let Y = X ⊕ K be the ciphertext where X and K are independent bit strings
of length n, and K is uniformly distributed. For any x and y, we have

Pr[X = x, Y = y] = Pr[X = x, K = x ⊕ y]

= Pr[X = x] × Pr[K = x ⊕ y]
= Pr[X = x] × 2

−n.

By adding over all x we obtain that Pr[Y = y] = 2
y] = Pr[X = x] for any x and y.

−n. We deduce that Pr[X = x|Y =
(cid:1)

1.3.4

(cid:1)Product Ciphers

Given two ciphers C and C
the product cipher C

deﬁned by two secret key distributions K and K

(cid:8) ◦ C with the product distribution on the secret key (K , K

, we deﬁne

(cid:8)

(cid:8)

(cid:8)

).

1.4 Exercises

Exercise 1.1. Propose a way in order to break simple substitution ciphers.

Exercise 1.2. Friedrich Kasiski, a Prussian military ofﬁcer, worked on the Vigen`ere
cipher in the early nineteenth century and developed a famous test. The Kasiski Test
consists of counting the number of occurrences of multigrams. (Multigrams are sub-
words of the cryptogram. Example: digrams are multigrams of length two, trigrams are
multigrams of length three, etc.) Explain how we can use the Kasiski Test in order to
break the Vigen`ere cipher.

Exercise 1.3. Compute the mutual index of coincidence between two streams of English
text transformed with the same random substitution.

Compute the mutual index of coincidence between two streams of English text

transformed with two independent random substitutions.

20
Chapter 1
Exercise 1.4. Let n be an integer. A Latin square of order n is an n × n array L with
entries in {1, . . . , n} such that each integer appears exactly once in each row and each
column of L. It deﬁnes a cipher over the message space {1, . . . , n} and the key space
{1, . . . , n} in which the encryption of i under the key k is L(k, i).

Prove that a Latin square deﬁnes a cipher which achieves perfect secrecy if a key

is used once and is uniformly distributed.

Exercise 1.5. We assume that the plaintext

is encrypted into the ciphertext

conversation

HIARRTNUYTUS

by using the Hill cipher. This cipher uses an m × m invertible matrix in Z26 as a secret
key. First the messages are encoded into sequences of blocks of m Z26-integers. Each
block is then separately encrypted by making a product with the secret matrix.

Recover m and the secret key by a known plaintext attack.

Exercise 1.6. Product of Vigen`ere ciphers.

1. Given a ﬁxed key length, prove that the set of all Vigen`ere encryption function

deﬁned by all possible keys of given length is a group.

2. What is the product cipher of two Vigen`ere ciphers with different key lengths?

2
Conventional Cryptography

Content

DES: Feistel Scheme, S-boxes
Modes of operation: ECB, CBC, OFB, CFB, CTR, UNIX passwords
Classical designs: IDEA, SAFER K-64, AES
(cid:1)Case study: FOX, CS-CIPHER
Stream ciphers: RC4, A5/1, E0
Brute force attacks: exhaustive search, tradeoffs, meet-in-the-middle

In Chapter 1 we saw the foundations of cryptography. Shannon formalized secrecy
with the notion of entropy coming from information theory, and proved that secrecy was
not possible unless we used (at least) the Vernam cipher. Except for the red telephone
application, this is not practical. We can however do some cryptography by changing
the model and relying on a computational ability. Before carefully formalizing com-
putability in Chapter 8 we use an intuitive notion of complexity. Indeed, the need of
an industry for practical cryptographic solutions pushed toward adopting an empirical
notion of secrecy: a cryptographic system provides secrecy until someone ﬁnds an
attack against it.

We recall that symmetric encryption relies on three algorithms:

r a key generator which generates a secret key in a cryptographically random or
r an encryption algorithm which transforms a plaintext into a ciphertext using a
r a decryption algorithm which transforms a ciphertext back into the plaintext

pseudorandom way;

secret key;

using the secret key.

Symmetric encryption is assumed to enable conﬁdential communications over an inse-
cure channel assuming that the secret key is transmitted over an extra secure channel.
Fig. 2.1 represents one possible use of this scheme. Here the secret key is transmitted
from the receiver to the sender in a conﬁdential way, and the adversary tries to get
information from the ciphertext only.

22

Chapter 2

Adversary



Plaintext

-

X

Encryption

Ciphertext

-

Y

-

Decryption

Y

-

X

6

K

CONFIDENTIAL

6

Key
K

Generator

Figure 2.1. Symmetric encryption.

2.1 The Data Encryption Standard (DES)

DES is the Data Encryption Standard which was originally published by the NBS—
National Bureau of Standards—a branch of the Department of Commerce in the USA.1
After a call for proposals, DES was originally proposed as a standard by IBM, based on a
previous cipher called LUCIFER developed by Horst Feistel. The US Government (and
particularly the NSA) contributed to the development. DES was adopted as a standard
and published in 1977 as a FIPS—Federal Information Processing Standard.2 It was de-
veloped based on the need for security in electronic bank transactions. NIST did not re-
new the standard in 2004 and so the standard is now over. It is however still widely used.

In the seventies, the information technology business was mostly driven by hard-
ware industry (and not by software and service companies). DES was intended for
hardware implementations.

DES is a block cipher: it enables the encryption of 64-bit block plaintexts into
64-bit block ciphertexts by using a secret key. It is thus a family of permutations over
the set of 64-bit block strings. Encryption of messages of arbitrary length is done
through a mode of operation which is separately standardized (see Section 2.2).

The secret key is also a 64-bit string, but eight of these bits are not used at all.

Therefore, we usually say that DES uses secret keys of length 56 bits.3

DES consists of a 16-round Feistel scheme. A Feistel scheme (named from the
author of the previous cipher LUCIFER which was based on a similar structure) is a
ladder structure which creates a permutation from a function. Actually, the input string
is split into two parts of equal length, and the image of one part through a round function

1 The NBS is now replaced by the NIST—National Institute of Standards and Technology.
2 The standard has been updated several times. The 1999 version is available as Ref. [5].
3 More precisely, the 64-bit key is represented as 8 bytes, and the most signiﬁcant bit of every byte may be

used for parity check.

Conventional Cryptography

23

K1

K2

K3

??
⊕

F
hhhhhh
((((((
??
⊕

F
hhhhhh
((((((
?
⊕

?

?
F

?

r

r

r

Figure 2.2. Function (F K1 , F K2 , F K3 ).

is XORed to the other part. We obtain two parts which are then exchanged (except in
the ﬁnal round). The round function uses subkeys derived from a secret key.

This elementary process is iterated, and the number of round function applications
is called the number of rounds. We usually denote (F1, . . . , Fr ) the permutation ob-
tained from an r-round Feistel scheme in which the round functions are F1, . . . , Fr . All
Fi may come from a single function F with a parameter Ki deﬁned by a subkey. We de-
note Fi = F Ki . Fig. 2.2 illustrates a 3-round Feistel scheme. DES consists of 16 rounds.

More precisely, DES starts by a bit permutation IP, performs the Feistel cipher
using subkeys generated by a key schedule, and ﬁnally performs the inverse of the IP
permutation. This is illustrated in Fig. 2.3.

K

?

Schedule

K1

K2

.
 
.
 
.



K16

?X

IP

?

Feistel

?
−1

IP

?Y

Figure 2.3. DES architecture.

24

Chapter 2

S1

S2

S3

S4

S5

S6

S7

S8

⊕

⊕⊕
⊕⊕⊕⊕
⊕⊕⊕⊕⊕
⊕⊕⊕
⊕⊕⊕
⊕⊕⊕
⊕⊕⊕
⊕⊕⊕⊕⊕
⊕⊕⊕⊕
⊕⊕
⊕⊕
⊕⊕⊕
⊕
⊕⊕⊕
⊕⊕⊕

⊕

Figure 2.4. DES round function.

The round function of DES has a main 32-bit input, a 48-bit subkey parameter
input, and a 32-bit output. For every round, the 48-bit subkey is generated from the
secret key by a key schedule. Basically, every 48-bit subkey consists of a permutation
and a selection of 48 out of the 56 bits of the secret key. As illustrated in Fig. 2.4, the
round function consists of

to get 48 bits,

r an expansion of the main input (one out of two input bits is duplicated) in order
r a XOR with the subkey,
r eight substitution boxes which transform a 6-bit input into a 4-bit output,
r a permutation of the ﬁnal 32 bits (which can be seen as a kind of transposition).

The substitution boxes (called S-boxes) have a 6-bit input and a 4-bit output. We
have eight S-boxes called S1, S2, . . . , S8. They are deﬁned by tables in the standard.
The tables however need to be read in a special way. For instance, S3 is deﬁned by

0

10
13
13
1

1

0
7
6
10

2

9
0
4
13

3

14
9
9
0

4

6
3
8
6

5

3
4
15
9

6

15
6
3
8

7

5
10
0
7

8

1
2
11
4

9

13
8
1
15

10

12
5
2
14

11

7
14
12
3

12

11
12
5
11

13

4
11
10
5

14

2
15
14
2

15

8
1
7
12

Conventional Cryptography

25

The 6-bit input b1b2b3b4b5b6 is split into two parts b1b6 and b2b3b4b5. The ﬁrst
part b1b6 indicates which line to read: 00 is the ﬁrst line, 01 the second, 10 the third,
and 11 the fourth. The second part b2b3b4b5 indicates which column to read in binary.
For instance, 0101 is column 5. The entry is the 4-bit output in decimal, to be converted
in binary. Hence the image of 001011 by S3 is 0100 (4).

The DES key schedule is done by the following algorithm. We use two registers C
and D of 28 bits. The 56 key bits from K are ﬁrst split into C and D following a ﬁxed
bit selection table PC1. Each round then rotates the bits in C and D by ri positions
depending on the round number i. (The ri ’s are also deﬁned by a table.) Then another
bit selection table PC2 takes 24 bits from each of the two registers and concatenates
them in order to make a round key.

PC1−→ (C, D)
for i = 1 to 16 do
C ← ROLri (C)
D ← ROLri (D)
Ki ← PC2(C, D)

1: K
2:
3:
4:
5:
6: end for

Here ROLr is a circular rotation of r bits to the left. The ri ’s are deﬁned by

i 1
ri 1

2
1

3
2

4
2

5
2

6
2

7
2

8
2

9
1

10
2

11
2

12
2

13
2

14
2

15
2

16
1

Note that the sum of all ri ’s is 28 so that we can generate the round keys in the decryption
ordering by starting with the same C and D and by running the loop backwards.

2.2 DES Modes of Operation

DES enables the encryption of 64-bit blocks. In order to encrypt plaintexts of arbi-
trary length, we have to use DES in a mode of operation. Several modes have been
standardized for DES (ECB, CBC, OFB, CFB, CTR) in Ref. [6].

2.2.1 Electronic Code Book (ECB)

The plaintext x is split into 64-bit blocks x1, . . . , xn, and the ciphertext y is the con-
catenation of encrypted blocks.

x = x1||x2||···||xn
y = C(x1)||C(x2)||···||C(xn)

(See Fig. 2.5.) There are a few security problems.

26

Chapter 2

x1
?
C

?
y1

x2
?
C

?
y2

x3
?
C

?
y3

. . .

. . .

Figure 2.5. ECB mode.

xn
?
C

?
yn

Information Leakage by Block Collisions
If two plaintext blocks are equal (say xi = x j ), then the two corresponding ciphertext
blocks are equal. The equality relation is an information which leaks.

This would not be a problem if the plaintext blocks were totally random as the
probability of equalities would be reasonably low. However, real plaintexts have lots of
redundancy in practice, so equalities are frequent.

Integrity Issues

Although encryption is assumed to protect conﬁdentiality, and not integrity, a third
party can intercept the ciphertext and permute two blocks. The legitimate recipient of
the modiﬁed ciphertext will decrypt the message correctly and obtain two permuted
plaintext blocks.

Similarly, a block can be deleted, replaced by another one, etc. The plaintext is

thus easily malleable by an adversary.

2.2.2 Cipher Block Chaining (CBC)

The plaintext x is split into 64-bit blocks x1, . . . , xn, and the ciphertext y is the concate-
nation of blocks which are obtained iteratively. We have an initial vector IV which is a
fake initial block. As illustrated in Fig. 2.6, encryption is performed by the following
rules.

x = x1||x2||···||xn
y0 = IV
yi = C(yi−1 ⊕ xi )
y = y1||y2||···||yn

CBC decryption is easily performed by the following rules.

y = y1||y2||···||yn
y0 = IV
xi = yi−1 ⊕ C
−1(yi )
x = x1||x2||···||xn

Conventional Cryptography

27

-

IV

-

x1
?⊕
?
C

?
y1

x2
- ?⊕
?
C

x3
- ?⊕
?
C

?
y2

?
y3

Figure 2.6. CBC mode.

xn
?⊕
?
C

?
yn

The initial vector does not have to be secret. There are actually four different ways to
use the IV.

1. Generate a pseudorandom IV which is given in clear with the ciphertext.
2. Generate a pseudorandom IV which is transmitted in a conﬁdential way.
3. Use a ﬁxed IV which is a known constant.
4. Use a ﬁxed IV which is another part of the secret key.

The US standards recommend one of the two ﬁrst solutions.

There are a few security problems.

Information Leakage by First Block Collisions

If for two different plaintexts the ﬁrst blocks x1 are the same and the IV is ﬁxed, then
there is still a leakage of the equality of these blocks. This is why we prefer having a
random IV.

Integrity Issues

A third party can replace ciphertext blocks so that all but a few plaintext blocks will
decrypt well. This may be an integrity problem.

2.2.3 Output Feedback (OFB)

The plaintext x is split into (cid:9)-bit blocks x1, . . . , xn, and the ciphertext y is the concate-
nation of blocks which are obtained iteratively. We still have an initial vector IV. As
depicted in Fig. 2.7, the encryption obeys the following rules.

x = x1||x2||···||xn
s1 = IV
ri = truncL(cid:9)(C(si ))
si+1 = truncR64(si||ri )

28

Chapter 2

si−1

(cid:1)

?

(cid:1)

?
si

?

C

?

ri

(cid:1)

?
- ⊕

?
yi

xi

Figure 2.7. OFB mode.

yi = xi ⊕ ri
y = y1||y2||···||yn

Here truncL(cid:9) truncates the (cid:9) leftmost bits, and truncR64 truncates the 64 rightmost bits.
When (cid:9) is set to the full block length (here 64 bits), the description of the OFB mode is
quite simple as illustrated in Fig. 2.8. Note that it is not recommended to use (cid:9) smaller
than the block length due to potential short cycles (see Ref. [57]).

Actually, the OFB mode can be seen as a pseudorandom generator mode which is
followed by the one-time pad. Here IV must be used only once (otherwise the cipher
is equivalent to a one-time pad with a key used several times). The IV does not have to
be secret.

x1

x2

x3

IV

?
C

?
C

?
C

?

-

⊕?
?
y1

-

⊕?
?
y2

-

⊕?
?
y3

Figure 2.8. OFB mode with (cid:9) set to the block length.

···

···

xn

-

⊕?
?
yn

Conventional Cryptography

29

si−1



(cid:1)

(cid:1)


si

?
C

?

(cid:1)ri

?
- ⊕

xi

?
yi

Figure 2.9. CFB mode.

2.2.4 Cipher Feedback (CFB)

The plaintext x is split into (cid:9)-bit blocks x1, . . . , xn, and the ciphertext y is the concate-
nation of blocks which are obtained iteratively. We still have an initial vector IV. As
depicted in Fig. 2.9, the encryption is according to the following rules.

x = x1||x2||···||xn
s1 = IV
ri = truncL(cid:9)(C(si ))
yi = xi ⊕ ri
si+1 = truncR64(si||yi )
y = y1||y2||···||yn

The simple version of the CFB mode with (cid:9) set to the block length (here 64 bits) is
depicted in Fig. 2.10. As for the OFB mode and since the ﬁrst block is encrypted by a
one-time pad, IV need not be secret, but must be fresh (i.e. used only once).

x1
?⊕

-

?
y1

x2
?⊕

-

?
y2

C
6

x3
?⊕

-

?
y3

C
6

···

···

6

Figure 2.10. CFB mode with (cid:9) Set to the block length.

C
6

IV

xn
?⊕

-

?
yn

30

2.2.5 Counter Mode (CTR)

Chapter 2

The plaintext x is split into (cid:9)-bit blocks x1, . . . , xn, and the ciphertext y is the concate-
nation of blocks which are obtained iteratively. We use a sequence t1, . . . , tn of counters
and the encryption is performed by

yi = xi ⊕ truncL(cid:9)(C(ti )).

For a given key, all counters must be pairwise different. For this we can, for instance,
let ti be equal to the binary representation of t1 + (i − 1) so that each ti “counts” the
block sequence. The initial counter t1 can either be equal to the latest used counter
value stepped by one unit or include a nonce which is speciﬁc to the plaintext. In the
latter case nonces must be pairwise different.

In Fig. 2.11 the CTR mode with (cid:9) set to the block length of C is depicted.

2.3 Multiple Encryption

DES relies on a secret key of 56 effective bits, which is rather short. To
strengthen its security, people suggested to use multiple DES encryption with several
keys.

2.3.1 Double Mode

A ﬁrst proposal was to use a double mode following the regular product cipher:

Enc = Ck1

◦ Ck2

One security problem is that we may face meet-in-the-middle attacks (see Section 2.9.5).
For this reason double modes are not recommended.

t1
?
C

x1

-

⊕?
?
y1

t2
?
C

x2

x3

t3
?
C

-

⊕?
?
y2

-

⊕?
?
y3

···

···

tn
?
C

xn

-

⊕?

?
yn

Figure 2.11. CTR mode with (cid:9) set to the block length.

Conventional Cryptography

2.3.2 Triple Mode

31

Since the double mode does not improve security so much, we use a standard triple
mode. The regular triple-DES is deﬁned by three 56-bit keys k1, k2, and k3 by

Enc = Ck3

◦ C

−1
k2

◦ Ck1

.

(See Ref. [5].) In addition, three keying options are deﬁned:

1. k1, k2, k3 are three independent keys (the key length is thus 168 bits);
2. k1 = k3 and k2 are two independent keys (the key length is thus 112 bits);
3. k1 = k2 = k3 which is equivalent to DES in simple mode. (This is a kind of

retrocompatibility with DES.)

Note that some advanced brute force attacks against triple modes exist as well.

2.4 An Application of DES: UNIX Passwords

A famous application of DES is the old UNIX CRYPT algorithm.4 It is used for access
control of users based on passwords.

Basically, the “encrypted” version of passwords is stored in a database /etc/
passwd whose conﬁdentiality was not originally meant to be ensured. Whenever
a user types his login name and password, the system “encrypts” the password and
compares it to the “encrypted” password stored in the corresponding record of the
database. The encryption is based on modiﬁed DES due to the following observations
(see Fig. 2.12).

W (56)

?

?

0 - ≈DES - ≈DES -

···

6

6

?
- ≈DES

6

- /etc/passwd

6

Salt (12)

6

Clock

Figure 2.12. UNIX passwords.

4 A summary of the speciﬁcation can be found in Ref. [128], pp. 393–394.

32

r We do not want the security to rely on the secrecy of a critical secret key. Thus we

Chapter 2

want to make the decryption impossible even with full knowledge. Thus we use
DES in a kind of a one-way mode: instead of computing C(W ) for a password
W used as a plaintext, we compute CW (0) on the null plaintext with W used as
a key. (W is truncated onto its ﬁrst eight characters. It must consist of ASCII
characters, thus 7-bit long, which makes 56 bits.)

r In order to make the exhaustive search more lengthy, we use a more complicated

encryption. This can be tolerated for human user authentication as long as it does
not require more than a fraction of a second. Thus we use 25 iterations of DES
with the password used as a secret key.

r In order to prevent brute force attacks based on mass manufactured DES chips,
r In order to thwart attacks based on precomputed tables, the modiﬁcation of

we modify DES in order to make these chips unusable.

DES involves a random 12-bit salt which is stored in clear with the encrypted
password. Actually, some of the 48 bits of the expanded block are swapped
depending on the salt. The 12-bit salt is generated from the system clock when
the password is set up.

2.5 Classical Cipher Skeletons

Many block ciphers are described in the literature. We survey classical design skeletons.

2.5.1 Feistel Schemes

The Feistel scheme is the most popular block cipher skeleton. It is fairly easy to use
a random round function in order to construct a permutation. In addition, encryption
and decryption hardly need separate implementations.

An example is DES, described in Section 2.1.

Here are some possible generalizations of the Feistel scheme.

r We can add invertible substitution boxes in the two branches of the Feistel scheme
r We can replace the XOR by any other addition law. We do not necessarily
(as done in the BLOWFISH cipher).
need commutativity nor associativity: only regularity (like a ∗ x = a ∗ y implies
x = y).
r We do not need to have balanced branches. We may also have unbalanced ones
r We can generalize the scheme so that it has more than two branches:

(like in the BEAR and LION cipher).

(a) round functions with one input and several outputs (like in MARS),
(b) round functions with several inputs and one output (like in MD4),
(c) round functions with several inputs and outputs.

The ﬁrst three variants are illustrated in Fig. 2.13.

Conventional Cryptography

33

?

F



-

?

G

?

F



?
⊕


π

?
π

?
∗



?

K1

K2

K3

?
σ

-

?
+

σ

?

Figure 2.13. Variants of the Feistel scheme with two branches.

2.5.2 Lai–Massey Scheme

A famous block cipher which is not based on the Feistel scheme is the IDEA cipher.
IDEA stands for International Data Encryption Algorithm. It follows two previous
versions called PES (Proposed Encryption Standard) and IPES (Improved Proposed
Encryption Standard). It was developed during the PhD studies of Xuejia Lai under
the supervision of James Massey at the ETH Z¨urich. IDEA was published in Lai’s
thesis (Ref. [110]) in 1992. It is patented by Ascom and made freely available for
noncommercial use.5

Like DES, IDEA is a block cipher for 64-bit blocks. IDEA uses much longer keys
than DES as it allows for 128-bit keys. In the same way that DES was dedicated to
hardware, IDEA was dedicated to software implementation on 16-bit microprocessors
(which used to be a luxurious architecture in the early nineties). It makes an extensive
use of the XOR, the addition modulo 216, and the product of nonzero residues modulo
216 + 1.

IDEA uses a structure similar to the Feistel scheme which can be called the
Lai–Massey scheme. It also enables making a permutation from a function. It however
requires a two-branch balanced structure and a commutative and associative law like the
XOR operation. As depicted in Fig. 2.14, it simply consists of adding to both branches
the output of a round function whose input is the difference of the two branches: a

5 The commercial development of IDEA is currently managed by the company MediaCrypt.

34

Chapter 2

−- 
?

F

−- 
?

F

−- 
?

F

?

+

?

+

?

+
?

K1

-

?
+

K2

-

?
+

K3

-

?
+
?

Figure 2.14. The Lai–Massey scheme.

(xL , x R) pair is mapped onto the (yL , yR) pair deﬁned by

yL = xL + t
yR = x R + t

where t = F(xL − x R). Note that this scheme is invertible by replacing the + by − and
changing the order of the subkeys. Unfortunately, the Lai–Massey scheme cannot be
used as is since the pair difference is an invariance: we have

xL − x R = yL − yR.

This is obviously an unsuitable property for security. For this reason we must insert
(at least) a permutation σ as depicted in Fig. 2.15 and have

yL = σ (xL + t)
yR = x R + t

as it will be detailed in Section 2.6.1 for the FOX algorithms. When the permutation σ
is such that z (cid:11)→ σ (z) − z is also a permutation, we say that σ is an orthomorphism for
the + law. We can demonstrate that when σ is an orthomorphism, then the Lai–Massey
scheme provides security properties which are similar to those for the Feistel scheme.
So the invariance of the basic Lai–Massey scheme is no longer a problem. In IDEA,
key-dependent permutations (namely, products and additions) are used instead of a
ﬁxed σ .

IDEA consists of eight rounds. One round is as represented in Fig. 2.16. The· repre-
sents the multiplication modulo 216 + 1 to a subkey, the+ is the regular addition modulo
216 to a subkey,⊕ is the bitwise XOR, and MA is the Multiplication–Addition structure,
which is depicted in Fig. 2.17. The MA structure also requires multiplication to subkeys.
The addition law which is used in the Lai–Massey scheme of IDEA is the XOR.

Conventional Cryptography

35

−- 
?

F

−- 
?

F

−- 
?

F

?

+
?
σ

?

+
?
σ

?

+
?

K1

-

?
+

K2

-

?
+

K3

-

?
+
?

Figure 2.15. The Lai–Massey scheme with orthomorphism σ .

? ?
·
 
+

? ?
·
 
+

-

⊕


-

⊕
??





MA

?
⊕



?
⊕



?
⊕

-

?
⊕

-

? ?

? ?

Figure 2.16. One round of IDEA.

-

-

? ?
·
+
? ?
·
+
? ?





Figure 2.17. The MA structure in IDEA.

36

Chapter 2

2.5.3 Substitution–Permutation Network

Shannon originally deﬁned the encryption as a cascade of substitutions (like the Caesar
cipher, or like the S-boxes in DES) and permutations (or transpositions, like the Spartan
scytales, or the bit permutation after the S-boxes in DES). Therefore, many block
ciphers ﬁt to the category of substitution–permutation networks. However, this term
was improperly used in order to refer to cascade on invertible layers made from invertible
substitutions of coordinate permutations. Feistel schemes and Lai–Massey schemes are
not considered to belong to this category in general.

SAFER K-64 is an example of a substitution–permutation network. It was made by
James Massey for Cylink and was published in 1993 (see Refs. [121, 122]). It encrypts
64-bit blocks with 64-bit keys and is dedicated to 8-bit microprocessors (which are
widely used in embedded system, for instance in smart cards). It uses XORs and
additions modulo 28. It also uses exponentiation in basis 45 in the set of residues
modulo 257 and its inverse which are implemented with lookup tables.

SAFER K-64 is a cascade of six rounds which consists of

r a layer of XOR or addition to subkeys,
r a layer of substitutions (exponentiation or logarithms as above),
r a layer of XOR or addition to subkeys,
r three layers of parallel linear diffusion boxes which make an overall transforma-

tion similar to the fast Fourier transform.

(See Fig. 2.18.) Diffusion boxes consist of mappings denoted by 2-PHT which are linear
with respect to the Z256 structure. They are represented with their inverse in Fig. 2.19.

? ? ? ? ? ? ? ?
⊕ + + ⊕ ⊕ + + ⊕

? ? ? ? ? ? ? ?

E

L

L

E

E

L

L

E

? ? ? ? ? ? ? ?
+ ⊕ ⊕ + + ⊕ ⊕ +

? ? ? ? ? ? ? ?
2−PHT

2−PHT

2−PHT

2−PHT

	

?
2−PHT

)

q


2−PHT

j
2−PHT

R

2−PHT

?

	

)

q


2−PHT

?
2−PHT
? ? ? ? ? ? ? ?

j
2−PHT

2−PHT

R

?

Figure 2.18. One round of SAFER.

Conventional Cryptography

37

x

y

R 	
2−PHT
	 R

u = 2x + y mod 256

v = x + y mod 256

v

u
R 	
2−PHT
−1
	 R

x = u− v mod 256

y = 2v− u mod 256

Figure 2.19. Diffusion in SAFER.

2.6 Other Block Cipher Examples

2.6.1

(cid:1)FOX: A Lai–Massey Scheme

FOX is a family of block ciphers which was released in 2003 (see Refs. [96, 97]). It
was designed by Pascal Junod and Serge Vaudenay for the MediaCrypt company. The
family includes block ciphers with 64-bit and 128-bit blocks. Round numbers and key
sizes are ﬂexible. We use an integral number r of rounds between 12 and 255 and a key
of k bits with an integral number of bytes, up to 256 bits. The name FOX64/k/r refers
to the block cipher of the family characterized by 64-bit blocks, r rounds, and keys of k
bits. Similarly, FOX128/k/r refers to the block cipher with 128-bit blocks. The nominal
choices denoted by FOX64 and FOX128 refer to FOX64/128/16 and FOX128/256/16
respectively. Namely, we use r = 16 as a nominal number of rounds and a key length
which corresponds to two blocks.

A key schedule processes the key K and a direction (either “encrypt” or “decrypt”)
and produces a sequence RK1, . . . , RKr of r round keys in this ordering if the direction
is “encrypt” or the opposite if the direction is “decrypt.” Encryption is performed
through r rounds as depicted in Fig. 2.20. Every round processes a data block and a
round key RK (whose size consists of two blocks) and produces another data block.
The r − 1 ﬁrst rounds have identical structure but the last round is a little different.

K

?

Direction

?

Key

schedule

 RK1

 RK2

...



RKr

?
Round

?
Round
?

.
 
.
 
.

?
Round

∗

?

Figure 2.20. The FOX skeleton.

38

Chapter 2

- ⊕

- ⊕
?
?

f32 

RKi

?
⊕


?
⊕


?⊕

? ?

-

?
⊕

-

?
⊕

? ?

Figure 2.21. One round of FOX64 with an orthomorphism.

The FOX64 round is a Lai–Massey scheme as deﬁned in Section 2.5.2 with the
XOR as the addition law and an orthomorphism as depicted in Fig. 2.21. Note that
branches in the Lai–Massey scheme are split into two in the ﬁgure, leading us to four
branches in total. The orthomorphism appears in the bottom left branches (circled in
the ﬁgure). It maps (a, b) onto (b, a ⊕ b) for the encryption and onto (a ⊕ b, a) for
the decryption. The last round of FOX64 is the same Lai–Massey scheme without
the orthomorphism. The FOX128 round is an extended Lai–Massey scheme with two
orthomorphisms as depicted in Fig. 2.22. The last round omits the orthomorphisms.
With this design we easily demonstrate that ﬂipping the key schedule direction effects
two permutations which are the inverse of each other.

Round functions are denoted f 32 and f 64 for FOX64 and FOX128 respectively.
Those functions process a data of 32 and 64 bits respectively and a round key RKi
which is split into two halves RKi0 and RKi1. As depicted in Figs. 2.23 and 2.24, RKi0
is ﬁrst XORed to the input data. Then a byte-wise substitution is performed using a
substitution box denoted S-box followed by a linear transform denoted mu4 and mu8

- ⊕

- ⊕

- ⊕

- ⊕

?

?

?

?

f64



?
⊕


?
⊕


?⊕

? ?

-

?
⊕
-

?
⊕

? ?

?
⊕


?
⊕


?⊕

? ?

Figure 2.22. One round of FOX128 with orthomorphisms.

RKi

-

?
⊕
-

?
⊕

? ?

Conventional Cryptography

39

RKi0

RKi1

⊕?
?
S-box
?

⊕?
?
S-box
⊕?
?

⊕?
?
S-box
?

⊕?
?
S-box
?

mu4

⊕?
?
S-box
⊕?
?

⊕?
?
S-box
⊕?
?

⊕?
?
S-box
?

⊕?
?
S-box
⊕?
?

Figure 2.23. Round function f32 of FOX64.

respectively. Then RKi1 is XORed with the output of mu4 (or mu8) and another byte-
wise substitution takes place. Finally, a last XOR to RKi0 is performed. Functions mu4
and mu8 are linear in the sense that they process vectors of bytes that are considered
as elements of the ﬁnite ﬁeld GF(28) by multiplying them with a constant matrix.

The key schedule of FOX highly depends on the parameters. The main idea, as
depicted in Fig. 2.25, consists of ﬁrst padding the key with some constant in order to
get a 256-bit key, then mixing those bytes in order to avoid trailing constant bytes, and
obtain a 256-bit main key. This key is XORed to constants which are generated by a
linear feedback shift register (LFSR) which can be clocked in one direction or the
other. The XOR is then processed through a nonlinear (NL) function which produces
a round key. There are some subtleties depending on the parameters.

⊕?
?

⊕?
?

⊕?
?

⊕?
?

⊕?
?

⊕?
?

⊕?
?

⊕?
?

S-box
?

S-box
?

S-box
?

S-box
?

S-box
?

S-box
?

S-box
?

S-box
?

mu8

⊕?
?

S-box
⊕?
?

⊕?
?

S-box
⊕?
?

⊕?
?

S-box
⊕?
?

⊕?
?

S-box
⊕?
?

⊕?
?

S-box
⊕?
?

⊕?
?

S-box
⊕?
?

⊕?
?

S-box
⊕?
?

⊕?
?

S-box
⊕?
?

Figure 2.24. Round function f4 of FOX128.

RKi0

RKi1

40

Chapter 2

K
?
Pad
?
Mix

?
⊕


?
⊕






NL

NL

Direction

?

LFSR









?
⊕


NL
...
Figure 2.25. Key schedule of FOX.

...

r For FOX64 with key of length up to 128 bits, there is no real need for having

a 256-bit main key and this actually induces a penalty for the implementation
performances. Indeed we use a 128-bit main key and LFSR and NL functions
updated accordingly.

r When the key has a “full size,” i.e. k = 256, or k = 128 with FOX64, there is no

need for padding and byte mixing. Indeed, we omit them. In order to avoid key
schedule interference between several kinds of keys, we slightly modify NL.

It should be noted that NL is deﬁned by using functions which are similar to en-
cryption rounds. It was designed in order to be “one way” and to generate unpredictable
round keys.6

2.6.2

(cid:1)CS-CIPHER: A Substitution–Permutation Network

Another example is the CS-CIPHER (CSC) which was developed by Jacques Stern and
Serge Vaudenay at the Ecole Normale Sup´erieure for the company Communication &
Systems. It was published in 1998 (see Refs. [176, 181]). It encrypts 64-bit blocks with
keys of variable length from 0 to 128 bits and is dedicated to 8-bit microprocessors,
and consists of eight rounds of fast Fourier transform (FFT)-like layers (see Fig. 2.26).
The difference with SAFER is that this transform is not linear.

One round of CSC is an FFT-like layer with a mixing box M as an elementary
operation. M has two input bytes and two output bytes. It includes a one-position bitwise
rotation to the left (denoted ROTL), XORs (denoted with the ⊕ notation), a nonlinear
permutation P deﬁned by a table, and a special linear transform ϕ deﬁned by

ϕ(x) = (ROTL(x) AND 55) ⊕ x

6 See Ref. [96] for a complete description.

Conventional Cryptography

41

? ? ? ? ? ? ? ?
⊕ ⊕ ⊕ ⊕ ⊕ ⊕ ⊕ ⊕

 ki

M

M

M

M

-c

-

(cid:5)

c

?
?
⊕ ⊕ ⊕ ⊕ ⊕ ⊕ ⊕ ⊕

R

q

j



)

	

M

M

M

M

?
?
⊕ ⊕ ⊕ ⊕ ⊕ ⊕ ⊕ ⊕

R



)

j

	

q

M

M

M

M

?

	



)

q

j

R

?

Figure 2.26. One round of CS-CIPHER.

where AND is the bitwise logical AND and 55 is an hexadecimal constant which is
01010101 in binary. We notice that ϕ is linear, and actually an involution since

ϕ(ϕ(x)) = (ROTL(ϕ(x)) AND 55) ⊕ ϕ(x)

= x.

Thus ϕ is a linear permutation. The permutation P is deﬁned in order to be a nonlinear
involution:

P(P(x)) = x.

We can then ﬁnally deﬁne M. Fig. 2.27 represents M with the XOR with subkey bytes
at the input. It is easy to see that Fig. 2.28 represents the inverse transform where ϕ(cid:8)
is
deﬁned by

ϕ(cid:8)

(x) = (ROTL(x) AND aa) ⊕ x.

xl

xr

xl

xr

⊕
?
-kl
?

⊕
?
kr
?

M

?
yl

?
yr

ROTL

⊕
?
-kl
R
ϕ?
⊕
?
	
?
P
?
yl

⊕
?
kr

⊕
?
R
?
P
?
yr

Figure 2.27. The mixing box of CSC.

42

Chapter 2

yl
?

yr
?

−1

M

⊕
?
-kl

?
xl

⊕
?
kr

?
xr

yr
?
P

⊕?
kr

yl
?
P

	?
⊕
?
ϕ(cid:5)

R

ROTL

⊕
?
-kl
?
xl

⊕
?
R
?
xr

Figure 2.28. The invert mixing box of CSC.

For completeness we also provide a complete view of CSC in Fig. 2.29. We see

that the key schedule is actually deﬁned by a Feistel scheme.

2.7 The Advanced Encryption Standard (AES)

With the improvement of computer technology due to the Moore law, the security of
DES is no longer appropriate for electronic commerce. The US Government decided
to restart a standardization process called the Advanced Encryption Standard (AES)

k−1

k−2

k0

k1

k2

k3

k4

k5

k6

k7

k8

? ?
⊕

F
? ?
⊕

F
? ?
⊕

F
? ?
⊕

F
? ?
⊕

F
? ?
⊕

F
? ?
⊕

F
? ?
⊕

F
?
⊕
? F


c0

c1

c2

c3

c4

c5

c6

c7

c8

x
⊕
?

E
⊕
?

E
⊕
?

E
⊕
?

E
⊕
?

E
⊕
?

E
⊕
?

E
⊕
?

E
⊕
?

y

Figure 2.29. External view of CSC.

Conventional Cryptography

43

in 1997. This process was open: anyone was invited to submit a candidate and to send
public comments. Fifteen candidates were accepted (a few other submissions did not
meet the requirements and were rejected) in 1998. Based on public comments (and
apparently on popularity), this pool was downsized to ﬁve ﬁnalists in 1999. In October
2000, one of these ﬁve algorithms was selected as the forthcoming standard: Rijndael
(see Refs. [1, 54]).

Rijndael was designed by Joan Daemen (from the Belgium company Proton World
International) and Vincent Rijmen. They both originated from the Catholic University of
Leuven. Rijndael was designed for the AES process. Following the AES requirements, it
encrypts 128-bit blocks with keys of size 128, 192, or 256. It is dedicated to 8-bit micro-
processors. It consists of several rounds of a simple substitution–permutation network.

AES is based on the structure of SQUARE.7 This design simply consists of writ-
ing the 128-bit message block as a 4 × 4 square matrix of bytes. (Formally, Rijndael
tolerates other block sizes, but 128-bit was the target block size for AES.) Encryption
is performed through 10, 12, or 14 rounds depending on whether the key size is 128,
196, or 256 bits. The number of rounds is denoted by Nr. Each round (but the ﬁnal one)
consists of four simple transformations:

1. SubBytes, a byte-wise substitution deﬁned by a single table of 256 bytes,
2. ShiftRows, a circular shift of all rows (row number i of the matrix is rotated by

i positions to the left for i = 0, 1, 2, 3),
by a 4 × 4 matrix of GF(28) elements,

3. MixColumns, a linear transformation performed on each column and deﬁned

4. AddRoundKey, a simple bitwise XOR with a round key deﬁned by another

matrix.

The ﬁnal round is similar, except for MixColumns which is omitted. The round keys
are generated by a separate key schedule.

More formally, one block s is encrypted by the following process, in which w is

the output subkey sequence from the key schedule algorithm.

SubBytes(s)
ShiftRows(s)

AES encryption(s, W )
1: AddRoundKey(s, W0)
2: for r = 1 to Nr − 1 do
3:
4:
5: MixColumns(s)
6:
7: end for
8: SubBytes(s)
9: ShiftRows(s)
10: AddRoundKey(s, WNr)

AddRoundKey(s, Wr )

7 SQUARE was designed by the same authors and Lars Knudsen in 1997 (see Ref. [55]).

44
Chapter 2
The block s is also called state and represented as a matrix of terms si, j for i, j ∈
{0, 1, 2, 3}. Terms are bytes, i.e. elements of a set Z of cardinality 256. SubBytes is
deﬁned as follows.

for j = 0 to 3 do

SubBytes(s)
1: for i = 0 to 3 do
2:
3:
4:
5: end for

si, j ← S-box(si, j )
end for

Here S-box is the substitution table. Mathematically,
{0, 1, . . . , 255}. ShiftRows is deﬁned as follows.

it

is a permutation of

ShiftRows(s)

1: replace [s1,0, s1,1, s1,2, s1,3] by [s1,1, s1,2, s1,3, s1,0]

2: replace [s2,0, s2,1, s2,2, s2,3] by [s2,2, s2,3, s2,0, s2,1]

{rotate row 1 by one position to the left}
{rotate row 2 by two positions to the left}
{rotate row 3 by three positions to the left}

3: replace [s3,0, s3,1, s3,2, s3,3] by [s3,3, s3,0, s3,1, s3,2]

We deﬁne the set Z as the set of all the 256 possible combinations

a0 + a1.x + a2.x 2 + ··· + a7.x 7

where a0, a1, a2, . . . , a7 are either 0 or 1 and x is a formal term. Elements of Z are thus
deﬁned as polynomials of degree at most 7. AddRoundKey is deﬁned as follows.

AddRoundKey(s, k)
1: for i = 0 to 3 do
2:
3:
4:
5: end for

for j = 0 to 3 do
si, j ← si, j ⊕ ki, j
end for

Here the ⊕ operation over Z is deﬁned as an addition modulo 2, i.e.

(cid:2)

7(cid:1)

i=0

(cid:3)

(cid:2)

7(cid:1)

i=0

(cid:3)

= 7(cid:1)

i=0

⊕

ai .x i

bi .x i

(ai + bi mod 2).x i .

A multiplication × in Z is further deﬁned as follows.

Conventional Cryptography

45

1. We ﬁrst perform the regular polynomial multiplication.
2. We make the Euclidean division of the product by the x 8 + x 4 + x 3 + x + 1

polynomial and we take the remainder.

3. We reduce all its terms modulo 2.

Later in Chapter 6 we will see that this provides Z with the structure of the unique
ﬁnite ﬁeld of 256 elements. This ﬁnite ﬁeld is denoted by GF(28). This means that we
can add, multiply, or divide by any nonzero element of Z with the same properties that
we have with regular numbers. We can further deﬁne matrix operations with terms in
Z. We can then deﬁne MixColumns as follows.

MixColumns(s)
1: for i = 0 to 3 do
2:
3:
4: end for

let v be the 4-dimensional vector with coordinates s0,i , s1,i s2,i s3,i
replace s0,i , s1,i s2,i s3,i by the coordinates of M × v

Here M is a 4 × 4-matrix over Z deﬁned by
x + 1
x
1
1

⎛
⎜⎜⎝ x

M =

1
1
x + 1

⎞
⎟⎟⎠ .

1
x + 1
x
1

1
1
x + 1
x

The substitution table S-box is deﬁned by the inversion operation x

−1 (except for
x = 0 which is mapped to zero) in the ﬁnite ﬁeld GF(28). This operation has good
nonlinear properties. In order to “break” the algebraic structure of this table, an afﬁne
transformation is added on this function.

The linear transformation in MixColumns is deﬁned by a matrix following prin-
ciples similar to the mixing box of CSC (see Section 2.6.2): whenever i input bytes of
this linear transformation are modiﬁed, we make sure that this induces a modiﬁcation
of at least 5 − i output bytes.8

We complete the description of AES by outlining the key expansion. It is easier to
consider W as a row sequence (i.e. four bytes) of length 4Nr starting by w0 and up to
w4Nr−1. Hence

Wi = [w4i , w4i+1, w4i+2, w4i+3].

The key expansion proceeds with a key described as a sequence of Nk rows (i.e. Nk is
either 4, 6, or 8) starting from key0. The expansion works as follows.

8 Equivalently, the set of all (x, M(x)) 8-byte vectors is an MDS code if M denotes the linear transformation,

or in other words, M is a multipermutation.

46

Chapter 2

wi ← keyi

KeyExpansion(key, Nk)
1: for i = 0 to Nk − 1 do
2:
3: end for
4: for i = Nk to 4 × ( Nr + 1) − 1 do
5:
6:
7:
8:
9:

t ← wi−1
if i mod Nk = 0 then
replace [t0, t1, t2, t3] by [t1, t2, t3, t0] in t
apply S-box to the four bytes of t
XOR x i /Nk−1 (raise the polynomial x to the power i /Nk − 1
in GF(28)) to the ﬁrst byte of t

else if Nk = 8 and i mod Nk = 4 then
apply S-box to the four bytes of t

10:
11:
12:
13: wi ← wi−Nk⊕t
14: end for

end if

2.8 Stream Ciphers

2.8.1 Stream Ciphers versus Block Ciphers

All conventional encryption schemes that we have seen so far are block ciphers in the
sense that they encrypt blocks of plaintexts. They are often opposed to stream ciphers
which encrypt streams of plaintext on the ﬂy. A stream cipher often encrypts streams of
plaintext bits, or streams of plaintext bytes. This distinction is often misleading since
block ciphers are used as well in a mode of operation so that they can encrypt streams
of blocks. Nevertheless, we will call block cipher an encryption scheme in which the
underlying primitive is deﬁned on a large ﬁnite set (of “blocks”) which cannot be
enumerated exhaustively in practice. With this deﬁnition we cannot assimilate a bit or
a byte to a block. Conversely, we call stream cipher an encryption scheme which can
encrypt streams of information in a smaller ﬁnite set.

First of all we notice that we can transform a pseudorandom generator into a stream
cipher. Stream ciphers are indeed often deﬁned by a key-stream generator which is used
as a one-time pad: instead of having a large random key for the Vernam cipher, we use
a pseudorandom key which is generated as a key-stream.

We also notice that we can transform a block cipher into a stream cipher by using
the CFB, or CTR mode, with a small parameter (cid:9)9 (see Sections 2.2.3, 2.2.4, and 2.2.5).

2.8.2 RC4

RC4 is an encryption algorithm which was designed in 1987 by Ronald Rivest at MIT.
It was kept as a commercial secret until it was disclosed in 1994. In particular there is no

9 It is not recommended to do the same for OFB.

Conventional Cryptography

47

patent on RC4, but RC4 is a registered trademark of RSA Data Security. RC4 is widely
used, for instance in SSL/TLS (see Section 12.3). In particular, some Internet browsers
and servers may use RC4 as a default encryption algorithm for protected transactions.

RC4 works as a ﬁnite automaton with an internal state. It reads a plaintext as a byte
stream and produces a ciphertext as a byte stream. Its heart is actually a key-stream
generator which is used for the one-time pad algorithm. In an initialization stage, a
secret key is processed without producing keys. The automaton ends up in an internal
state which is thus uniquely derived from the secret key only. Then, every time unit,
the automaton updates its internal state and produces a key byte which is XORed to a
plaintext byte in order to lead to a ciphertext byte.

We consider the set {0, 1, . . . , 255} of all bytes. The internal state consists of
two bytes i and j and a permutation S of this set which is encoded as an array
S[0], S[1], . . . , S[255]. All operations are done on bytes (i.e. additions are taken
modulo 256).

In the initialization, we process a key which is represented as a sequence
K [0], K [1], . . . , K [(cid:9) − 1] of (cid:9) bytes. The internal state is ﬁrst initialized as fol-
lows. Byte j is set to 0, and the permutation S is set to the identity, i.e. S[i] = i
for i = 0, 1, . . . , 255. Key bytes are then iteratively processed, and the bytes i and j
are reset to 0.
1: j ← 0
2: for i = 0 to 255 do
3:
4: end for
5: for i = 0 to 255 do
6:
7:
8: end for
9: i ← 0
j ← 0
10:

j ← j + S[i] + K [i mod (cid:9)]
swap S[i] and S[ j]

S[i] ← i

The key size (cid:9) is typically between 5 and 16 bytes (i.e. between 40 and 256 bits).

It is important that we never use the same state twice. Thus, plaintexts are iteratively
encrypted, which means that the initial state for a new plaintext is equal to the ﬁnal
state for the previous plaintext.

The key-stream generator works as follows. Every time unit, we perform the

following sequence of instructions.

1: i ← i + 1
2: j ← j + S[i]
3: swap S[i] and S[ j]
4: output S[S[i] + S[ j]]

48
Chapter 2
Thus we update i, j, and S, and we output a byte which is given by S at index S[i] + S[ j].

2.8.3 A5/1: GSM Encryption

A5/1 is another stream cipher which is part of the A5 family. It is used in the GSM
mobile telephone networks. It is used in order to secure phone calls in the radio link
from the mobile telephone to the base station. It was designed by the SAGE group of
ETSI. The description of A5/1 is another trade secret, but the algorithm was reverse-
engineered and published in the Internet. It is commonly admitted that this description
is similar to the ETSI one.

A5/1 is also based on a ﬁnite automaton with an internal state. As depicted in
Fig. 2.30, A5/1 is based on three LFSRs with a mutual clock control. The three registers
R1, R2, R3 contain 19, 22, and 23 bits respectively. The internal state thus has 19 +
22 + 23 = 64 bits. Every time unit, some registers are clocked and some may not be
clocked at all. When a register is clocked, it means that its content is shifted by one
bit position and that a new bit is pushed. This new bit is the XOR of a few bits of the
involved LFSRs.

More precisely, R1 has 19 bits R1[0], . . . , R1[18]. When R1 is clocked, the content
R1[0], . . . , R1[18] is replaced by b, R1[0], . . . , R1[17], i.e. R1 is shifted by inserting a
new bit b which is computed by

b = R1[13] ⊕ R1[16] ⊕ R1[17] ⊕ R1[18].

R2 has 22 bits R2[0], . . . , R2[21]. When R2 is clocked, it is similarly shifted by inserting
a new bit

×

×







- ?⊕ ?⊕ - ?⊕

?
⊕
6

- ?⊕

- ?⊕ ?⊕

×

- ?⊕

Figure 2.30. A5/1 automaton.

Conventional Cryptography

49

b = R2[20] ⊕ R2[21].

R3 has 23 bits R3[0], . . . , R3[22]. When R3 is clocked, it is similarly shifted by inserting
a new bit

b = R3[7] ⊕ R3[20] ⊕ R3[21] ⊕ R3[22].

In order to determine which registers to clock, we use three special bits called
“clocking taps” from every register, namely R1[8], R2[10], and R3[10]. We compute
the majority bit among those three bits, and registers whose clocking tap agree with
the majority are clocked. Consequently, we are ensured that at least two registers are
clocked. All registers are clocked if the three clocking taps agree on the same bit.

Every time unit, a bit is output from this scheme. This output bit is the XOR of

the leading bits, namely

R1[18] ⊕ R2[21] ⊕ R3[22].

We use the generated key stream as in the one-time pad.

A5/1 also includes an initialization which generates the initial internal state from
an encryption key and some GSM parameters. It is required that a new key is set up
for any new frame of 114 bits. More precisely, the key is set up from a 64-bit secret
key KC and a 22-bit frame number Count. This is indeed a kind of CTR mode. The
usage of a secret key KC is thus limited. Because of the structure of the frame number
in the GSM standard we can have at most 2.7 million frames for a single key, which
corresponds to 4 hours of GSM communication.

The A5/1 initialization works as follows. The three registers are ﬁrst set to zero.
Then every bit of KC is processed in 64 clock cycles by XORing them to the ﬁrst
register cells and stepping all registers (i.e. the clock control is disabled). Every bit
of the frame number Count is then processed in a similar way and the A5/1 automa-
ton is run for 100 clock cycles with its clock control enabled (but output bits are
discarded).

1: set all registers to zero
2: for i = 0 to 63 do
R1[0] ← R1[0] ⊕ KC[i]
3:
R2[0] ← R2[0] ⊕ KC[i]
4:
R3[0] ← R3[0] ⊕ KC[i]
5:
6:
clock all registers
7: end for
8: for i = 0 to 21 do
9:
10:

R1[0] ← R1[0] ⊕ Count[i]
R2[0] ← R2[0] ⊕ Count[i]

50

Chapter 2

R3[0] ← R3[0] ⊕ Count[i]
11:
12:
clock all registers
13: end for
14: for i = 0 to 99 do
15:
16: end for

clock the A5/1 automaton

2.8.4 E0: Bluetooth Encryption

E0 is another stream cipher which is used in the Bluetooth standard (see Ref. [18]).
As in A5/1, E0 is an automaton which generates keystreams which are simply XORed
to the plaintext as in the Vernam cipher. E0 generates one bit every clock cycle and
frames of 2745 bits. After a frame is generated, the E0 automaton is reset to another
state.

The state of the E0 automaton is described by the content of four linear feedback
shift registers LFSR1, LFSR2, LFSR3, LFSR4 of length 25, 31, 33, 39 respectively, and
the content of two 2-bit registers ct−1 and ct . Every clock cycle, the four registers are
clocked, ct , is moved to ct−1, and ct is updated by using ct−1, ct , and the registers.

More precisely, one bit x i
+ x 3

computes yt = x 1
The automaton outputs a new keystream bit zt = y0
value ct+1 = c1

t is output from LFSRi at every clock cycle. A summator
+ x 4
t y0
t .
t . The new

t and represents it in binary using three bits y2

t where ct = c1
t c0

+ x 2

⊕ c0

t y1

t

t

t

t+1c0

t+1 of ct is computed by
⊕ c1
⊕ c0
(cid:10)

= s1
t+1
= s0
t+1

c1
t+1
c0
t+1

t

t

st+1 =

yt + ct

2

⊕ c0
t−1
⊕ c1
t−1

⊕ c0
t−1

t

(cid:11)

.

where

E0 is actually a little more complicated. E0 is based on two levels of the above
automaton as depicted in Fig. 2.31. Formally, the encryption algorithm E0 takes the
logical address BD ADDR of the master (Bluetooth is based on master-slave protocols)
which is represented on 48 bits, the clock value of the master CLK which is represented
on 26 bits, and an encryption key Kc of 128 bits. The ﬁrst level is used in order to
initialize the automaton for every frame. The second level generates frames with the
initialized automaton. Concretely, the encryption key Kc is ﬁrst linearly shrunk and
then expanded into a 128-bit key so that the effective key length can be lowered for
regulation purposes. Then, the reexpanded encryption key, the master address, and the
master clock enter into the LFSR of the automaton which is clocked. After running
the automaton, 128 bits are output, which serve as the initial value of the second-level

Conventional Cryptography

51

E0 level 1

128 bits

-

E0 level 2

-

Kc

BD ADDR

CLK

-

-

-

Plaintext frame

2745 bits ⊕ - Ciphertext frame

-

6

Figure 2.31. The E0 keystream generator.

automaton for producing the keystream frame. Frames are ﬁnally used as a keystream
in the Vernam cipher.

E0 is very efﬁcient in hardware. Its circuit is depicted in Fig. 2.32. The division
by two at the output from the second summator simply consists of dropping one bit.
−1 symbol as a clock
Internal memory bit pairs for ct and ct−1 are represented by the z
time delay.

2.9 Brute Force Attacks

We now wonder what kind of security we can expect from symmetric encryption from
a generic point of view. Namely, we wonder about performances of generic attacks
using brute force which can apply to any encryption algorithm considered as a black
box. Security will be measured in terms of the encryption parameters such as the key
length and the message space size.

-

6zt
⊕
6

c0
t

c1
t

−1

z

t+1

 c0


c1
t+1

LFSR1

LFSR2

LFSR3

LFSR4

x1
t

-
x2
t

-
x3
t

x4
t

?

+

6

-
y0
t
-
y1
t
-
y2
t

?

?

-×

-
-

−1

z

c0
t−1

c1
t−1

-⊕

?
-⊕
6

?
-⊕
6

+

s1
t+1

s0
t+1

Figure 2.32. One level of the E0 keystream generator.

52

Chapter 2

Oracle

Yes/No

	
k

Attack

- Key

Figure 2.33. Key recovery with a stop test oracle.

2.9.1 Exhaustive Search

Exhaustive search consists of trying all possible keys exhaustively until it is the correct
key. We can simplify the attack model by assuming that we have an oracle, which for
each key K answers if it is correct or not. The attack is thus a machine which plays with
the oracle in order to get some information out of it by sending queries (see Fig. 2.33).
Without any more formal notion of a “machine,” we can still deﬁne the complexity in
terms of number of oracle calls.10 In a cipher with a key space of N possible keys which
are uniformly generated, we can prove that the best attack has a worst case complexity
of N oracle calls and an average complexity of (about) N
2 oracle calls. Clearly, the best
attack can be assumed to be deterministic (otherwise, we just take the machine which
simulates the best deterministic behavior of the probabilistic machine), and does not
query the oracle with twice the same question. Therefore, the best attack can be deﬁned
as an ordering of the candidates for the right key. The optimal ordering is deﬁned by
the a priori distribution of the target key.11

In practice we do not know the a priori distribution of the target. Thus, if we take a
ﬁxed ordering in order to try key candidates, we may fall into the worst case complexity.
For this we randomize the ordering of the key candidates. Fig. 2.34 shows a program
draft for this attack. In the worst case, the right key is the last one which is sent to
the oracle. If the right key is the i-th candidate, the complexity is equal to i. Since the
permutation σ is randomly chosen, the probability that the right key is the i-th queried
one is 1

N . Thus the average complexity is

N(cid:1)

i=1

= N + 1

2

.

i

1
N

The ultimate security goal of a cipher with keys of n bits is to have a best attack

of complexity (2n) with a “not-too-small” work factor.12

10 In Chapter 8 we deﬁne a formal notion of machine and the notion of complexity of the computation.
11 See Exercise 2.9 on p. 62.
12 This  notation means that there exists a constant c > 0 called work factor such that for any n, the
complexity is at least c2n elementary operations. The notion of complexity is formally deﬁned in Chapter 8.

Conventional Cryptography

53

There is a trick in order to strengthen the security against exhaustive search: make
a key schedule complicated. In general, the key schedule is legitimately used only
once for many encryptions. Exhaustive search uses the key schedule many times. For
instance, BLOWFISH has a complicated key schedule.

Input: an oracle O, a set of possible keys K = {k1, . . . , kN}
Oracle interface: input is an element of K, output is Boolean
1: pick a random permutation σ of {1, . . . , N}
2: for all i = 1 to N do
if O (kσ (i)) then
yield kσ (i) and stop

3:
4:
end if
5:
6: end for
7: search failed

Figure 2.34. Exhaustive search algorithm.

We can wonder now what the hypothesis about the availability of the oracle really
means in practice. We may only have some hints about the key. Namely, we can have
some equations that the key must satisfy. Typically, we can assume that we have a
plaintext–ciphertext pair so that the key must solve the equation which says that the
plaintext encrypts into the ciphertext. Another typical situation is when we intercept
a ciphertext and we know that the plaintext has some redundant information, e.g. the
plaintext is an English text. If the equations are characteristic enough for the key, the
unique solution is the right key candidate so that we can simulate the oracle by equation
solution checking.

2.9.2 Dictionary Attack

Another brute force attack is the dictionary attack. A dictionary is a huge table which
has been precomputed in order to speed up a key search. In practice, when one looks
for the deﬁnition of a word, it is too expensive to do it by exhaustive search! For this a
dictionary sorts all deﬁnitions by using an ordering on a key.

Here is a way to formalize a dictionary attack. We ﬁrst precompute a list of many
(CK (x), K ) pairs for a ﬁxed plaintext x. The ﬁrst entry CK (x) is used as an index in
order to sort the list as in a dictionary, and the second entry K is the “deﬁnition.”
Then, if we obtain a CK (x) value (by chosen plaintext attack), we directly have a list
of suggested K values. Let M be the number of entries in the dictionary. If K is in a
set of N possible keys, and uniformly distributed, the probability of success, i.e. the
probability that it is in the dictionary, is M/N . When K is not uniformly distributed,
we can focus on a dictionary of the most likely K values. This is done in practice when
we try to crack passwords.

We now assume that we have T targets CK (x) instead of one: there are T secret
keys that we try to attack simultaneously and we are interested in getting at least

54

Chapter 2

Input: an encryption scheme C, a ﬁxed message x
Preprocessing

compute CK (x)
insert (CK (x), K ) in a dictionary

1: for M different candidates K do
2:
3:
4: end for
5: output the dictionary
Attack
Attack input: T many values yi = CKi (x), a dictionary
6: for i = 1 to T do

look at yi in the dictionary
for all (CK (x), K ) with CK (x) = yi do
yield i, K

7:
8:
9:
10:
11: end for

end for

Figure 2.35. Multitarget dictionary attack.

one. (Fig. 2.35 illustrates the program structure.) Let p be the probability of success.
Assuming that the target K values are independent and uniformly distributed, we have
1 − p = (1 − M/N )T . Hence

−M T /N .
This becomes quite interesting when M ≈ T ≈ √

p ≈ 1 − e

N . Of course the probability of
success increases substantially when the targets are not uniformly distributed and we
focus on most likely candidates.

2.9.3 Codebook Attack

Yet another brute force attack is the codebook attack. . It consists of, ﬁrst, collect-
ing all (CK (x), x) pairs, then, upon reception of a y to decrypt, look for the entry
in the collection for which y = CK (x). The whole collection of pairs is called the
codebook.

2.9.4

(cid:1)Time–Memory Tradeoffs

An optimized way to perform exhaustive search is to use a large precomputed table
and to make a compromise between time complexity and memory requirements. Here
we must consider four parameters: the time to perform the precomputation, the size of
the precomputed table, the probability of success when looking for a key, and the time
complexity for looking for a key. For the latter parameter, we can also distinguish worst
case complexity and average case complexity.

Conventional Cryptography

55

The original method for time-memory tradeoffs was proposed by Martin Hellman
in 1980 in order to break DES (see Ref. [88]). We assume that we are looking for a
key K for which we know a plaintext–ciphertext pair (x, y) with a ﬁxed x. This ﬁxed
x is necessary to prepare the precomputed table. In practice this attack assumption
makes sense when the adversary performs a chosen plaintext attack (in order to get y
from x) or when the encryption protocol requires that the sender starts by encrypting
this ﬁxed message x. We further assume that for all k the bitstrings Ck(x) have the same
size which is larger than the key length so that it is likely that values of k for which
y = Ck(x) reduce to the unique solution k = K .

One ﬁrst selects an arbitrary “reduction function” R which reduces a bitstring of

the size of y to a key candidate R(y). This leads to the deﬁnition of a function f

f (k) = R(Ck(x))

which maps a key candidate to another key candidate so that we can iterate it. The
precomputation consists of making a table of m pairs (ki,0, ki,t ) where all ki,0 are
randomly selected and ki,t is the t-th term of a sequence ki, j deﬁned by

ki, j = f (ki, j−1).

(See Fig. 2.36.) In order to look for K once we are given y, we can apply R to y and
repeatedly apply f until we ﬁnd a value which matches one ki,t in the table. If we ﬁnd
one value ki,t , we can get ki,0 from the table and apply f repeatedly until we ﬁnd R(y)
again. Note that if the total number of f applications reaches t − 1 without success we
can give up and the attack fails. Otherwise we ﬁnd a key k such that f (k) = R(y). This
may be due to Ck(x) = y, which leads to K = k. Otherwise the attack fails. Figs. 2.37
and 2.38 with (cid:9) set to 1 illustrate this method.

T1 :

T(cid:1) :

k1
1,0
k1
2,0
k1
3,0
...
k1
m,0

k(cid:1)
1,0
k(cid:1)
2,0
k(cid:1)
3,0
...
k(cid:1)
m,0

1,1

2,1

3,1

f1(cid:6)→ k1
f1(cid:6)→ k1
f1(cid:6)→ k1
...
f1(cid:6)→ k1

m,1

1,1

2,1

3,1

f(cid:1)

f(cid:1)

f(cid:1)

(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
...
(cid:6)→ k(cid:1)

f(cid:1)

m,1

1,2

2,2

3,2

f1(cid:6)→ k1
f1(cid:6)→ k1
f1(cid:6)→ k1
...
f1(cid:6)→ k1

m,2

1,2

2,2

3,2

f(cid:1)

f(cid:1)

f(cid:1)

(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
...
(cid:6)→ k(cid:1)

f(cid:1)

m,2

1,3

2,3

3,3

f1(cid:6)→ k1
f1(cid:6)→ k1
f1(cid:6)→ k1
...
f1(cid:6)→ k1
...
(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
...
(cid:6)→ k(cid:1)

f(cid:1)

f(cid:1)

f(cid:1)

f(cid:1)

m,3

1,3

2,3

3,3

m,3

f1(cid:6)→ ···
f1(cid:6)→ ···
f1(cid:6)→ ···
...
f1(cid:6)→ ···

f(cid:1)

f(cid:1)

f(cid:1)

(cid:6)→ ···
(cid:6)→ ···
(cid:6)→ ···
...
(cid:6)→ ···

f(cid:1)

f1(cid:6)→ k1
f1(cid:6)→ k1
f1(cid:6)→ k1

f1(cid:6)→ k1

f(cid:1)

f(cid:1)

(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)

f(cid:1)

f(cid:1)

(cid:6)→ k(cid:1)

1,t−1
2,t−1
3,t−1
...
3,t−1

1,t−1
2,t−1
3,t−1
...
3,t−1

1,t

2,t

3,t

f1(cid:6)→ k1
f1(cid:6)→ k1
f1(cid:6)→ k1
...
f1(cid:6)→ k1

m,t

1,t

2,t

3,t

f(cid:1)

f(cid:1)

f(cid:1)

(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
(cid:6)→ k(cid:1)
...
(cid:6)→ k(cid:1)

f(cid:1)

m,t

Figure 2.36. Table for Time–Memory tradeoff.

(k1
1,t
(k1
2,t
⇒ (k1

)
)
)

, k1
1,0
, k1
2,0
, k1
3,0
...
m,t, k1

3,t

m,0

)

(k1

(k(cid:1)
1,t
(k(cid:1)
2,t
⇒ (k(cid:1)

)
)
)

, k(cid:1)
1,0
, k(cid:1)
2,0
, k(cid:1)
3,0
...
m,t, k(cid:1)

3,t

m,0

)

(k(cid:1)

56

Chapter 2

Input: an encryption scheme C, a ﬁxed message x
Parameter: (cid:9), m, t
Preprocessing
1: for s = 1 to (cid:9) do

2:

pick a reduction function Rs at random and deﬁne fs : k (cid:11)→
Rs(Ck(x))
for i = 1 to m do
(cid:8)
pick k
at random
k ← k
(cid:8)
for j = 1 to t do
compute k ← fs(k)
end for
insert (k, k

) in table Ts

(cid:8)

3:
4:
5:
6:
7:
8:
9:
10:
11: end for

end for

Figure 2.37. Time–Memory tradeoff (Preprocessing).

Attack
Attack input: y = CK (x)
1: for s = 1 to (cid:9) do

(cid:8)

set i to 0
set k to Rs(y)
while Ts contains no (k, .) entry and i < t do
increment i
k ← fs(k)
end while
if Ts contains a (k, .) entry then
get the (k, k
) entry from table Ts
while Ck(cid:8)(x) (cid:4)= y and i < t do
increment i
(cid:8) ← fs(k
(cid:8)
)
k
end while
if Ck(cid:8)(x) = y then
(cid:8)
yield k

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
end if
17:
18: end for
19: abort: the attack failed

end if

Figure 2.38. Time-Memory tradeoff (Attack).

Conventional Cryptography

We can estimate the number of pairwise different ki, j in one table as follows.

(cid:12)

#{ki, j ; 1 ≤ i ≤ m, 0 ≤ j < t}(cid:13) = m(cid:1)

t−1(cid:1)

E

(cid:14)

Pr

j=0

i=1

(cid:4)∈ {ki(cid:8), j(cid:8); i

(cid:8) < i or j

ki, j

57

.

(cid:8) < j}(cid:15)

the end value of each chain.) Let Freshi, j be the event
. Since ki, j = f (ki, j−1), we have Freshi, j ⊆ Freshi, j−1.

(cid:14)

(cid:4)∈ {ki(cid:8), j(cid:8); i

(We do not count
(cid:8) < i or j
ki, j
Furthermore, we have

(cid:8) < j}(cid:15)
(cid:14)

Pr

Hence we have

Freshi, j|Freshi, j−1
(cid:16)

(cid:14)

(cid:15) ≥

Pr

Freshi, j

.

(cid:15) ≥ 1 − it
(cid:17) j+1

2n

.

1 − it
2n

We deduce that the expected number of keys per table is greater than

m(cid:1)

t−1(cid:1)

(cid:16)

i=1

j=0

1 − it
2n

(cid:17) j+1

.

The probability of success for looking for K is the probability that K is one of the keys
in the table. Thus we have

m(cid:1)

t−1(cid:1)

(cid:16)

i=1

j=0

1 − it
2n

(cid:17) j+1

.

p ≥ 2

−n

As the table gets larger, some chain will eventually collide and merge, which pre-
vents the number of keys per table from increasing. So it may be preferable to keep
many small tables. Figs. 2.37, and 2.38 illustrate the multitable version. Obviously the
precomputation complexity is P = (cid:9)mt encryptions and reductions. The time complex-
ity of the attack is T = (cid:9)t encryptions and reductions in the worst case, and Te = (cid:9)
2 t
in the average case (for success only). The memory size is about M = (cid:9)m blocks. The
probability p of success is such that

(cid:2)
1 − 2
−n

m(cid:1)

t−1(cid:1)

(cid:16)

i=1

j=0

1 − it
2n

(cid:3)(cid:9)

(cid:17) j+1

.

p ≥ 1 −

−L be the right-hand term of the p inequality. For (cid:9) = λx,

3 and 1 − e

We let x = 2 n
m = µx, and t = τ x, when µτ (cid:15) x, we have
µx(cid:1)

(cid:2)
1 − x

−3

L = −λx log

(cid:16)

τ x−1(cid:1)

1 − i τ
x 2

i=1

j=0

(cid:3)

(cid:17) j+1

58

(cid:17) j+1
(cid:17)τ x(cid:17)(cid:16)

Chapter 2

(cid:17)

1 − i τ
x 2

i=1

µx(cid:1)
µx(cid:1)
µx(cid:1)
(cid:20) µτ 2

i=1

i=1

1
i

1
i

0

≈ λ
x 2
= λ
τ
≈ λ
τ
≈ λ
τ

(cid:16)

τ x−1(cid:1)
(cid:16)
(cid:18)

j=0
1 −

1 − e
1 − e
−t
t

1 − i τ
(cid:16)
x 2
1 − i τ
(cid:19)
x 2

− i τ 2

x

dt.

(Note that this is equivalent to λ
when µ and τ are large enough.) With (cid:9) ≈ m ≈ t ≈ 2 n
22n/3, and p ≥ 0.549.

τ log(µτ 2) when µτ 2 increases, so that p ≥ 1 − (µτ 2)
− λ
3 we obtain P ≈ 2n, T ≈ M ≈

τ

There exist several improvements of this technique, e.g. using the notion of “dis-
tinguished points.” The most efﬁcient so far uses the notion of “rainbow tables” and is
due to Philippe Oechslin (see Ref. [142]). Here, instead of using many tables with one
reduction function in each, we use a larger table with many reduction functions (see
Fig. 2.39). We deﬁne

ki, j = f j (ki, j−1).

The algorithm is depicted in Figs. 2.40, 2.41.

Here we make sure that no chains collide in the precomputation. This induces an
overhead in the precomputation. At the i-th iteration the probability for a new chain to
collide is approximately 1 − e

2n , so the precomputation complexity is

−t i−1

P ≈ m(cid:1)

i=1

tet i−1

2n = t

2n − 1
e mt
2n − 1
e t

≈ 2n

(cid:18)

(cid:19)

2n − 1
e mt

.

The probability of a key being in a column is m.2
p = 1 − (1 − m.2

−n so
−n)t

T :

k1,0
k2,0
k3,0
...
km,0

f1(cid:6)→ k1,1
f1(cid:6)→ k2,1
f1(cid:6)→ k3,1
...
f1(cid:6)→ km,1

f2(cid:6)→ k1,2
f2(cid:6)→ k2,2
f2(cid:6)→ k3,2
...
f2(cid:6)→ km,2

f3(cid:6)→ k1,3
f3(cid:6)→ k2,3
f3(cid:6)→ k3,3
...
f3(cid:6)→ km,3

f4(cid:6)→ ···
f4(cid:6)→ ···
f4(cid:6)→ ···
...
f4(cid:6)→ ···

(cid:6)→ k1,t−1
ft−1
(cid:6)→ k2,t−1
ft−1
(cid:6)→ k3,t−1
ft−1
...
(cid:6)→ k3,t−1
ft−1

Figure 2.39. Rainbow table.

ft

ft

(cid:6)→ k1,t
(k1,t ,k1,0)
(cid:6)→ k2,t
(k2,t ,k2,0)
(cid:6)→ k3,t ⇒ (k3,t ,k3,0)

ft

...
(cid:6)→ km,t

ft

...

(km,t ,km,0)

Conventional Cryptography

59

Input: an encryption scheme C, a ﬁxed message x
Parameter: m, t
Preprocessing
1: for j = 1 to t do

2:

pick a reduction function R j at random and deﬁne f j : k (cid:11)→
R j (Ck(x))

3: end for
4: for i = 1 to m do

repeat
(cid:8)
pick k
at random
k ← k
(cid:8)
for j = 1 to t do
compute k ← f j (k)
end for

5:
6:
7:
8:
9:
10:
11:
12:
13: end for

until T contains no (k, .) entry
insert (k, k

) in table T

(cid:8)

Figure 2.40. Time–Memory tradeoff with a rainbow table (Preprocessing).

− mt
2n .

≈ 1 − e
The time complexity of the attack is T = t 2
case, and Te = t 2
M = m blocks. With m = 2 2n
M ≈ 2 2n

3 , T ≈ 0.240 × 2 2n

3 , and p ≈ 1

2 encryptions and reductions in the worst
8 in the average case (for success only). The memory size is about
3 we obtain P ≈ 2n,

3 and t = (log 2)2 n

3 ≈ 0.693 × 2 n

2 . This is a substantial improvement.

To summarize brute force generic attacks, we list in the following table the essential

complexity of each part of the attack for a probability of success close to one.

Strategy

Preprocessing

Memory

Time

Exhaustive search
Dictionary attack
Tradeoffs

1
N
N

1
N
2
N
3

N
1
2
N
3

2.9.5 Meet-in-the-Middle Attack

In order to strengthen DES, people ﬁrst tried to make a simple product with itself,
resulting in a double-DES (see Section 2.3.1). More generally, we have

(cid:12)

(cid:13)

CK1 K2(X) = C

(cid:8)(cid:8)
K2

(cid:8)
K1(X)

C

.

60

Chapter 2

Attack
Attack input: y = CK (x)
1: for i = t down to 1 do

set k to Ri (y)
for j = i to t do
k ← f j (k)
end for
if T contains one (k, .) entry then
) entry from table T
(cid:8)

(cid:8)
get the (k, k
for s = 1 to i − 1 do
(cid:8) ← fs(k

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
end if
15:
16: end for
17: abort: the attack failed

k
end for
if Ck(cid:8)(x) = y then
yield k
abort: the attack succeeded

end if

)

(cid:8)

Figure 2.41. Time–Memory tradeoff with a rainbow table (Attack).

(cid:8)

(cid:8)(cid:8)

(cid:8)

(cid:8)(cid:8)

and C

have key spaces of N

keys respectively, the ultimate security
If C
goal is to have a security related to a minimal complexity of N = N
. This goal
is not achieved as shown by the following attack. The attack is called “meet-in-the-
middle attack” and was devised by Ralph Merkle and Martin Hellman in 1981 (see
Ref. [133]).

(cid:8) × N

and N

(cid:8)(cid:8)

As illustrated in Fig. 2.42, let us consider a known plaintext attack: we know a
(x, y) pair such that y = CK1 K2(x). The meet-in-the-middle attack consists of making a
table of all possible CK1(x) (which is of size N
), and for all possible K2, in computing
−1
(y) and looking for the value in the table. This will suggest a list of possible K1 K2
C
K2
keys which can be tried with other known plaintexts (see Fig. 2.43). The complexity
(cid:8) + N
instead of N . Therefore doubling the structure is not a good paradigm for
is N
strengthening the security.

(cid:8)(cid:8)

(cid:8)

2.10 Exercises

Exercise 2.1. Prove that

DESK (x) = DESK (x).

K1
?

C

K2
?

C

-z

x -

- y

Figure 2.42. Meet-in-the-Middle.

Conventional Cryptography

61

(cid:8)

(cid:8)(cid:8)

and C

with two corresponding sets

and K(cid:8)(cid:8)

Input: two encryption schemes C
1: for all k1 ∈ K(cid:8)

of possible keys K(cid:8)
do
compute z = C
(cid:8)
k1
insert (z, k1) in a hash table (indexed with the ﬁrst entry)

, an (x, y) pair with y = C

(cid:8)(cid:8)
K2

(x)

(cid:8)
K1

(C

(x))

2:
3:
4: end for
5: for all k2 ∈ K(cid:8)(cid:8)

6:
7:
8:
9:
10: end for

end for

do
compute z = C
for all (z, k1) in the hash table do

(cid:8)(cid:8)−1
k2 (y)

yield (k1, k2)

Figure 2.43. Meet-in-the-middle attack.

Deduce a brute force attack against DES with average complexity 254 DES encryp-
tions.13

Exercise 2.2. We say that a DES key K is weak if DESK is an involution. Exhibit four
weak keys for DES.

Exercise 2.3. We say that a DES key K is semi-weak if it is not weak and if there exists
K

such that

(cid:8)

DES

−1
K

= DESK (cid:8) .

Exhibit four semi-weak keys for DES.14
Exercise 2.4. In CBC Mode, show that an opponent can replace one ciphertext block
so that the decryption will let all plaintext blocks but xi and xi+1 unchanged.

Exercise 2.5. Describe how the OFB decryption is performed.

Describe how the CFB decryption is performed.

Exercise 2.6. Prove that the multiplication over nonzero residues modulo 216 + 1 de-
ﬁnes a group operation over a set of 216 elements.

We represent nonzero residues modulo 216 + 1 with their modulo 216 reduction.

(This means that “1” represents 1, “2” represents 2, etc., and “0” represents 216.)

Explain how to efﬁciently implement this operation in software from regular CPU

operations. (This operation is used in IDEA.)
Exercise 2.7. Show that x (cid:11)→ (45x mod 257) mod 256 is a permutation over
{0, . . . , 255}. (This permutation is used as a substitution box in SAFER K-64.)

13 This exercise was inspired by Ref. [89].
14 This exercise was inspired by Ref. [89].

62

Chapter 2

Exercise 2.8. We want to break a keyed cryptographic system. We assume we have
access to an oracle which on each queried key answer whether or not the key is correct.
We iteratively query the oracle with randomly selected keys (in an independent way).
Compute the expected complexity (in term of oracle queries) in general, and when the
distribution of the key is uniform. How can we improve the attack?

Exercise 2.9. We assume that we have an oracle which for any of the N possible keys
K answers if it is correct or not.

If the a priori distribution of the keys is not uniform but known by the adversary,
what is the best algorithm for ﬁnding the key with the oracle? Prove that its complexity
relates to the guesswork which is deﬁned by

W (K ) = N(cid:1)

i=1

i. Pr[K = ki ]

where all possible keys k1, . . . , kN are sorted such that the Pr[K = ki ] sequence is
decreasing.15

As an example, let us consider a pseudorandom generator which generates an
element x of {0, 1, . . . , p − 1} uniformly for a given prime number p. In order to get
a random n-bit string, we consider K = x mod 2n.

1. Compute W (K ) for p > 2n. Example: n = 64 and p = 264 + 13.
2. Compute W (K ) for p < 2n. Example: n = 64 and p = 264 − 59.
3. Let  = | p − 2n|. Express W (K ) in terms of  and compare both cases.

Exercise 2.10. Explain how to make a dictionary attack against UNIX passwords.
What is the complexity?

Exercise 2.11. We assume that we have an oracle which for any of the N possible keys
K answers if it is correct or not.

If the a priori distribution of the keys is not uniform but known by the adversary,
what is the best memoryless algorithm for ﬁnding the key with the oracle? Prove that
its complexity relates to the Renyi entropy of coefﬁcient 1

2 which is deﬁned by

(cid:3)2
(cid:21)
Pr[K = ki ]

(cid:2)
N(cid:1)

i=1

(K ) =

H 1
2

where k1, . . . , kN is the list of all possible keys.16

15 This exercise was inspired by Ref. [123].
16 This exercise was inspired by Ref. [39].

3
Dedicated Conventional
Cryptographic Primitives

Content

Hash functions: MD5, SHA, SHA-1
Generic attack against hash funtions: birthday paradox
(cid:1)Analysis of hash functions: dedicated attack against MD4
Message authentication codes: CBC-MAC, HMAC
(cid:1)Pseudorandom generator: congruential generator

In Chapter 2 we saw several conventional encryption algorithms. They were ded-
icated to conﬁdentiality. In this chapter we study other conventional cryptographic
primitives that are dedicated to integrity, authentication, and randomness.

3.1 Cryptographic Hashing

3.1.1 Usage

In computer science, hash functions are used in order to arrange a database so one of
its element can be accessed very efﬁciently. An entry is usually a pair (x, y) where x
is the entry label and y is data. It is stored at the location h(x) in the database. Later
on, if we want to have access to data related to the label x, we just look at the location
such that h(x) = h(x
).
h(x). Problems arise when we have two different labels x and x
This is called a collision. Efﬁcient hash functions are functions whose domain space is
small and whose expected number of collisions is small in practical applications.

(cid:8)

(cid:8)

In cryptography, hash functions are used to protect the integrity of data: instead of
protecting the integrity of data of arbitrary length, we want to concentrate on protecting
the integrity of really small bitstrings. Thus, we need to hash the data onto a string of
ﬁxed length which is called the hashed value, or the message digest, or the ﬁngerprint,
or even (improperly) the cyclical redundancy check (CRC). CRCs are used for error
detection, but “cryptographic CRCs” are different: here the adversary is assumed to be
malicious and no longer a random noise process. Assuming we succeed in protecting
the integrity of the hashed value, we can detect if the data has been modiﬁed by hashing
it again and comparing the two hashed values. We can thus use an expensive integrity
channel in order to provide integrity over an insecure channel (see Fig. 3.1).

Adversary

64

Message

X

Hashing

Digest

d

INTEGER

Figure 3.1. Integrity channel.

Chapter 3

X

Hashing

d

Compare

Cryptographic hash functions are sometimes called manipulation detection codes

(MDC).

For security, we have to ensure that a change in the data content without a change
in the hashed result is an impossible scenario. Actually, the forgery of two different data
with the same hashed value must be intractable. Therefore, we need the collisions to be
intractable. We improperly say that the hash function is collision-free. This qualiﬁcation
is improper because collisions do exist. They are simply hard to ﬁnd. A more correct way
is to say that it is collision-resistant. The difference between cryptographic hash func-
tions and regular hash functions (which are used in the hash table data structure) is that
collisions are intractable for a malicious adversary instead of just being unlikely events.

Cryptographic hash functions are also used for commitment. Someone who wants
to commit on data x without revealing it (for instance a bid for a contract) can just reveal
h(x||r) where r is a random string. He can later open the commitment by revealing x
and r. In this case, we need the hash function to be one-way: we need that given h(x||r),
it is intractable to recover x or any information about x.

3.1.2 Threat Models

As we had several classical attack models for block ciphers (like known or chosen
plaintext attacks), here are three important attack models for cryptographic hash func-
tions.

First preimage attack: from a ﬁxed y we try to get x such that h(x) = y.
Second preimage attack: from a ﬁxed x we try to get x
(cid:8) (cid:4)= x such that h(x) = h(x
(cid:8)
Collision: we try to get x

(cid:8) (cid:4)= x such that h(x) = h(x
).

(cid:8)

).

Depending on the application, these attack models are relevant or not.

Dedicated Conventional Cryptographic Primitives

65

In the integrity protection scenario, the goal of the adversary is to replace a given
with the same hashed value. This clearly corresponds

(cid:8)

message x by another message x
to the second preimage attack.

(cid:8)

In the commitment scenario, the participant who commits to x may try to cheat by
replacing x by a different x
with the same hashed value before opening the commit-
ment. In this case he has control on x so he can also choose it. Hence this corresponds to
the collision attack scenario. The other participant of the commitment scheme may try
to retrieve information about x from the commitment. This is a kind of ﬁrst preimage
attack.

So, as we can see, many security properties may be required for hash functions

depending on the threat model.

As usual, it is pretty hard to formally deﬁne these security properties. From the
viewpoint of complexity theory, we should consider families of hash functions instead
of a single one engraved in stone. In practice, the hash function is really ﬁxed and we
consider an intuitive notion of security.

3.1.3 From Compression to Hashing

A classical cryptographic hash function design, due independently to Ralph Merkle
and Ivan Damg˚ard, consists of iterating a compression function. As with encryption,
which can be based on a block cipher by using a mode of operation, we can construct a
cryptographic hash function from a compression function. Here, a compression function
is a function which takes ﬁxed length inputs and returns a ﬁxed length output (which
is shorter than the total input length).

For instance, the MD5 cryptographic compression function (see below) has two
inputs, H of length 128 bits and B of length 512 bits, and outputs a 128-bit string. As
depicted in Fig. 3.2, hashing an arbitrary length message M proceeds as follows.

1. We pad M with one bit equal to 1, followed by a variable number of zero bits,
and 64 bits encoding the length of M in bits, so that the total length of the
¯M
padded message is the smallest possible multiple of 512 (see Fig. 3.3). Let
denote the padded message.

2. We cut ¯M into a sequence of 512-bit blocks B1, . . . , Bn.
3. We let H0 = IV, a ﬁxed initial value.

Message

Initial
value

512
?
- C
128

512
?
C

-

-

···
···

Pad

?
- C

-
128

Figure 3.2. The Merkle–Damg˚ard scheme.

66

Chapter 3

Pad =

1
1

0

···

64

0

Length

Figure 3.3. Padding in the Merkle–Damg˚ard scheme.

4. For i = 1, . . . , n, we let Hi = C(Hi−1, Bi ).
5. We deﬁne Hn = h(M).

We notice that this construction restricts to messages of size less than 264 bits. In
practice this is not a problem since this represents 2
152 Tera Bytes, which cannot
be managed with current technology.

097

(cid:8)

(cid:8)

Theorem 3.1 (Merkle–Damg˚ard 1989 [56, 130]). With the above construction, if the
compression function C is collision-resistant, then the hash function h is collision-
resistant as well.
Proof. If we can construct a collision for the hash function h(M) = h(M
¯M =
(cid:8) = B
(cid:8)
(cid:8)
(cid:8)
B1, . . . , Bn and ¯M
, . . . , B
j following the above
n(cid:8). We also deﬁne Hi and H
scheme. We have Hn = H
n(cid:8). Thus we have C(Hn−1, Bn) = C(H
(cid:8)
(cid:8)
(cid:8)
1
n(cid:8)−1
n(cid:8)). Either this
is a collision on the compression function, or the inputs are equal. If this is the case, then
we have Bn = B
(cid:8)
(cid:8)
n(cid:8), but since the last block includes the length of the messages, M and M
have the same length, thus n = n
n−1, thus C(Hn−2, Bn−1) =
(cid:8)
(cid:8)
n−1). We continue the proof the same way. Since the messages are different,
C(H
(cid:8)
i at some point. Thus we must get a collision on the compression
we must have Bi
(cid:1)
function.

. We also have Hn−1 = H

(cid:4)= B

(cid:8)
n−2

, B

(cid:8)

), let

(cid:8)

, B

3.1.4 Example of MD5

MD5 is a famous cryptographic hash function example. It is widely used in Internet
applications. It was designed by Ronald Rivest at MIT in 1991 and published as the
RFC 1321 Internet standard in 1992 (Ref. [157]). It hashes arbitrary length bitstrings
onto 128 bits. MD5 stands for “Message Digest,” and is based on a previous algorithm,
MD4. It uses the Merkle-Damg˚ard construction from a 128 × 512 → 128 compression
function.1

The compression function is made from an “encryption function” by the Davies-
Meyer scheme: given an “encryption function” C0 which maps a 128-bit value H =
(A, B, C, D) and a 512-bit key block B into a 128-bit value, we deﬁne

C(H, B) = C0(H, B) + (A, B, C, D)

where the addition here is the word-wise addition modulo 232.

1 Note that some collisions on MD5 were exhibited at the CRYPTO’04 conference by Xiaoyun Wang and
Xuejia Lai. This means that MD5 should no longer be considered as a secure hash function and should be
replaced in most existing standards.

Dedicated Conventional Cryptographic Primitives

67

The encryption C0 consists of four rounds as depicted in Fig. 3.4: the message
block (key block) is permuted into a sequence x0, . . . , x15 following a permutation
which depends on the round number, and every of the four input words is sequentially
transformed through a generalized four-branch Feistel scheme. Each transformation is
deﬁned by a box with a main input a, a key input x, and three side inputs b, c, and d
as shown in Fig. 3.5.

The output of the transformation box is

ROTLαi, j (a + fi (b, c, d) + x + ki, j ) + b

where αi, j and ki, j are deﬁned by a table, ROTL is the bitwise rotation to the left (by
a number of position speciﬁed as a superscript), and fi is a bitwise Boolean function
deﬁned by

f1(b, c, d) = if b then c else d
f2(b, c, d) = if d then b else c
f3(b, c, d) = b XOR c XOR d
f4(b, c, d) = c XOR (b AND (NOT d))

where the “if . . . then . . . else” function is deﬁned by

if b then c else d = (b AND c) OR ((NOT b) AND d).

3.1.5 Examples of SHA and SHA-1

SHA is another famous cryptographic hash function example. SHA stands for “Secure
Hash Algorithm”. It was published by the US Government as the FIPS 180 standard
(Ref. [15]) in 1993. It is based on MD5 and is mainly used in digital signature schemes. It
hashes onto 160 bits and uses the Merkle-Damg˚ard construction from a 160 × 512 →
160 compression function. The original version was replaced by a slightly different
one, SHA-1, in 1995 (see the FIPS 180-1 standard, Ref. [16], which is now superseded
by the FIPS 180-2 standard, Ref. [17]). There was no justiﬁcation for this replace-
ment (just a mention about security problems). However, Florent Chabaud and Antoine
Joux, from the French Department of Defense, publicly raised a weakness of the orig-
inal SHA which seemed to have disappeared in SHA-1 (see Ref. [43]). Antoine Joux
later announced to have found a collision on the original SHA.2 As for MD5, the
compression functions of SHA and SHA-1 are made from an “encryption function”
by the Davies-Meyer scheme: given an “encryption function” C0 which maps 160-bit
value H = (A, B, C, D, E) and a 512-bit key block B = (x0, . . . , x15) into a 160-bit

2 The collision was announced at the CRYPTO’04 conference, surprisingly at the same time that a collision

on MD5 was found.

68

Chapter 3

A

B

C

D

B
L
O
C
K

σi

B0
i

B4
i

B8
i

B12
i

B1
i

B5
i

B9
i

B13
i

B2
i

B6
i

B10
i

B14
i

B3
i

B7
i

B11
i

B15
i

Figure 3.4. One round of MD5.

Dedicated Conventional Cryptographic Primitives

69

b
c

d

b
c

d

ROTLαi, j(a + fi(b,c,d) + x + ki, j) + b

Figure 3.5. Transformation box in MD5.

value, we deﬁne

C(H, B) = C0(H, B) + (A, B, C, D, E)

where the addition here is the word-wise addition modulo 232.

The encryption C0 consists of four rounds, each consisting of 20 transformation
steps. The message block (key block) is expanded following a linear relation, and every
of the ﬁve input words is sequentially transformed through a generalized ﬁve-branch
Feistel scheme. Each transformation is deﬁned by a box with a main input a, a key input
x, and four side inputs b, c, d, and e. More precisely, C is deﬁned by the algorithm
depicted in Fig. 3.6.

Input: an initial hashed value a, b, c, d, e, a message block x0, . . . , x15
Output: a hash a, b, c, d, e
1: for i = 16 to 79 do

xi ← ROTL1 (xi−3 XOR xi−8 XOR xi−14 XOR xi−16)

for j = 0 to 19 do
t ← ROTL5(a) + fi (b, c, d) + e + x20(i−1)+ j + ki
e ← d
d ← c
c ← ROTL30(b)
b ← a
a ← t
end for

2:
3: end for
4: for i = 1 to 4 do

5:
6:
7:
8:
9:
10:
11:
12:
13: end for
14: a ← a + ainitial
15: b ← b + binitial
16: c ← c + cinitial
17: d ← d + dinitial
18: e ← e + einitial

Figure 3.6. Compression in SHA-1.

70

Chapter 3

Each transformation layer uses an fi function and a ki constant. fi is a bitwise

Boolean function deﬁned by

f1(b, c, d) = if b then c else d
f2(b, c, d) = b XOR c XOR d
f3(b, c, d) = majority(b, c, d)
f4(b, c, d) = b XOR c XOR d

where the majority function is deﬁned by

majority(b, c, d) = (b AND c) OR (c AND d) OR (d AND b).

(As a Boolean function, if two bits out of b, c, and d are equal, then majority(b, c, d)
is equal to that bit.)

As previously mentioned, the difference between SHA and SHA-1 is little. SHA
has exactly the same compression function except for the “key schedule” which is
replaced by

1: for i = 16 to 79 do
2:
3: end for

xi ← xi−3 XOR xi−8 XOR xi−14 XOR xi−16

with the ROTL rotation missing!

For completeness we mention that the the US standard on secure hash algorithms
also includes four other algorithms called SHA224, SHA256, SHA384, and SHA512
(see Ref. [17]). These four algorithms are quite similar, and are more complex variants of
SHA-1. The message digest length is 224, 256, 384, and 512 bits respectively. SHA384
and SHA512 work with 64-bit words instead of 32-bit words, and cut messages in
blocks of 1024 bits instead of 512.

3.2 The Birthday Paradox

If the hashed value is of size n, regular brute force preimage or second preimage attacks
require 2n hash computations: we iteratively pick a random x until we ﬁnd a solution.
The probability that the complexity is exactly i (which means that the i-th trial succeeds,
and all previous ones fail) is (1 − 2
−n. Thus, the average complexity in terms
of number of hash computations is
+∞(cid:1)

−n)i−12

−n)i−12

−n = 2n.

i(1 − 2

i=1

Collisions are easier to ﬁnd with the attack in Fig. 3.7, thanks to the birthday
paradox. This paradox simply notices that if we pick 23 people, by assuming that their

Dedicated Conventional Cryptographic Primitives

71

Input: a cryptographic hash function h onto a domain of size N
Output: a pair (x, x

) such that x (cid:4)= x

and h(x) = h(x

)

(cid:8)

(cid:8)

(cid:8)

√

) pair in the hash table then

N many different x do

compute y = h(x)
(cid:8)
if there is a (y, x

(cid:8)

1: for θ
2:
3:
4:
5:
6:
7: end for
8: search failed

yield (x, x

) and stop

end if
add (y, x) in the hash table

Figure 3.7. Collision search with the birthday paradox.

birthdays are independent and uniformly distributed among 365 days, then at least two
of them are likely to share the same birthday. It can be mathematically expressed as
follows.
Theorem 3.2. If we pick independent random numbers in {1, 2, . . . , N} with uniform
distribution θ

N times, we get at least one number twice with probability

√

1 −

√

N !
N (N − θ

√

N )!
For N = 365, we obtain the following ﬁgures.

N θ

−→
N→+∞ 1 − e

− θ2
2 .

√

θ

N

θ

Probability

10

0.52
12%

15

0.79
25%

20

1.05
41%

25

1.31
57%

30

1.57
71%

35

1.83
81%

40

2.09
89%

So we can ﬁnd a collision on a hash function by hashing random numbers until the list
of hashed values have a collision. If we want to ﬁnd a collision on MD5 which uses
128-bit hashed values, we thus use θ.264 MD5 computations.

Proof. The probability that we have no collision is the number of ordered subsets of
{1, . . . , N} of cardinality θ
N )!, divided by the number of
sequences of numbers of length θ

N , which is N !/(N − θ
√
N , which is N θ
N .

√

√

√

We approximate the probability by using the Stirling Approximation3

√

n! ∼
n→+∞

−nnn.

2πne

3 The ∼ notation means that there is equality between both sides with a proportional coefﬁcient which tends
toward 1 when n tends toward inﬁnity.

72

Hence

1 − p =

∼

(cid:16)

√

√
N !
(cid:17)−N+θ
N (N − θ
N θ
1 − θ√
(cid:22)
N
√
−θ

N )!
√
N

= exp

N + (−N + θ

√

N

−θ

e

Chapter 3

(cid:17)(cid:23)

(cid:17)(cid:23)

(cid:16)

1 − θ√
N

(cid:16)

1 − θ√
N

N ) log

√

√

N ) log

We now use log(1 − ε) = −ε − ε2
√

2

+ o(ε2)

1 − p ∼ exp
∼ exp

N + (−N + θ
(cid:23)
+ o(1)

(cid:22)
−θ
(cid:22)

− θ 2
2

which means it tends towards e

− θ2
2 .

(cid:1)

A simpler way to express the birthday paradox consists of estimating the number
of collisions. If we take a sequence X1, . . . , Xn of independent uniformly distributed
random variables in {1, . . . , N}, the number of collisions is

C = n−1(cid:1)

n(cid:1)

i=1

j=i+1

1Xi=X j

and we have Pr[Xi = X j ] = 1

N , so the expected number of collisions is
E(C) = n(n − 1)

× 1
N

2

√

√

and for n = θ

N , we have E(C) ∼ θ 2

2 . The variance is V (C) ∼ θ 2

2 as well.

Here, the analysis is quite easy when the complexity is ﬁxed, but there is no real
N as in Fig. 3.7. The general analysis is

need, in practice, for upper-bounding it by θ
the subject of Exercise 3.3.

For completeness we provide a similar result which is used in order to ﬁnd collisions
between two separated lists (for instance, if we want to ﬁnd a male–female couple of
people with the same birthday).

√

√

Theorem 3.3. If we make two sequences of length θ1
N by picking inde-
pendent random numbers in {1, 2, . . . , N} with uniform distribution, we get at least

N and θ2

Dedicated Conventional Cryptographic Primitives

73

one common number in the two sequences with probability

−θ1θ2 .

p −→
N→+∞ 1 − e
√

√

Proof. Let N1 = θ1
variables with uniform distribution X1, . . . , X N1
to compute

N , N2 = θ2

N , and let us consider the independent random
, Y1, . . . , YN2 in {1, . . . , N}. We want

Let C be the cardinality of {X1, . . . , X N1

p = Pr[∃i, j Xi = Y j ].
}. We have
(cid:18)
1 − c
N

Pr[C = c]

(cid:19)N2

.

(cid:17)N2 −→

N→+∞ e

−θ1θ2 .

1 − N1
N

1 − p = N1(cid:1)
(cid:16)

c=1

1 − p ≥

Since c is always less than N1, we have

√
On the other hand, let αN be such that αN −→ +∞ and αN = o(

the above sum to c = N1 − αN . We have
1 − N1 − αN

1 − p ≤

(cid:16)

(cid:17)N2 + Pr[C < N1 − αN ].

N ). We truncate

N ). It remains to prove that

The ﬁrst term tends toward e
Pr[C < N1 − αN ] is negligible.

Let

N

√
−θ1θ2 since αN = o(

(cid:1)

i < j

D =

1Xi=Y j

.

We have Pr[C < N1 − αN ] ≤ Pr[D > αN ]. Obviously we have E(D) = N1(N1−1)
Since all 1Xi=X j are pairwise independent, we can also compute the variance

2N

.

V (D) = N1(N1 − 1)

2N

Now from the Chebyshev Inequality we have

(cid:16)

1 − 1
N

(cid:17)

.

Pr[D > αN ] ≤

V (D)

(αN − E(D))2

.

74

Chapter 3

Since E(D) tends toward a constant, it is negligible compared to αN . Since V (D) tends
(cid:1)
toward a constant, this probability tends toward zero.

To summarize, when using a hash function which hashes down to n bits, brute

force attacks have the following complexities.

r Preimage attacks (ﬁrst and second preimage attacks) have complexity 2n.
r Collision attacks have complexity 2 n

2 .

3.3 (cid:1)A Dedicated Attack on MD4

The birthday paradox provides a generic way to attack hash functions in order to forge
collisions. These generic attacks are always possible, and their complexity depends on
the output size. For MD5, the output hash length is of 128 bits, so birthday-paradox-
based attacks have a complexity of the order of 264. The ideal goal of conventional
cryptographic primitives is to have no better attacks.

However, the particular design of some primitives may lead to some dedicated

attacks. Here we exhibit an attack of this type against a simpliﬁed version of MD4.

MD4 is a hash function which is similar to MD5, but with the following differ-

ences.4

r MD4 has three rounds (instead of four for MD5).
r f2 is deﬁned by
r The output of the transformation box does not add b any more.

f2(b, c, d) = (b AND c) OR (c AND d) OR (d AND b)

which is the majority function.

The MD4 permutations are deﬁned as follows.

(cid:16)
(cid:16)
(cid:16)

σ1 =
σ2 =
σ3 =

0 1 2
0 1 2
0 1 2
0 4 8
0 1 2
0 8 4

4 5 6
3
4 5 6
3
3
4
12 1
3
4
12 2

7
7

8 9 10 11
8 9 10 11

12
12

13
13

14
14

15
15

8 9 10
6
7
5
2 6 10
9 13
5
8 9
10
6
7
5
5
14 1 9
10 6

11
14
11
13

12
3
12
3

13
7
13
11

14
11
14
7

(cid:17)

(cid:17)
(cid:17)

15
15
15
15

This means, e.g., that σ2(8) = 2. The constants ki, j actually do not depend on j and
are deﬁned by

k1 = 0,

k2 = 5a827999,

k3 = 6ed9eba1

4 MD4 was presented at the CRYPTO’90 conference. See Ref. [156].

Dedicated Conventional Cryptographic Primitives

75

in hexadecimal. Rotations αi, j depend on i and j mod 4 only and are deﬁned as follows.

j mod 4

i

1
2
3

0

3
3
3

1

7
5
9

2

11
9
11

3

19
13
15

For instance, α3,6 = 11.

In the dedicated attack we simplify MD4 by suppressing the ﬁnal round, so that
we have two rounds instead of three. This example is meant to be illustrative only since
this does not lead to any attack against the full MD4 itself.

A useful building block for information diffusion in conventional cryptographic
primitives is the notion of multipermutation.5 Intuitively, a multipermutation is a func-
tion with multiple inputs and multiple outputs with the property that modifying one
or several inputs of the function has the inﬂuence of modifying a maximal number
of outputs from the computation. Concretely, if a function f has p inputs and q out-
puts, modifying r inputs must have the inﬂuence of modifying at least q − r + 1
outputs. For instance, if p = q = 4, modifying r inputs leads to modifying at least
5 − r outputs. The linear transform M of MixColumns in the Advanced Encryp-
tion Standard (AES) has this property. If p = q = 2, modifying r inputs leads to
modifying at least 3 − r outputs. The mixing box M of CS-CIPHER has this prop-
erty. If p = 3 and q = 1, modifying r = 1 input must lead to the modiﬁcation of
the output. The f1 and f2 functions of MD4 do not have this property as shown
below.

Indeed, we observe that we can control modiﬁcations on a single input of f1
or f2 as follows. For any a, the function f1(11··· 1, a,·) is a constant equal to a.
The functions f1(00··· 0,·, a) and f1(·, a, a) have the same property. Similarly, the
functions f2(a, a,·),
f2(a,·, a), and f2(·, a, a) have the same property. From these
properties we deduce that for any transformation box in the second round, if the main
input is 0, if the key input is −k2 (the constant of the second round), and if two out of
the three other registers are 0, then the output remains 0.

The permutation σ1 of the ﬁrst round is in fact the identity permutation: the key
values are used in their original ordering. Therefore, provided that x0, . . . , x11 are ﬁxed,
it is easy to choose x12, x13, and x14 so that the content of the A, C, and D registers are
all 0 (see Fig. 3.8a).

5 Multipermutations were ﬁrst proposed by Claus Schnorr and Serge Vaudenay (Ref. [162]). A more complete

study is available in Ref. [179].

-

-

-

-

-

-

-

-

-

-

-

-

76

B
L
O
C
K

x12

x13

x14

∗
x
15

(a)

a

b

c

d

?

?
B0
1





-
-
-

?

?
B1
1

?



?
B2
1

-
-

?




?
B3
1

-

?

?
B4
1





-
-
-

?

?
B5
1

-
-

?




?
B7
1

σ1

-

?

?
B8
1





?



?
B6
1

B
L
O
C
K

-
-
-

?

?
B9
1

?



?
B10
1

-
-

?




?
B11
1

-

?

?
B12
1





-
-
-

?

?
B13
1

?



?
B14
1

-
-

?




?
B15
1

?
∗

?
0

?
0

(b)

-

?
0

0

0
2

0

B4
2

B8
2

0

B12
2

−k2

−k2

−k2

−k2

−k2

−k2

−k2

−k2

−k2

−k2

∗

0

0

B1
2

B5
2

B9
2

B13
2

B2
2

B6
2

B10
2

B14
2

B3
2

B7
2

B11
2

B15
2

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

B
L
O
C
K

∗
x
15

(c)

?

?
B0
3





Chapter 3

×

-
-
-

?

?
B1
3

?



?
B2
3

-
-

?




?
B3
3

-

?

?
B4
3





-
-
-

?

?
B5
3

?



?
B6
3

-
-

σ3

-

?




?
B7
3

?

?
B8
3





-
-
-

?

?
B9
3

?



?
B10
3

-
-

?




?
B11
3

-

?

?
B12
3





-
-
-

?

?
B13
3

?



?
B14
3

-
-

-

?

?




?
B15
3

?
∗

?

?

Figure 3.8. Dedicated attack against MD4. (a) First round, (b) second round, and (c) third round.

The permutation σ2 of the second round is such that x12, x13, x14, x15 are the key
values used in the B register. Therefore, if all other values are set to−k2, and x12, x13, x14
are chosen in order to achieve the above property, we make sure that the content of the
A, C, and D registers remains zero (see Fig. 3.8b). This property holds for any choice
of x15.

We can consider the output of the second round as a random function of x16 for
which three registers are always zero. Randomness is thus limited to 32 bits. Considering
the birthday paradox, we can deduce a collision after 216 trials: we obtain two different
key blocks for which the output after the second round is the same. This breaks MD4
reduced to its ﬁrst two rounds (see Fig. 3.9).

We can even look at what happens in the third round with this attack. The permu-
tation σ3 of the third round is such that x16, the only modiﬁed key value, is used in the
which
last transformation box (see Fig. 3.8c). Therefore, the two key blocks x and x
) only differ
collide after the second round have the property that MD4(x) and MD4(x
in their second 32-bit quarter. In optimizing this attack, we can ﬁnd two key blocks for
which the MD4 digests differ in one single bit, which is quite remarkable. (If digest

(cid:8)

(cid:8)

Dedicated Conventional Cryptographic Primitives

77

(cid:8)

← −k2

Output: two message blocks x and x
1: for i = 0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13 do

2

xσ −1
2:
(i)
3: end for
4: x11 ← random {we have deﬁned x0, . . . , x11}
5: initialize a, b, c, d to the initial hash value
6: for i = 0 to 11 do

7:
8: end for
9: for i = 12 to 14 do

xi ← −(a + f1(b, c, d) + k1)
(a, d, c, b) ← (d, c, b, 0)

10:
11:

(a, d, c, b) ← (d, c, b, ROTLαi,1(a + f1(b, c, d) + xi + k1))

(cid:8)
i

(cid:8)

(cid:8)
15)

12: end for{we now have deﬁned all xi but x16}
13: b0 ← a {b0, x0, . . . , x14 deﬁne a function g as detailed below}
14: look for a collision g(x15) = g(x
= xi for i = 0, . . . , 14
15: deﬁne x
16: output x and x
Function g(x15):
1: initialize a, c, d to 0
2: b ← ROTLα15,1(b0 + x15 + k1)
3: for i = 0 to 15 do

(a, d, c, b) ← (d, c, b, ROTLαi,2(a + f2(b, c, d) + xσ2(i) + k2))

and exit

4:
5: end for
6: return b

Figure 3.9. Dedicated attack against MD4.

values are compared by human beings, a modiﬁcation in one bit out of 128 may not be
noticed!)

These attacks against MD4 were further improved by Hans Dobbertin, a mathe-
matician from the German Intelligence Agency, who showed how to build full collisions
(see Refs. [61, 62]). As depicted in Fig. 3.10, the main idea consists of keeping track
of a single bit ﬂip in the message block. Indeed, we consider two messages such that
the least signiﬁcant bit of x12 is ﬂipped. This message modiﬁcation starts inﬂuencing
the computation at the output of the B12
1 box. The ﬁrst part of the analysis consists
of looking for some entering values of the A, B, C, and D registers before this box
and some values of x0, x4, x8, x12, x13, x14, and x15 which are the only values which
are used up to the B3
2 box. All those values must be such that the inﬂuence of ﬂipping
the bit of x12 leads to ﬂipping only two bits of the A, B, C, and D registers, namely the
bit of 225 in B and the bit of 25 in C. This part of the attack is based on solving some
equations. The second part of the analysis consists of picking some random matching
x1, x2, x3, x5, x6, x7, x9, x10, x11 so that we get the right content of the A, B, C, and D
registers before the B12
1 box. The third part of the analysis consists of expecting that
those two bit ﬂips in B and C have a controlled propagation behavior as depicted in
Fig. 3.10, they do not propagate to A and D and only two bits are ﬂipped in total. Since

78

B
L
O
C
K

∗
x
12

x13

x14

x15

a

B0
1

B4
1

B8
1

B3
1

B7
1

B11
1

B12
1

b

c

d

x0

x4

x8

∗
x
12

B
L
O
C
K

1

σ2

B1
1

B5
1

B9
1

B13
1

B2
1

B6
1

B10
1

B14
1

B15
1

∗

∗

∗

∗

x0

x8

x4

∗
x
12

B
L
O
C
K

1

σ3

∗

B0
2

0

B4
2

0

B8
2

B12
2

0

∗

∗

∗

B1
2

0

B5
2

0

B9
2

B13
2

0

B2
2

225

25

B6
2

25

214

B10
2

B14
2

B3
2

B7
2

B11
2

B15
2

0

B0
3

0

B4
3

B8
3

B12
3

Chapter 3

1

1

0

B1
3

0

B5
3

B9
3

B13
3

B2
3

B6
3

B10
3

B14
3

B3
3

B7
3

B11
3

B15
3

0

1

1

0

0

0

0

0

Figure 3.10. Differential graph for a full collision in MD4.

2 , and B14

(cid:13)7 × 2

3
4

2 , B11

2 , and B15

−21 ×(cid:12)

the ﬂipped bit of B is rotated by 13 positions in B7
2 , it arrives in the
least signiﬁcant position before B3
3 where x12 enters again. Similarly, the ﬂipped bit of
2 , B10
C is rotated by 9 positions in B6
2 , it also arrives in the least signiﬁcant
position before B2
3 where the ﬂipped bit of B can be used to cancel it. Indeed, we
(cid:12)
estimate that those bit ﬂips propagate as explained with a probability which is roughly
−2 ≈ 2
−21 is for preventing the propagation of one ﬂipped
−26. (The 2
2
3
bit from one register to the other, the
−2 is for the ﬁnal cancellation of the ﬂipped bits.)
4
bit in a carry in an addition, and the 2
This means that we should obtain a collision after about 226 trials once we run the MD4
computation.

(cid:13)7 is for preventing the propagation of one ﬂipped

3.4 Message Authentication Codes

3.4.1 Usage

Message authentication codes are called MACs. They can guarantee the authenticity
of a document in an insecure channel. If we want to do this with a cryptographic
hash function, the hashed value must be transmitted through a secure channel which

Dedicated Conventional Cryptographic Primitives

79

Adversary

Message

X

X

6
c

X,c

X

X

c

MAC

c

Compare

MAC

Generator

Key
K

CONFIDENTIAL
AUTHENTICATED

K

Figure 3.11. Authentication channel.

guarantees the authenticity. As it is done for encryption, we can separate the usage of
this channel from the transmission of the document by using it to transmit a secret key
and no message dependent value. Once the key is set up, the transmission of the hashed
value is replaced by the transmission of the MAC through the insecure channel. We can
thus use an expensive authentication channel in order to provide authentication over an
insecure channel (see Fig. 3.11).

Note that message authentication implicitly includes message integrity and is thus
a little stronger. It is a common mistake to mix up both and to speak about message
integrity when we want to speak about message authentication. One reason may be
that we want to reserve the noun “authentication” for peer authentication rather than
message authentication.

The MAC is thus computed (from the message and the secret key) by a function
which is improperly called MAC. It usually works like a cryptographic hash function
with a key.

3.4.2 Threat Model

We want to protect ourselves against an adversary who has already seen several pairs
(M, c) where c is the MAC for M, and who wants to create a pair (M, c) with a new M
(which is not authentic) and a valid MAC (which would pass the authenticity check).
Like for encryption, the pairs can be obtained by known or chosen message attacks
(see Fig. 3.12).

Note that we concentrate on the authentication of single messages but we do not
address integrity of a communication session: as long as messages are authenticated, we

80

Chapter 3

MAC

ci

Mi

Adversary

(M,c)

Figure 3.12. Adversarial model for message authentication codes.

cannot suspect them for having been sent to the recipient by an adversary since it must
have been generated by the legitimate user. An adversary may however try to swap,
erase, or replay some MAC-ed messages in a communication session. This is typically
thwarted by adding a sequence number within the message, as it will be illustrated in
Chapter 12.

3.4.3 MAC from Block Ciphers: CBC-MAC

A famous construction for MAC is the CBC-MAC construction. A ﬁrst idea for this
construction is to take the last encrypted block of the CBC encryption of the message
as a MAC.

This idea is however not secure. Let us assume that we know three pairs (M1, c1),
(M2, c2), (M3, c3) such that M1 and M3 have the same length and M2 is the concatenation
of M1 and another message. Let us denote M2 = M1||B||M
(cid:8)
2 where B is a single
message block. If C denotes the block cipher (with an unknown key), the encrypted
block of M2 which matches B is

C(B ⊕ c1).
(cid:8) = B ⊕ c1 ⊕ c3 and M4 = M3||B
(cid:8)||M
is

We deﬁne B
(cid:8)
matches B

(cid:8)
2. The encrypted block of M4 which

(cid:8) ⊕ c3) = C(B ⊕ c1),

C(B

(cid:8)

which is the same encrypted block as that of M2. Therefore, all encrypted blocks for M4
after B
will be equal to the encrypted blocks for M2 after B. Hence the last encrypted
block of M2 is equal to the last encrypted block of M4, which means that the MAC of
M4 is c2. We can thus forge a new valid pair (M4, c2).

Following a second idea called “Encrypted MAC (EMAC),” one can encrypt the
last encrypted block of the CBC encryption with another key. We still have an attack
which uses the birthday paradox. If the opponent gets θ
N valid pairs (where N is the
number of possible MACs and θ is a constant which deﬁnes the probability of success

√

Dedicated Conventional Cryptographic Primitives

81

x1

C

x2

C

x3
⊕

C

···
···

···

xn
⊕

C

(cid:5)

C

trunc

MAC

Figure 3.13. Encrypted CBC-MAC.

of the attack), which is 1 − e
−θ 2/2, he is likely to observe a collision: he gets a pair
(M1, c) and a pair (M2, c) with the same MAC c. If the opponent can now request the
MAC corresponding to a message M3 = M1||M
(cid:8)
3 obtained by concatenating M1 with
) with
any M
M4 = M2||M
(cid:8)
3. This attack is essentially optimal as the result in the next section shows.

). He will then be able to create a new pair (M4, c

(cid:8)

(cid:8)
3, he will get (M3, c

(cid:8)

A third idea consists of dropping a few bits to avoid collision attacks (see Fig. 3.13),
e.g. truncating the MAC to the ﬁrst half of the bits. This is actually the ISO/IEC 9797
standard (Ref. [11]) MAC algorithm.

One problem remains: how to handle messages whose lengths are not multiples of
the block size? For this, three constructions called XCBC, RMAC, and TMAC has been
proposed. Furthermore, the key length in the previous constructions looks unnecessarily
long. A ﬁnal variant called OMAC as “One-key CBC MAC” has been proposed as a
new standard (see Ref. [94]). OMAC is actually a family of MAC algorithms whose
OMAC1 seems to be the favorite instance at this time. An instance of the family is
deﬁned by two constants Cst1 and Cst2, a MAC length t, and a function H which maps
a message block and a constant to a message block. Given a message block L, we
let HL denote the function which maps the remaining constant to a message block.
OMAC works as follows. Let us assume that we are given a MAC key K and a message
M = x1||x2||···||xn where all xi (except xn) are full message blocks and the length of
xn is at most the size of a full message block.

1. Let L be the encryption of the zero block, i.e. the message block whose all bits
are set to zero. Compute HL(Cst1) and HL(Cst2). (Note that this step can be
preprocessed for a given key K since it does not depend on the message M.)

2. If xn has not the full block length, concatenate it with a bit 1 followed by as
many bits as necessary (if any) to reach the block length. In the latter case, we
say that the message was padded.

82

Chapter 3
3. If xn was not padded, replace it by xn ⊕ HL(Cst1). If xn was padded, replace it
4. Compute the regular CBC-MAC of x1||x2||···||xn.
5. Truncate it to its t leftmost bits and obtain the MAC of M.

by xn ⊕ HL(Cst2).

In the OMAC1 instance, H is deﬁned in the ﬁnite ﬁeld GF(2n) where n is the block
length. Concretely, blocks are n-bit strings which represent a polynomial in the variable
u whose coefﬁcients are binary and read in descending order from the coefﬁcient of
un−1 (the leftmost bit) to the constant coefﬁcient (the rightmost bit). The addition of
polynomials corresponds to the XOR of bitstrings. The multiplication of a polynomial
represented by the bitstring B by u is obtained from B by dropping the leftmost bit
and concatenating a zero bit, then by XORing the result to a constant if the dropped
bit is 1. The constant is, in hexadecimal, 0x000000000000001b if n = 64 and
0x00000000000000000000000000000087 if n = 128. These rules and ﬁeld
properties fully deﬁne a multiplication law over n-bit strings. Finally, HL(x) is the
multiplication of L by x, Cst1 is the bitstring representing u, i.e. 0x00···02, and Cst2
is the bitstring representing u2, i.e. 0x00···04. Note that we only have to compute
L × u and L × u2, which means that we just have to multiply L by the constant u twice.
Implementation of the full GF(2n) arithmetics is not necessary. So ﬁnally, we can get
rid of all these algebraic aspects (which will be made clearer in Chapter 6) and deﬁne
the operation y ← x × u by

1. take x;
2. drop the leftmost bit and insert a new bit zero to the right;
3. if the dropped bit was 1, XOR with 0x00···1b for n = 64 and 0x00···87 for

n = 128;

4. get y.

In OMAC1, HL(Cst1) is deﬁned by L × u, and HL(Cst2) is deﬁned by HL(Cst1) × u.

3.4.4

(cid:1)Analysis of CBC-MAC

To illustrate the CBC-MAC constructions from the previous section, we provide here a
security analysis of the encrypted CBC-MAC, i.e. the EMAC construction. Assuming
that the block encryption with the ﬁrst key is denoted C1 and that the ﬁnal block double
encryption with the two keys is denoted C2, EMAC matches the construction in the
following theorem.

Theorem 3.4. Given two independent uniformly distributed random permutations C1
and C2 on M of cardinality N , let us deﬁne

MAC(x1, . . . , x(cid:9)) = C2(C1(··· C1(C1(x1) ⊕ x2) ⊕ ··· ⊕ x(cid:9)−1) ⊕ x(cid:9)).

We assume that the MAC function is implemented by an oracle, and we consider an
(adaptive) adversary who can send queries to the oracle with a limited total length of q:

Dedicated Conventional Cryptographic Primitives
83
if M1, . . . , Md denote the ﬁnite block sequences on M which are sent by the adversary
to the oracle, we assume that the total number of blocks is less than q. The purpose of
the adversary is to output a message M which is different from all Mi together with its
MAC value c. The probability of success of any adversary (i.e. the probability that the
MAC value is correct) is smaller than
q(q + 1)

+ 1

N − d

.

× 1

N − q

2

√

Note that this is less than q2/N when 4 ≤ q ≤ N /4.
When q = θ
2 ). Therefore,
if the total length of all authenticated messages is negligible against
N , there is no
better way than brute force attack to get collisions on the CBC-MAC (provided that C1
and C2 are perfectly random!).

2 (which is greater than 1 − e
− θ2
√

N , this is approximately θ 2

Proof. First of all, we can assume without loss of generality that all Mi are pairwise
different. The next step consists of transforming the adversary into a nonadaptive
collision ﬁnder against a hash function H deﬁned by

H(x1, . . . , x(cid:9)) = C1(··· C1(C1(x1) ⊕ x2) ⊕ ··· ⊕ x(cid:9)−1) ⊕ x(cid:9).

Let A be the adversary against the MAC function. We deﬁne a collision ﬁnderB against
H as follows.

1. We pick a random permutation C2.
2. We simulate A. Every time that A tries to send a query Mi to the oracle, we
answer ci = c2(H(Mi )) toA (i = 1, . . . , d). The simulation ends whenA issues
a pair (M, c) or fails.
3. If A failed, we send M1, . . . , Md to oracle H. If it outputs no collision, then B
4. If A issues a forged pair (M, c) and if c is different from all the ci ’s, then B
fails. Otherwise we send M1, . . . , Md , M to oracle H. If it outputs no collision,
then B fails; otherwise it succeeds.

fails; otherwise it succeeds.

Let Es be the event that A succeeds when run with the MAC oracle. Let ps = Pr[Es].
We will prove that B succeeds with a probability p, which is at least ps − 1
N−d . Then
we conclude using Lemma 3.5 given later.

When runningA with the MAC oracle, let Ec be the event that some queries collide,
i.e. that MAC(Mi ) = MAC(M j ) for some 1 ≤ i < j ≤ d. Clearly, H(Mi ) = H(M j ),
so B will succeed.

Provided that Ec does not occur, the distribution of c1, . . . , cd in the simulation is
exactly the same as the distribution of the MAC(Mi ) values. Hence the simulation of

84
Chapter 3
A succeeds with probability at least Pr[Es|Ec]. Let E p be the event that the output c is
equal to one answer ci from the oracle. If E p occurs then B succeeds. We have proven
that B succeeds with probability

p ≥ Pr[Ec] + Pr[Es ∩ E p|Ec] Pr[Ec].

If neither Ec nor E p occurs, then either H(M) = H(Mi ) for some i and A fails, or
H(M) is new but the probability that C2(H(M)) = c is equal to
1
N−d . Therefore the
probability that A succeeds is at most

1
N−d . We have
Pr[Es ∩ E p|Ec] ≤ 1
N − d

hence

(cid:16)

p ≥ Pr[Ec] +

(cid:17)

Pr[Ec]

Pr[Es|Ec] − 1
N − d

thus p ≥ ps − 1
(cid:1)
N−d .
Lemma 3.5. Given one uniformly distributed random permutation C on M of cardi-
nality N , let us deﬁne

H(x1, . . . , x(cid:9)) = C(··· C(C(x1) ⊕ x2) ⊕ ··· ⊕ x(cid:9)−1) ⊕ x(cid:9).

We assume that the H function is implemented by an oracle, and we consider a non-
adaptive adversary who can send queries to the oracle with a limited total length of q:
if M1, . . . , Md denote the ﬁnite sequences on M which are sent by the adversary to
the oracle, we assume that the total length is less than q. The purpose of the adversary
is to output two different sequences among all the queried one with the same H value.
The probability of success of any adversary is smaller than

q(q + 1)

2

× 1

N − q

.

Proof. Let Mi = mi,1|| . . .||mi,qi . We assume that the Mi ’s are pairwise different. We
let E denote the event that we have H(Mi ) = H(M j ) for some i (cid:4)= j.

We deﬁne

Ui, j = C(··· C(C(mi,1) + mi,2)··· + mi, j−1) + mi, j

which is an intermediate value which is used to compute MAC(Mi ). We call “collision”
an event Ui, j = Ur,s. This collision is trivial if mi,1|| . . .||mi, j = mr,1|| . . .||mr,s (it will
happen for any C) and nontrivial otherwise (it will depend on C). We let Coll denote
the event that a nontrivial collision occurs.

Dedicated Conventional Cryptographic Primitives

85

We call “inversion” the event Inv that C(Ui, j ) = 0 for some i, j.
The E event is clearly included in Inv ∪ Coll: if Ui,qi

(cid:4)=
mr,qr and it is a nontrivial collision, or it reduces to Ui,qi−1 = Ur,qr−1 and we can iterate.
Thus Pr[E] ≤ Pr[Inv] + Pr[Coll].

= Ur,qr , then either mi,qi

In order to determine an upper bound on Pr[Inv], we can transform the collision
attack against MAC into an inversion attack against C: we say that it succeeds whenever
C(Ui, j ) = 0 for some i and j. Clearly Pr[Inv] is the probability of success of this
attack. But the probability that any adaptive attack against C ﬁnds a preimage of 0
after q queries is less than q
N−q : once the attack has submitted i queries, C is bound
to i input/output pairs, but since C is uniformly distributed, the encryption of any
new block is uniformly distributed among all other possible outputs, thus it is the
zero block with probability 1
1
N−q when i is less than q. Thus
Pr[Inv] ≤ q

N−i which is less than

N−q .

The remaining part of the proof is devoted to ﬁnding the upper bound of

Pr[Coll].

We let U be the set of all Ui, j -indices, which means the set of all (i, j) such that

1 ≤ i ≤ d and 1 ≤ j ≤ qi . For A ⊆ U we let c(A) be

c(A) = {(i, j);∃(r, s) ∈ A i = r and j ≤ s}.

Thus c(A) is the set of the indices of all Ui, j which are required in order to compute
all Ur,s values for (r, s) ∈ A. We deﬁne an ordering on the set 2
of all subsets of U
U
by

A ≤ B ⇐⇒ A ⊆ c(B).

A is less than B if all indices in A are required to compute the indices in B.

We let I be the set of all pairs of indices of potential nontrivial collisions
Ui, j = Ur,s, namely the set of all pairs {(i, j), (r, s)} of U-elements such that
mi,1|| . . .||mi, j
(cid:4)= mr,1|| . . .||mr,s. For {(i, j), (r, s)} ∈ I we let Colli, j,r,s be the event
of the collision Ui, j = Ur,s (which is necessarily nontrivial since {(i, j), (r, s)} ∈ I ),
and we let MinColli, j,r,s be the complementary, in Colli, j,r,s, of the union of all
Colli(cid:8), j(cid:8),r(cid:8),s(cid:8) for {(i
)} < {(i, j), (r, s)}: we have a
collision Ui, j = Ur,s with no lower nontrivial collision Ui(cid:8), j(cid:8) = Ur(cid:8),s(cid:8). We easily no-
tice that the nontrivial collision event Coll is the union of all nontrivial minimal
collisions:

)} ∈ I and {(i

(cid:8), j

(cid:8)

(cid:8), s

(cid:8)

(cid:8), s

), (r

(cid:8)

(cid:8), j

), (r

(cid:8)

(cid:24)

Coll =

MinColli, j,r,s .

{(i, j),(r,s)}∈I

86
We have at most q(q−1)

2

terms in I . Hence

Chapter 3

Pr[Coll] ≤ q(q − 1)

2

max

{(i, j),(r,s)}∈I

Pr[MinColli, j,r,s].

(We need this inequality because it is easier to upper bound the probability of a nontrivial
collision when we know that there is no sub-nontrivial collisions.)

For {(i, j), (r, s)} ∈ I , let us consider the MinColli, j,r,s event. We assume without
loss of generality that s ≤ j. Since we have no previous collision we must have mi, j
(cid:4)=
mr,s. We cannot have j = 1 (otherwise s = j = 1, but C(mi,1) = C(mr,1) is either
trivial or impossible). Thus j > 1 and the event is

C(Ui, j−1) ⊕ mi, j = Ur,s .

If we have a collision Ui, j−1 = Ui(cid:8), j(cid:8) with (i, j − 1) (cid:4)= (i
) ∈
c ({(i, j), (r, s)}), it must be trivial (otherwise the Ui, j = Ur,s collision is not mini-
(cid:8) = r, and mi,1|| . . .||mi, j−1 = mr,1|| . . .||mr, j−1. If
mal) which means j
s < j we have Ui, j = Ur,s and Ur,s = Ui,s thus Ui, j = Ui,s which is nontrivial, which
contradicts the minimality of the initial collision. Thus we must have s = j, but the
trivial collision Ui, j−1 = Ur, j−1 make a nontrivial collision Ui, j = Ur,s impossible.
Therefore Ui, j−1 is equal to no Ui(cid:8), j(cid:8) for (i

) ∈ c(i, j, r, s)\{(i, j − 1)}.

(cid:8) = j − 1, i

) and (i

(cid:8), j

(cid:8), j

(cid:8), j

(cid:8)

(cid:8)

(cid:8)

This implies that the marginal distribution of C(Ui, j−1) with the knowledge of
all previous Ui(cid:8), j(cid:8) and their C-images is uniform among a set of at least N − q + 1
elements. Hence Pr[MinColli, j,r,s] ≤ 1
+ q(q − 1)

= q(q + 1)

N−q . Finally,

× 1

× 1

Pr(E) ≤ q

N − q

2

N − q

N − q

2

(cid:1)

3.4.5

(cid:1)MAC from Stream Ciphers

Traditional hash functions must have improbable collisions. A traditional way to study
regular hash functions in computer science is to consider the function as a random
variable: we do not have a ﬁxed hash function, but a family of hash functions, and
we pick one at random. Hence we must consider the probability that H(x) = H(y)
for any different x and y, over the distribution of H. Following Carter and Wegman in
Ref. [42], we say that a random hash function H is ε-universal if for any x (cid:4)= y we have

Pr[H(x) = H(y)] ≤ ε.

Dedicated Conventional Cryptographic Primitives
We say that it is ε-strongly-universal if for any x (cid:4)= y and any a and b we have

87

Pr[H(x) = a, H(y) = b] ≤ ε
#D

where D is the output domain of H.

A MAC algorithm is actually a random hash function whose distribution is deﬁned
by the secret key choice. We can construct a secure MAC from a universal hash function
by simply encrypting the output with the Vernam cipher.

Theorem 3.6 (Wegman–Carter 1981 [184]). Let (h K )K∈UK be a family of hash func-
tions over the output domain {0, 1}m deﬁned by a random key K which is chosen uni-
formly at random in a key space K. We assume that this family is ε-strongly-universal.
Given K and a sequence of keys K1, K2, . . . , which are independent and uniformly
distributed over {0, 1}m, we deﬁne a MAC algorithm which changes the key for every
new message. Namely, the MAC of the message xi of sequence number i is deﬁned by

ci = h K (xi ) ⊕ Ki

An authenticated message is a triplet (xi , i, ci ), where ci is computed as above. No
chosen message attack can forge a new authenticated message with a probability of
success greater than ε.

Hugo Krawczyk improved this result as follows.

Theorem 3.7 (Krawczyk 1994 [108]). Following the same notations, we say that h is
ε-XOR-universal if for any x (cid:4)= y and any a we have

Pr[h K (x) ⊕ h K (y) = a] ≤ ε.

The previous theorem still holds when the strong-universality hypothesis is replaced by
XOR-universality.

Clearly, a ε-strongly universal hash function is ε-XOR-universal, so this result is
stronger.
Proof. At the end, the adversary knows d authenticated messages (xi , i, ci ) for i =
1, . . . , d and forges (x, j, c) with x (cid:4)= xi for any i. The K and K j keys are conditioned
to the knowledge of all (xi , i, ci ).

If j is not in the [1, d] interval, then K j is uniformly distributed and independent
−m. (Note that
−m, so c is valid with probability

from this information, so the probability that c is a valid MAC of (x, j) is 2
if h is ε-XOR-universal, then ε must be greater than 2
less than ε.)

88

Chapter 3

If j is in the interval [1, d], the probability of success is

Pr[h K (x) ⊕ K j = c|h K (x j ) ⊕ K j = c j , I ]

where I = {h K (xi ) ⊕ Ki = ci ; i ∈ [1, d], i (cid:4)= j}. Because of the distribution of
K1, . . . , K j−1, K j+1, . . . , Kd, we can easily see that I is useless in the above prob-
ability. Finally we have

Pr[h K (x) ⊕ K j = c|h K (x j ) ⊕ K j = c j ] = Pr[h K (x) ⊕ h K (x j )

= c ⊕ c j|h K (x j ) ⊕ K j = c j ] ≤ ε

since K j is independent from K .

(cid:1)

Note that this construction generalizes by deﬁning the MAC of x as being h K (x)

encrypted using a stream cipher (instead of the Vernam cipher).

As an example of XOR-universal hash function family, we provide here a con-
struction based on linear feedback shift registers (LFSR) called LFSR Toeplitz hash
function, which is due to Hugo Krawczyk (see Ref. [108]). Given two parameters m
and n, we deﬁne a hash function h K from {0, 1}n to {0, 1}m based on a key K = ( p, s0)
where s0 = (s0, . . . , sm−1) is a bitstring of length m and p deﬁnes a polynomial
p(x) = p0 + p1x + ··· + pm x m of degree m, with coefﬁcients in GF(2), and which
is irreducible. (Note that this implies p0 = pm = 1.) We deﬁne an LFSR as a ﬁnite
automaton whose state at time t is a bitstring st = (st , . . . , st+m−1) of length m and
which is updated following the connection polynomial p(x), i.e. we have the following
recursion formula:

st+m = m−1(cid:25)

j=0

p j st+ j .

The LFSR is ﬁrst initialized to s0 deﬁned by K . Hashing a message x0, . . . , xn−1 of n
bits simply consists of XORing the states st , which correspond to a time t for which
xt = 1:

h K (x0, . . . , xn−1) =

st .

(cid:25)

0≤t <n
xt =1

We can prove that given a ﬁxed m and n, the family of all h K hash functions is ε-XOR-
universal with ε = n × 21−m.

3.4.6 MAC from Hash Functions: HMAC

Making a hash function from a MAC algorithm is quite trivial: we just need to take
a constant key. However, we need to keep in mind that MAC does not usually protect
against collision attacks.

Dedicated Conventional Cryptographic Primitives

89

We can also make a MAC from cryptographic hash functions. The following
construction, called HMAC, is due to Mihir Bellare, Ran Canetti, and Hugo Krawczyk.
It has been published as the Internet standard RFC 2104 (Ref. [109]).

We assume that we are given a cryptographic hash function H which internally
processes messages by blocks of B bytes and produces a digest of L bytes. For instance,
if H is SHA-1, we have B = 64 and L = 20. We further use a parameter t which is
the required size of the MAC in bytes. We require that t is at least 4 and at most
the output size of H. Computing the MAC of a message m with a key K works as
follows.

1. If K has more than B bytes, we ﬁrst replace K by H(K ). (Having a key of such

a long size does not increase the security.) Note that H(K ) has L bytes.

2. We append zero bytes to the right of K until it has exactly B bytes.
3. We compute

H(K ⊕ opad||H(K ⊕ ipad||m))

where ipad and opad are two ﬁxed bitstrings of B bytes. The ipad consists of
B bytes equal to 0x36 in hexadecimal. The opad consists of B bytes equal to
0x5c in hexadecimal.

4. We truncate the result to its t leftmost bytes. We obtain HMACK (m).

We can prove that if H(K ⊕ ipad||m) deﬁnes a secure MAC on ﬁxed-length messages
and if H is collision-resistant, then HMAC is a secure MAC on variable-length messages
with two independent keys. However, we do not have any result on H(K ⊕ ipad||m).
More precisely, here is the security result.

Theorem 3.8 (Bellare–Canetti–Krawczyk 1996 [24]). Let H be a hash function
which hashes onto (cid:9) bits. Given K1, K2 ∈ {0, 1}(cid:9) we consider the following MAC
algorithm.

MACK1,K2(m) = H(K2||H(K1||m))

Assuming that H is collision-resistant and that m (cid:11)→ H(K2||m) is a secure MAC al-
gorithm for messages m of ﬁxed length (cid:9), then MAC is a secure MAC algorithm for
messages of arbitrary length.

Hence, provided that HMAC is indistinguishable from the construction in this theorem,
HMAC is a secure MAC algorithm as well.
Proof. In order to avoid confusion, we call m (cid:11)→ H(K2||m) the small MAC and
MACK1,K2 the big MAC. We assume that we have an adversary A for the big MAC.
We construct an adversary A(cid:8)

for the small MAC by simulation as follows.

1. We pick a random K1 and we simulate A.
2. Every time A sends a query Xi to the oracle, we compute H(K1||Xi ) and we
query the small MAC oracle with the result. We get ci which is returned to A
as the answer to the query.

90

Chapter 3

ci

K1

MAC

ci

H(K1||Xi)

Adversary

Xi

(X,c)

K1

(H(K1||X),c)

Figure 3.14. Adversary reduction by simulation in HMAC.

3. When A terminates by giving a forged (X, c) pair, we compute H(K1||X). If
the result is equal to one previously computed H(K1||Xi ), then this provides
a collision on H. But this was assumed to be infeasible. Hence, H(K||X) is
different from all previously queried H(K1||Xi ), and (H(K1||X), c) is thus a
forgery for the small MAC.

The simulation is depicted in Fig. 3.14.

(cid:1)

3.4.7 An Authenticated Mode of Operation

We conclude on message authentication codes with an example of mode of operation
which combines encryption and MAC at the same time. Namely, those combined modes
of operation can be used to achieve conﬁdential communications which need to have
strong integrity and authentication protection. They are called authenticated modes of
operation. Several authenticated modes of operation exist. The most popular one at this
time is called counter with CBC-MAC (CCM). It is designed to be used with AES or
any other block cipher which uses 128-bit blocks.

As its name suggests, the CCM mode combines the CTR mode and CBC-MAC.
Roughly speaking, the authenticated encryption of a message m is made by ﬁrst com-
puting the raw CBC-MAC T of m, and then encrypting T||m in CTR mode. The attacks
that we have seen on the raw CBC-MAC are thwarted by the encryption of the output
tag.

More precisely, the CCM uses two parameters: the size M (in bytes) of the CBC-
MAC tag T which must be even and lie between 4 and 16, and the size L (in bytes) of
the ﬁeld which will encode the message length, which must lie between 2 and 8. Note
that M and L are encoded using 3 bits each, namely by the binary expansion of M−2
and L − 1 respectively. The value L = 1 is reserved.6

2

6 For instance, future applications using messages whose length encoding (in bytes) requires more than 8

bytes (i.e. whose length is larger than 2568 bytes, or 234 GB) may later use this value.

Dedicated Conventional Cryptographic Primitives

91

A plaintext m is processed together with a key K to be used with the block cipher,
a nonce N of 15 − L bytes, and an additional authenticated data a which is not meant
to be encrypted. For instance, a can be a sequence number in a communication session,
or a packet header to be authenticated.

To compute the CBC-MAC tag T with an empty a, we ﬁrst compute a 128-bit block
B0, we then split m into a block sequence B1, . . . , Bn (if necessary, the last block Bn is
padded with zero bytes to make a full 128-bit block), we compute the raw CBC-MAC
of B0||B1|| . . .||Bn, and we take the M leftmost bytes T of the result. The initial block
B0 is formatted by

B0 = ﬂag||N||(cid:9)(m)

where N is the nonce (of 15 − L bytes), (cid:9)(m) is the length (in bytes) of m (of L bytes),
and ﬂag is a byte which is formatted by

ﬂag = 0||adata||M||L

where M and L are the encodings of the respective parameters on two 3-bit strings,
adata is a bit set to zero when the data a is of length zero. The leading bit 0 is reserved.
When the data a is of nonzero length, adata is set to one and a few blocks are inserted
between B0 and B1. Those blocks consist of the encoding of the length of a followed
by a, then padded, if necessary, with zero bytes so that it can split into an integral
number of blocks. The encoding rule for the length of a depends on the size of a.
For instance, when a consists of at most 65,279 bytes, the length of a is encoded on
2 bytes.

Following a counter mode, we construct a sequence of counter blocks A0, A1,

A2, . . . by formatting them by

Ai = ﬂag||N||i

where N is the nonce (of 15 − L bytes), i is the counter (encoded with L bytes), and
ﬂag is a byte whose three rightmost bits encode L and all others are basically set to
zero.

To encrypt T , we XOR it to the ﬁrst M bytes of CK (A0) where C is the block cipher.
To encrypt the message m, we XOR it to the ﬁrst (cid:9)(m) bytes of CK (A1)||CK (A2)||···.
Processing m ﬁnally yields the concatenation of the two ciphertexts.

Decryption is quite straightforward from M, K , and N . Note that we can decrypt
on the ﬂy. We can also compute the CBC-MAC on the ﬂy and do the ﬁnal check with
the decrypted T .

92

Chapter 3

Generator

r1, . . . ,rd

Adversary

rd+1

Figure 3.15. Adversarial model for a pseudorandom generator.

3.5 Cryptographic Pseudorandom Generators

3.5.1 Usage and Threat Model

Regular probabilistic algorithms use pseudorandom generators. They usually have to
look like random to statistical tests. Secure application also need randomness, like
simply choosing a secret key. Cryptographic pseudorandom generators need to be
robust against malicious adversaries who will try to predict new random generations
as depicted in Fig. 3.15.

For instance, if we use the Vernam cipher with a secret key generated by a cryp-
tographic pseudorandom generator, we do not want an adversary who can perform a
known plaintext attack to be able to predict the next key, otherwise the encryption is
not secure against a known plaintext attack.

3.5.2

(cid:1)Congruential Pseudorandom Generator

A famous pseudorandom generator which was ﬁrst proposed for secure applications
is the congruential pseudorandom generator. A general version of it is deﬁned by a
parameter k and k functions ϕ1, . . . , ϕk from Zk to Z. The pseudorandom generator
is deﬁned by a secret modulus m, secret coefﬁcients α1, . . . , αk, and a secret initial
state s1, . . . , sk. The time counter is initially set to i = k. For a new generation, we ﬁrst
increment i and output

si = k(cid:1)

j=1

α j ϕ j (si−k , . . . , si−1) mod m.

This pseudorandom generator is quite weak as shown by Joan Boyar and
Hugo Krawczyk (see Refs. [36, 106, 107]). Assuming that an adversary has seen

Dedicated Conventional Cryptographic Primitives

93

si−k , . . . , si−1, he can compute a vector deﬁned by

⎛
⎜⎜⎜⎝

Bi =

⎞
⎟⎟⎟⎠ .

ϕ1(si−k , . . . , si−1)
ϕ2(si−k , . . . , si−1)

ϕk(si−k , . . . , si−1)

...

Once he gets more than k vectors, say for instance B1, . . . , Bi , he can deduce a linear
relationship

γi Bi = i−1(cid:1)

j=0

γ j B j

with integral coefﬁcients γ1, . . . , γi . Now since we know that si is a scalar product
between Bi and a secret constant vector modulo a secret modulus m, we deduce that

γi si ≡ i−1(cid:1)

j=0

γ j s j

(mod m).

This is used to get an estimate of m by the difference

γi si − i−1(cid:1)

j=0

γ j s j .

More precisely, we attack the generator as depicted in Fig. 3.16. Here ¯m is a multiple of
m, which approximates m with increasing precision. After a few steps, we get ¯m = m,
and we can recover the αi by solving a linear system modulo m.

3.5.3 Practical Examples

We mention three practical cryptographic pseudorandom generators.

First, we notice that the OFB mode and CTR mode of block ciphers consist of trans-
forming the block cipher into a pseudorandom generator to be used with the Vernam
cipher. The OFB and CTR modes thus already deﬁne how to make a pseudorandom
generator from a block cipher.

Second we mention the ANSI X9.17 standard generator (Ref. [2]) based on Triple-
DES: The generator uses a timestamp and an internal seed. The new generation r is

94

Chapter 3

Input: B0, B1, . . .
Output: m
1: start with i = k + 1
2: repeat
3:

relationship γi Bi =(cid:26)

i−1
j=0

γ j s j

γ j B j with integers

increment i
if ¯m (cid:4)= 0 then

compute x =(cid:26)
get a linear
γ0, . . . , γi with a gcd equal to 1
i−1
j=0
pick ¯m = x − γi si
repeat
get a linear relationship γi Bi ≡(cid:26)
(mod ¯m) with
compute x =(cid:26)
integers γ0, . . . , γi with a gcd equal to 1 and where γi is a
factor of ¯m
replace ¯m by the gcd of ¯m and x − γi si
end if
14: until ¯m (cid:4)= 0
15: output ¯m

until ¯m is stable or equal to 0

i−1
j=0

γ j B j

i−1
j=0

γ j s j mod ¯m

4:
5:
6:
7:
8:
9:

10:
11:
12:
13:

Figure 3.16. Attack on the linear congruential pseudorandom generator.

deﬁned by

J = Enc(timestamp)
r = Enc(J ⊕ Seed)

and the seed is replaced by

NextSeed = Enc(J ⊕ r).

Finally we mention the Yarrow-160 dedicated generator as proposed by John
Kelsey, Bruce Schneier, and Niels Ferguson from the Counterpane System Company
(see Ref. [101]). Here, there are two layers of pseudorandom generators. In the bot-
tom layer, the generation is simply EncK (counter), where counter is incremented after
each generation and K is a pseudorandom secret generated by the upper layer. This
key is changed regularly (every 10 generations in Yarrow-160). In the upper layer, the
generated key is simply the hashed value of a counter and an “entropy accumulator.”
This accumulator is fed, for instance, by all interrupt information from the hardware
which are supposed to be random, with odd distribution. The aim of this generator is to
accumulate the randomness and to convert it to a random key with a purer distribution.
This is an example of a nondeterministic pseudorandom generator. A similar approach
was adopted for Linux in the random.c library.

Some other ad hoc examples will be seen in Chapter 12.

Dedicated Conventional Cryptographic Primitives

95

3.6 Exercises

Exercise 3.1. We modify MD5 by suppressing the padding scheme (we pad with 0 bits
only). Exhibit a collision.

Exercise 3.2. We modify MD5 by suppressing the Davies-Meyer scheme: C is C0.
Prove that we can mount an inversion attack within a complexity of 264: for any target
digest h, we can ﬁnd a message m for which MD5(m) = h.

(Hint: perform a meet-in-the-middle attack.)

Exercise 3.3. We iteratively pick random elements in {1, 2, . . . , N} in an independent
and uniformly distributed way until we obtain a collision. Compute the expected number
of trials.

Exercise 3.4. Let x be a set. We call ( p, q)-multipermutation on X any function f from
X p to X q with the following property: for any two different tuples (x1, . . . , x p+q) such
that

(x p+1, . . . , x p+q) = f (x1, . . . , x p)

at least q + 1 coordinates take different values.

Show that f1 and f2 in MD4 are not (3, 1)-multipermutations. Show that f3 is a

(3, 1)-multipermutation.

Show that the 2-PHT transform in SAFER is not a (2, 2)-multipermutation. Show

that the M function in CSC is a (2, 2)-multipermutation.7
Exercise 3.5. Let K be a ﬁnite ﬁeld of order k. We deﬁne H(x) = Ax for a random
variable A uniformly distributed in K.

Show that H is 1

k -universal. Is it strongly-universal? How would you modify H to

make it so?

We now consider K= {0, 1}(cid:9) as a ﬁnite ﬁeld of order 2(cid:9). Show that H is 2

−(cid:9)-XOR-

universal.

7 This exercise was inspired by Ref. [179].

4
(cid:1)Conventional Security Analysis

Content

(cid:1)Attack methods: differential cryptanalysis, linear cryptanalysis
(cid:1)Security analysis: nonlinearity, Markov ciphers
(cid:1)Security strengthening: indistinguishability, dedicated construction, decorrelation

Previous chapters presented brute force attacks and dedicated attacks. This chapter
investigates classical general attack methods for conventional cryptographic algorithms
(namely, differential and linear cryptanalysis), and different ways to strengthen the
security in primitive design or to estimate the resistance against attacks. For further
readings we recommend the tutorial Ref. [90] of Howard Heys on differential and linear
cryptanalysis.

4.1 (cid:1)Differential Cryptanalysis

The idea of differential cryptanalysis is originally due to Eli Biham and Adi Shamir
from the Weizmann Institute in Israel.1 It assumes a chosen plaintext attack model:
the adversary can play with the encryption device as a black box, submitting chosen
plaintexts and getting ciphertexts in return (see Fig. 4.1). The aim of the attack is to
recover the secret key.

The basic idea of differential cryptanalysis is to investigate differential behaviors:
we submit pairs of random plaintext blocks the difference of which is a ﬁxed value a.
We then look at the corresponding ciphertext difference until it is a ﬁxed value b. A
ﬁrst analysis phase consists of looking for good a and b values in a heuristic way. A
crucial quantity is the differential probability deﬁned by

DP f (a, b) = Pr[ f (X + a) = f (X) + b]

where f is the encryption function and X is a uniformly distributed random variable.
The higher this probability is, the more efﬁcient the attack is. Additional dedicated
tricks enable the analysis of complicated ciphers by using differentials on simpliﬁed
variants.

We illustrate the differential cryptanalysis paradigm by the example of DES re-

duced to eight rounds instead of sixteen.

1 See Refs. [28–31].

98

Chapter 4

Ciphertexts

Enc

Plaintexts

Adversary

K

Figure 4.1. Chosen plaintext attack.

Let f be the ﬁve ﬁrst rounds of DES and let us deﬁne a and b in hexadecimal by

a = 405c0000 04000000,

b = 04000000 405c0000

The following heuristic analysis suggests that
DP f (a, b) ≈ 2

−13.4

which we summarize by

405c0000 04000000

−−−−−−−−−→

f

DP≈2−13.4

04000000 405c0000.

In order to trace how this difference propagates throughout the DES design, we will
analyze the S-boxes input and output differences. Hexadecimal notations are convenient
for the 64-bit blocks. However, since S-boxes have inputs of 6 bits, it is more convenient
to represent the 48-bit input differences in octal: any two consecutive octal digits
represent the input of a single S-box.

Assuming that the input difference of the right half of the ﬁrst round is 04000000
in hexadecimal, this corresponds to the input of the ﬁrst round function. After the
expansion, the difference is 00 10 00 00 00 00 00 00 in octal (see Fig. 4.2). This means
that the input of S2 is the only modiﬁed one. Next we observe that

DPS2(10, a) = 1
4

which means that an input difference of 10 in S2 will lead to an output difference of
a in hexadecimal (1010 in binary) with probability 1
4 . After the permutation in the
round function, we obtain an output difference of 40080000 in hexadecimal. This
means that the round function F of DES is such that

DPF (04000000, 40080000) = 1
4

.

(cid:1)Conventional Security Analysis

99

405c0000

Round #1

40080000

?⊕
?

P

0a000000

?

S

00 10 00 00

?

00 00 00 00

E⊕

04000000

?

04000000

Round #2

04000000

?⊕
?

P

00100000

?

S

00 00 12 30

?

00 00 00 00

E⊕

00540000

?

00540000

Round #3

?⊕
?

0

0

P

?

0

S

?

E⊕

0

?

0

Round #4

04000000

?⊕
?

P

00100000

?

S

00 00 12 30

?

00 00 00 00

E⊕

00540000

?

00540000

Round #5

40080000

?⊕
?

P

0a000000

?

S

00 10 00 00

?

00 00 00 00

E⊕

04000000

?

04000000

Round #6

?⊕
?

P

?0??0000

?

S

14 00 13 70

?

00 00 00 00

E⊕

405c0000

?

405c0000

Round #7

?⊕
?

P

?

S

?

E⊕

?

?

Round #8

........

?⊕
?
?
?

P

?.??....

?

S

?? .. ?? ??
.. .. .. ..

?

E⊕

........

?

04000000

p = 1
4

00540000

p = 5
128

0

p = 1

00540000

p = 5
128

04000000

p = 1
4

405c0000

p = 1

?

p = 1

........

p = 1

?

........

Figure 4.2. Differential cryptanalysis of DES reduced to eight rounds.

100

Chapter 4

The output difference of 40080000 XORs onto the difference of 405c0000 of the
left half of the DES input. We obtain a difference of 00540000. Therefore, after the
ﬁrst round, we have a difference of 04000000 00540000 with probability 1
4 .

If this holds, the input difference of 00540000 of the second-round function leads
to an input difference (in octal) of 00 00 12 30 00 00 00 00 to the S-boxes. From the
deﬁnition of S3 and S4 we obtain that

DPS3(12, 1) = 10
64

, DPS4(30, 0) = 16
64

.

× 16

We thus obtain an output difference of 00100000 from the S-boxes with probability
10
128 . After the permutation, this difference becomes 04000000. This means
64
that

= 5

64

DPF (00540000, 04000000) = 5
128

.

The output difference of 04000000 ﬁnally XORs onto the difference of 04000000,
which therefore vanishes. Hence, after the second round, we have a difference of
00540000 00000000 with probability 1
4

= 5
512 .

× 5

128

The input difference of zero in the third-round function leads to an output difference
of zero with probability 1. Therefore, after the third-round, we have a difference of
00000000 00540000 with probability 5

512 .

In the fourth round, we can use the same property as that in the second round,

namely

DPF (00540000, 04000000) = 5
128

.

Thus after the fourth round, we have a difference of 00540000 04000000 with proba-
bility 5
512

= 25
216 .

× 5

128

In the ﬁfth round, we can use the same property as that in the ﬁrst round, namely

DPF (04000000, 40080000) = 1
4

.

The 40080000 XORs onto 00540000 and leads to 405c0000. Hence, after the
=
ﬁfth round, we have a difference of 04000000 405c0000 with probability 25
218 ≈ 2
−13.4. This explains where the announced ﬁve-round differential characteristic
25

216 × 1

4

comes from.

(cid:1)Conventional Security Analysis

101
If x and x + a are plaintext blocks such that this characteristic holds, we call
(x, x + a) a right pair. Otherwise we call it a wrong pair. The above heuristic analysis
shows that we have at least a fraction of 2

−13.4 of right pairs.

After the expansion in the sixth round, the input difference of 405c0000 leads

to an input difference of

14 00 13 70 00 00 00 00

for the S-boxes: the input difference of S2, S5, S6, S7, and S8 are zero. Thus the output
difference of these S-boxes is zero. The output difference of these S-boxes in the eighth
round is XORed to (1) the output differences of the same S-boxes in the sixth round (thus
zero) and (2) the corresponding bits from the input difference in the sixth round, which
are known (provided that we have a right pair) in order to produce some ciphertext-
difference bits. Thus we can compute the inputs of the corresponding S-boxes in the
eighth round and the output differences for the right pairs. Next we can try all possible
corresponding key bits (six per S-box, which gives 30 bits) in order to suggest some
30-bit combinations.

In Fig. 4.2, unknown differences are represented by question marks, and com-
putable values (from the ciphertext pairs) are represented by dots. Thus for each ci-
phertext pair, we compute two vectors of 30 bits which are the inputs of S2, S5, S6, S7, S8
before the XOR with the subkey and one vector of 20 bits which is the output difference
of these S-boxes, provided that we have a right pair. Each of these vector triplets will
be consistent with some 30-bit subkey vector.

Now we use counters for every 30-bit combination. Namely, we make N exper-
iments in which we query (x, x + a) pairs. For each pair we increment the counter
of suggested combinations. At the end we hope the right combination to have been
suggested many times in order to distinguish it (see Fig. 4.3).

The right combination is suggested for every right pair. So it is at least suggested
−13.4 for each experiment. Its counter will thus eventually be in
N p1. This is called the signal.

with probability p1 ≈ 2
the range of N p1 ± √
are thus eventually in the range of N p2 ± √

Any other combination is suggested with probability p2 ≈ 2

−20. Their counters

N p2 and are considered as noise.

The signal over noise ratio is thus p1/ p2 = 100, which is high enough. We still

need to choose N such that(cid:21)

(cid:21)

N p2 <

N p1 (cid:15) N ( p1 − p2)

in order to separate the two distributions, which means N (cid:24) 1/ p1 ≈ 213.4.

102

Chapter 4

Precomputation:
1: for i = 2,5,6,7,8, u,u

a set SubCanditatei,u,u(cid:5),v to the empty set

2: for i = 2,5,6,7,8, for u,u

SubCanditatei,u,u(cid:5),Si(u⊕k)⊕Si(u(cid:5)⊕k)
Attack:
3: collect N pairs (x, y) and (x⊕ a, y

(cid:5) ∈ {0, . . . ,63} and v ∈ {0, . . . ,15} initialize
(cid:5),k ∈ {0, . . . ,63}, insert k in the set

(cid:5)) of plaintext-ciphertext pairs and
let yR (resp. yL) denote the right (resp. left) half of y, as well as for
(cid:5)
y

4: for each pair do
5:

compute u = E(yR) and u
04000000)

(cid:5) = E(y
(cid:5)
R

) and v = P

−1(yL ⊕ y
(cid:5)
L

⊕

(cid:5)
i

6: make the set product Candidate of all SubCanditateui,u

,vi for
(cid:5)
i = 2,5,6,7,8 where ui (resp. u
i) denotes the i-th 6-bit packet
(cid:5)
) and vi denotes the i-th 4-bit packet of v, i.e.
of u (resp. u
suggest in Candidate all 30-bit subkeys k2k5k6k7k8 for which
ki ∈ SubCandidateui,u
for all k in Candidate, increment a counter nk

7:
8: end for
9: sort all possible 30-bit subkeys k in decreasing order of nk
10: for each possible 30-bit subkey k, exhaustively look at all the re-

(cid:5)
i

,vi

maining 26 bits and try the corresponding key

Figure 4.3. Differential cryptanalysis of eight-round DES.

After this attack we recover 30 out of the 48 bits of the eighth subkey. The missing
56 − 30 = 26 bits can be found with an exhaustive search, whose cost is negligible
against the cost of recovering these 30 bits. Therefore we have a chosen plaintext attack
against eight rounds of DES which enables the discovery of the full key after a number
of plaintext pairs with the order of magnitude of 213.4. The complexity is essentially
in obtaining the corresponding ciphertexts and managing 230 counters. (This last cost
can be efﬁciently decreased by using further dedicated algorithmic tricks.)

The initial development of differential cryptanalysis techniques made possible to
break reduced versions of DES (as we have just shown) and some DES-like block
ciphers. Later, the cryptanalysis technique was improved and optimized. This led to an
attack on the full DES which requires 247 chosen plaintexts to be successful. Interest-
ingly, the attack is scalable in the sense that having fewer chosen plaintexts leads to
an attack with a reduced probability of success. Biham and Shamir have also shown
in 1991 that many slight modiﬁcations in the design of the S-boxes (even swapping
two existing S-boxes) lead to an impressive complexity breakdown (see Ref. [31]).
This suggests that the designers of DES indeed tried to make differential attacks as
hard as possible. Later, Don Coppersmith, one member of the original DES design
team at IBM, released in 1994 a technical report (Ref. [48]) explaining that DES was
made in order to optimally resist differential cryptanalysis attacks. This shows that the
research development of American standardization bodies was indeed 15 years ahead
of its time in the early seventies. However, because of the massive growth of industrial

(cid:1)Conventional Security Analysis

103

Ciphertexts

Enc

Plaintexts

Adversary

K

Figure 4.4. Known plaintext attack.

and academic activities in cryptography, it is likely that this gap, if still positive, has
substantially reduced.

4.2 (cid:1)Linear Cryptanalysis

A dual idea to differential cryptanalysis was invented by Mitsuru Matsui at Mitsubishi
Electronic (see Ref. [124, 125]) based on previous works from Henri Gilbert and his
colleagues of France Telecom (see Ref. [74, 75, 178]) and by the independent discovery
of anomalies in the S-boxes of DES by Matt Franklin and Adi Shamir.2 It has been
called linear cryptanalysis. In this method, instead of trying to keep track of difference
propagation by chosen plaintext attacks, we try to keep track of Boolean information
which is linearly obtained by a known plaintext attack: if we get one (x, C(x)) pair,
we make a statistical analysis of the Boolean information L(x, C(x)) and deduce some
information on the secret key (see Fig. 4.4).

We remind that all scalar linear mappings can be represented by a dot product with
a constant vector: given x ∈ {0, 1}m and a linear mapping ϕ : {0, 1}m → {0, 1} there
exists a unique a such that for any x we have

ϕ(x) = a · x = a1x1 ⊕ a2x2 ⊕ ··· ⊕ am xm .

The ﬁrst step of linear cryptanalysis on a function f
thus consists of ﬁnding some
good a and b vectors in a heuristic way so that (a · X) ⊕ (b · f (X)) is a biased random
variable. A crucial quantity is thus

Pr[a · X = b · f (X)].

For normalization reasons, which will be made clear in the next sections, we deﬁne

LP f (a, b) = (2 Pr[a · X = b · f (X)] − 1)2

2 See Ref. [165]. The Reference to the Master Thesis of Franklin can be found in the same reference.

104

Chapter 4

Computation circuit

Mask circuit

X

Y

a

a

⊕

⊕

Z

Z = X ⊕Y

a

a· Z = (a· X)⊕ (a·Y )

Figure 4.5. Dual circuit of a XOR gate.

over the uniform distribution of X. In a second step, we collect many (xi , f (xi )) pairs
for i = 1, . . . , N and we use the biased Boolean information in order to recover some
statistical information.

Let us start with a few preliminaries.
We assume that a keyed function f from {0, 1} p to {0, 1}q is described by a circuit
which contains only substitution boxes, XOR gates, key gates, and duplicate gates. The
circuit is an acyclic directed graph which can be represented into layers such that all gate
inputs in some layer i are outputs from gates in layer i − 1. Layer 1 includes p input bits
of f and key gates. The last layer includes q output bits of f . Starting from the output of
f , we consider b · f (x). This means that we consider a bit mask b applied to the last layer
of the circuit. This mask indicates which output bits from the layer we are considering
to XOR together. We can now propagate the mask by applying the following rules.

w, the mask on the previous layer will include v and w (see Fig. 4.5).

r If the mask includes a bit u which is output from a XOR gate with inputs v and
r If the mask includes two bits u and v which are output from a duplicate gate
with input w, the mask on the previous layer will not include w (see Fig. 4.6
with a = b).
r If the mask includes one and only one bit which is output from a duplicate gate
with input w, the mask on the previous layer will include w (see Fig. 4.6 with
a (cid:4)= b).

Computation circuit

X

Mask circuit

a⊕ b

?
Y

?
Z

X = Y = Z

?
a

?
b

(a·Y )⊕ (b· Z) = (a⊕ b)· X

Figure 4.6. Dual circuit of a duplicate gate.

(cid:1)Conventional Security Analysis

105

Computation circuit

Mask circuit

X
?

S

?
Y

Y = S(X)

b
?

B

?
a

a·Y = b· X ⊕ B

Figure 4.7. Dual circuit of a substitution box.

r If the mask includes some output bits from a substitution box S, namely some
β · S(u), the mask on the previous layer will include some α · u so that the
correlation between α · u and β · S(u) is signiﬁcant. We let B = (α · u) ⊕ (β ·
S(u)) be the bias bit which is associated to this substitution box (see Fig. 4.7).

Piling up all propagations, we obtain that

(b · f (x)) ⊕ (a · x) = (c · k) ⊕

(cid:25)

Bi

i

where k is the key vector and the Bi ’s are all bias vectors that we met in the substitution
boxes.

Let us now introduce the following lemma.

Lemma 4.1 (Piling-up lemma). For any Boolean variable B we deﬁne LP(B) =
(2 Pr[B = 0] − 1)2. Let B1, . . . , Bn be n independent random variables. We have

LP(B1 ⊕ ··· ⊕ Bn) = LP(B1) × ··· × LP(Bn).
To prove this, we simply observe that LP(B) = (E((−1)B))2,
(−1)α(−1)β, and we use the independence of the Bi ’s.

that (−1)α⊕β =

Going back to the propagation of masks, we now assume that the input of f is a
random variable X and that the key k is ﬁxed. We further make the heuristic assumption
that all Bi ’s are independent. We obtain from the piling-up lemma that

(cid:27)

LP f (a, b) =

LP(Bi ).

i

Hence, by making sure that all LP(Bi ) are large, then the product LP f (a, b) will be
large. We also have to make sure that the number of Bi biases is as low as possible.
This means that we have as few “active substitution boxes” as possible. Unfortunately
there exists no general way to ﬁnd these efﬁcient a and b masks, nor to approximate
LP f (a, b) with the above techniques. We only have heuristic ways to ﬁnd it.

106

Chapter 4

As an illustration we consider eight rounds of DES. As depicted in Fig. 4.8, the

output mask after the seventh round is

and the input mask is

b = 21040080 00008000

a = 01040080 00011000

so that if f is the function which maps a plaintext into the output from the seventh
round, we have

(a · x) ⊕ (b · f (x)) = cste ⊕ B1 ⊕ B3 ⊕ B4 ⊕ B5 ⊕ B7

where cste is a constant bit which depends on the key and the Bi ’s are bias bits in round
i. Note that B1, B3, B5, and B7 are bias bits around the S5 substitution box of DES
whereas B4 is a bias bit around S1. Due to the piling-up lemma we obtain

(cid:16)

LP f (a, b) =

16 × 10 × 2 × 20 × 20

325

(cid:16)

(cid:17)2 =

125
215

(cid:17)2 ≈ 2

−16.

Note that f consists of the ﬁrst seven rounds. The last round of eight-round DES
is a little special since we will use the ciphertext and a guess for some key bits in order
to compute the input of S1, to deduce the output of S1, and to subtract the bit masked
by 00008000 from the ciphertext. This would lead us to the output from the seventh
round, provided that the guess for the key bits is correct. This means that we have
indeed a characteristic property of the right guess: for each guess κ of the 6 bits which
are used in order to compute the input of S1 in the last round, we can make statistics
on the random bit (a · x) ⊕ (b · f (x)), where b · f (x) is computed from C(x) and κ. If
guess κ is not correct, this random bit is approximately a uniformly distributed one. If
guess κ is correct, this random bit has a bias LP approximately equal to λ = 2

−16.

More concretely, let X be a uniformly distributed random variable which represents
a plaintext, and let Y = C(X) be the ciphertext after we have reduced DES to eight
rounds with an unknown secret key. We let U be equal to the 6 bits of the right half of
Y which will input to S1 in the last round after a XOR with 6 key bits. We also let V
be a bit equal to

V = (a · X) ⊕ (bL · YR) ⊕ (bR · YL)

where b = bLbR and Y = YLYR, i.e. bL (resp. bR) denotes the left (resp. right) half of
b and the same for Y .

If κ is the right guess for the 6 bits of the key, we are interested in the bit

W κ = V ⊕ (bR · P(S1(U ⊕ κ)||0000000))

(cid:1)Conventional Security Analysis

107

01040080

Round #1

⊕

0

Round #2

⊕

01040080

Round #3

⊕

00008000

Round #4

⊕

21040080

Round #5

⊕

0

Round #6

⊕

21040080

Round #7

⊕

01040080

0000e000

P

S

00 00 00 00

42 00 00 00

E⊕

00011000

LPS5(42, e) = (16/32)2

00011000

P

S

E⊕

01040080

0

01040080

0000e000

P

S

00 00 00 00

20 00 00 00

E⊕

00008000

LPS5(20, e) = (10/32)2

00008000

40000000

P

S

20 00 00 00

00 00 00 00

E⊕

20000000

LPS1(20, 4) = (2/32)2

01040080

21040080

0000f000

P

S

00 00 00 00

20 00 00 00

E⊕

00008000

LPS5(20, f) = (20/32)2

00008000

P

S

E⊕

21040080

0

21040080

0000f000

P

S

00 00 00 00

20 00 00 00

E⊕

00008000

LPS5(20, f) = (20/32)2

00008000

21040080

Round #8

⊕

00008000

40000000

P

S

?? .. .. ..
.. .. .. ..

??.....?

E⊕

00008000

........

Figure 4.8. Linear cryptanalysis of DES reduced to eight rounds.

2

√

+ 1
2 (−1)β

108
Chapter 4
where P is the permutation in the DES round function and || denotes the concatenation.
This means that we consider the output bits of S1 only. If κ is correct then Pr[W κ =
1] = 1
λ, where β is an unknown constant that depends on some key bits.
Collecting N independent samples (Xi , Yi ) of (X, Y ) for i = 1, . . . , N , we compute N
= 1. Clearly,
i of W κ. Let cκ be the number of i’s such that W κ
independent samples W κ
2 (−1)β
− 1
− 1
i
√
the expected value of cκ
2 is 1
λ. We make
4N
N
λ, i.e. we neglect λ so that the standard deviation
approximations to the ﬁrst order of
√
is approximately 1
(cid:23)
2

√
λ and the variance is 1
4N

. As N increases, the central limit theorem states that

(cid:22)

N

√

Pr

cκ
N

− 1
2

√
t

<

2

≈ 1√
2π

− (x−(−1)β

2

λN)2

dx.

(cid:20)
−∞ e

t

We deduce that

(cid:22)(cid:28)(cid:28)(cid:28)(cid:28) cκ

N

Pr

(cid:28)(cid:28)(cid:28)(cid:28) <

− 1
2

(cid:23)

√
t

2

N

≈ 1√
2π
= 1√
2π
= 2√
2π

N

(cid:20)
(cid:20)

t
−t
t

(cid:20)

e

0
− λN

2

e

√

λN)2

− (x−(−1)β

2

e

dx

− (x−(−1)β

2

√
λN)2

− x2

t

e

0

2 cosh

(cid:20)

− (x+(−1)β

2

t

e

√
λN)2

dx

0

dx.

dx + 1√
(cid:18)
2π

(cid:19)

√
λN

x

If κ is not the right guess, we can compute W κ

we can approximate the expected value and standard deviation of cκ
N
respectively. A similar analysis leads to

i and cκ by the same formula and
√
2 by 0 and 1
2
N

− 1

(cid:20)

(cid:23)

(cid:28)(cid:28)(cid:28)(cid:28) <

(cid:22)(cid:28)(cid:28)(cid:28)(cid:28) cκ
(cid:28)(cid:28) be the grade for a guess κ. Let T be the triangle {(x, y) ∈

≈ 2√
2π

− 1
2

2 dy.

√
t

− y2

N

N

2

e

0

t

Pr

Let gκ =(cid:28)(cid:28)cκ − N

R2; x ≥ y ≥ 0}. We obtain that the probability that the grade of the right candidate is
smaller than the grade of a given bad candidate is approximately

2

(cid:20)(cid:20)

(cid:18)

(cid:19)

√
λN

cosh

x

dx dy.

p = 1 − 2

π

− λN

2

e

− x2+y2

2

e

T

This is a decreasing function in terms of r = λN , which runs from p = 1
to p = 0 (for λ = +∞). We have
(cid:20)(cid:20)

2 (for λ = 0)

e

2

− r

(cid:20)(cid:20)

p = 1 − 2
= 1 − 1

π

π

e

T

T

e
− (x−√

− x2+y2

2

r)2+y2
2

cosh(x
dx dy − 1

π

√
r)dxdy

(cid:20)(cid:20)

− (x+√

r)2+y2
2

dx dy.

e

T

(cid:1)Conventional Security Analysis

Let us write this sum as p = 1 − p1 − p2. In the second integral we let x = −√
ρ cos θ and y = ρ sin θ. Variable θ is between 0 and π
√
2r /(2 cos(θ + π/4)) to +∞. Hence
⎛
⎞
⎝(cid:20) +∞
(cid:20) π
⎠ dθ
(cid:20) π

109
r +
4 in T . Then ρ goes from

p2 = 1

2 cos(θ+ π
4 )

2 dρ

− ρ2

√
2r

ρe

π

0

4

−

4

e

r

4 cos2(θ+ π

4 ) dθ.

= 1
π

0

0 and π in T . When θ is between 0 and π
between π

In the ﬁrst integral we let x = √
r + ρ cos θ and y = ρ sin θ. Variable θ is between
4 , then ρ goes from 0 to +∞. When θ is
4 and π, then ρ goes from 0 to −√
2r /(2 cos(θ + π/4)).
(cid:2)(cid:20) −
(cid:20) π
(cid:16)
(cid:20) π
(cid:20) π

p1 = 1
4
= 1
4

+ 1
π
+ 1
π

4 cos2(θ+ π
4 )

2 cos(θ+ π
4 )

1 − e
−

(cid:3)

(cid:17)

2 dρ

− ρ2

√
2r

ρe

dθ

dθ

−

π
4

π
4

0

r

= 1 − 1

π

e

π
4

r

4 cos2(θ+ π

4 ) dθ.

When adding p1 and p2, by using the symmetry of cos2(θ + π
obtain

4 ) around θ = π

4 we

−

e

r

4 cos2(θ+ π

4 ) dθ

− r

4 cos2 θ dθ

p = 1

π
= 1
π

(cid:20) π
(cid:20) π

π
2

e

4

−π
4

(cid:20) π

p = 2

π

− r

4 cos2 θ dθ

4

e

0

By using the symmetry of cos θ around θ = 0 we obtain

Clearly, when 0 ≤ θ ≤ π

4 we have cos2 θ ∈ [ 1
2 ≤ p ≤ 1
− r
2

1
2

e

2

− r
4 .

e

, 1]. Therefore

Having a number of bad candidates equal to B, we deduce that the expected rank R of
the right candidates is such that

B
2

2 ≤ R ≤ B
− λN
2

e

− λN
4 .

e

110

Chapter 4

Attack:
1: initialize 27 counters nu,v to zero for all possibles 6-bit values u and

all possible bits v.

2: collect n plaintext-ciphertext (x,y) pairs,
3: for each (x,y) pair do
4:

set u to the 6 leading bits of the expansion E(yR) of yR in the
round function
v ← (a· x)⊕ (bL · yR)⊕ (bR · yL)
increment nu,v

5:
6:
7: end for
8: for all possible κ do
9:
cκ = n−∑

compute

u,v

nu,v⊕(bR·(P(S1(u⊕κ)||0000000)))

10: end for
11: sort all possible κ in decreasing order of |cκ − n
12: do an exhaustive search by using the sorted list for κ.

|,

2

Figure 4.9. Linear cryptanalysis of eight-round DES.

Thus, in order to be the top candidate, it sufﬁces to have

N = 4

λ

log

B
2

.

Here we have roughly B = 26 bad candidates, so the right candidate for κ will be
−16 we

the top one in the sorted list of graded candidates for N = 13.86
deduce that N = 220 is far enough.

λ . Since λ ≈ 2

The complete algorithm is depicted in Fig. 4.9. In order to avoid computing cκ with
a loop of N iterations for the 26 possible values of κ, we can ﬁrst preprocess the N sam-
ples into some counters so that each κ requires a little computation from these counters
in order to compute cκ. As we can see, the complete algorithm includes several phases.

1. A collecting phase of complexity N in which we preprocess the samples into

the counters.

2. An analysis phase of complexity 26 in which we grade each candidate κ.
3. A sorting phase of similar complexity in which we sort all candidates.
4. A searching phase in which we look for the key using the information obtained

from the analysis.

Here, the last phase requires a complexity of 250 when the right candidate is top-ranked.
This complexity can be substantially decreased by applying a similar analysis based on
another characteristic so that we recover some other bits of the key. For instance, once
we have recovered the key bits corresponding to S1 in the ﬁnal round, we can look for
the key bits corresponding to S5 in the ﬁrst round. Here the bias of the characteristic
is even larger since we cancel the (16/32)2 factor. This leads to 6 additional bits on
the key so that only 44 are missing. Further improvements and experiments show that
using N = 221 we recover the full key within a few seconds of computations with a

(cid:1)Conventional Security Analysis

111

success rate of 99%. This attack can further be improved in order to break the full DES
by using 243 known plaintexts (see Ref. [125]).

4.3 (cid:1)Classical Security Strengthening

4.3.1

(cid:1)Nonlinearities

In order to measure the nonlinearity of a function f we deﬁne
DP f (a, b) = Pr[ f (X + a) = f (X) + b]

LP f (a, b) = (2 Pr[a · X = b · f (X)] − 1)2

DP f

max

LP f

max

= max
a(cid:4)=0,b

= max
a,b(cid:4)=0

DP f (a, b)

LP f (a, b)

where the probabilities hold over the uniform distribution of the random variable X.
The nonlinearity for differential cryptanalysis corresponds to DP, and the nonlinear-
ity for linear cryptanalysis corresponds to LP. DP f (a, b) actually corresponds to the
probability of the a → b differential characteristic for f . LP f (a, b) corresponds to the
LP bias of the a · x ⊕ b · f (x) bit. DP and LP are connected with the discrete Fourier
transform.

Theorem 4.2. If f

: {0, 1} p → {0, 1}q , for any a ∈ {0, 1} p and b ∈ {0, 1}q we have
LP f (a, b) = 2
− p
DP f (a, b) = 2
−q

(cid:1)
(−1)(a·α)+(b·β)DP f (α, β)
(cid:1)
(−1)(a·α)+(b·β)LP f (α, β)

α,β

α,β

Proof. We ﬁrst notice that

thus

2

1a·x=b· f (x) = (−1)(a·x)+(b· f (x)) + 1
(cid:3)2
(cid:3)2

(cid:2)
21− p
(cid:2)

1a·x=b· f (x) − 1

x∈{0,1} p

LP f (a, b) =

(cid:1)
(cid:1)
(cid:1)

x∈{0,1} p

x,y∈{0,1} p

=
= 2

− p

2

−2 p

(−1)(a·x)+(b· f (x))
(−1)(a·(x⊕y))+(b·( f (x)⊕ f (y))).

112
Chapter 4
We can now sum over all possible values for x ⊕ y = α and f (x) ⊕ f (y) = β by
counting the number of (x, y) pairs that satisfy these relations. It is 2 pDP f (α, β). So
we obtain

LP f (a, b) = 2
− p

(cid:1)
(−1)(a·α)+(b·β)DP f (α, β)

α,β

Next we compute

α,β

(cid:1)
(−1)(a·α)+(b·β)LP f (α, β)
(cid:1)
(cid:1)
(−1)(a·α)+(b·β)2
(−1)(α·u)+(β·v)DP f (u, v)
− p
(cid:1)
(cid:1)
(−1)((a⊕u)·α)+((b⊕v)·β).

DP f (u, v)

u,v

−q
2
= 2
−q
= 2

α,β

− p−q

u,v

α,β

The last sum is nonzero only for a = u and b = v in which case it is 2 p+q. Thus we
(cid:1)
obtain the second relation.

4.3.2

(cid:1)Characteristics and Markov Ciphers

Differential cryptanalysis is essentially heuristic. In order to make a formal study, we
need to have a good model for the underlying primitives. One model, based on Markov
ciphers, is due to Xuejia Lai, James Massey, and Sean Murphy.

Deﬁnition 4.3 (Lai–Massey–Murphy [111]). A random permutation C over a group
G is called a Markov cipher on G if for any a, b, x ∈ G we have
(cid:13)

(cid:12)

Pr[C(x + a) − C(x) = b] = E

DPC (a, b)

where the probability and the expectation hold over the distribution of the permutation
C.
A basic example is C(x) = C0(x + K ) where C0 is a random permutation and K is an
independent uniformly distributed key in a group.

The name “Markov cipher” refers to Markov chains. We recall that a Markov chain
is a sequence X1, X2, . . . of “oblivious” random variables, which means that for any i
and any a1, . . . , ai we have

Pr[Xi = ai|X1 = a1, . . . , Xi−1 = ai−1] = Pr[Xi = ai|Xi−1 = ai−1].

The probability that Xi = ai only depends on the “recent past” Xi−1 = ai−1. A Markov
chain is homogeneous if for any a and b the probability
Pr[Xi = b|Xi−1 = a]

(cid:1)Conventional Security Analysis

113

does not depend on i. (Transition probabilities do not depend on “time.”) The following
property links Markov ciphers and Markov chains.

= Ci ◦ ··· ◦ C1(x j ) for j = 1, 2 and Y i = Y i

Theorem 4.4. Let C1, . . . , Cr be independent Markov ciphers on a group G, let x1 and
− Y i
x2 be two plaintexts, and let Y i
for i = 1, . . . , r. The sequence Y 0, . . . , Y r is a Markov chain. If the distribution of
1
j
the Ci ’s is the same, the Markov chain is furthermore homogeneous.
Proof. Let E be the event {Y 1 = ω1, . . . , Y i−2 = ωi−2} We have

2

(cid:1)
Pr[Y i = ωi|Y i−1 = ωi−1, E]
(cid:1)

1

Pr[Y i = ωi , Y i−1
Pr[Ci (y + ωi−1) − Ci (y) = ωi ] Pr[Y i−1

= y|Y i−1 = ωi−1, E]
= y]

y

1

=

=

y

due to the independence between Y i−1 and Ci . This last probability is equal to
E(DPCi (ωi−1, ωi )) due to the deﬁnition of Markov ciphers. Therefore we have
Pr[Y i = ωi|Y i−1 = ωi−1, E] = Pr[Y i = ωi|Y i−1 = ωi−1].

The homogeneous property is trivial.

(cid:1)

The Markov cipher notion is a good model for differential cryptanalysis, as the

following result shows.

Theorem 4.5. With the notations of the previous theorem, we now consider x1 and x2
as random, independent, and uniformly distributed. For any differential characteristic
 = (ω0, . . . , ωr ) we have

(cid:16)

(cid:17)
[yi = ωi , i = 1, . . . , r|y0 = ω0]

E

Pr
x1,x2

(cid:12)

= r(cid:27)

i=1

(cid:13)

E

DPCi (ωi−1, ωi )

where the expectations are over the distribution of the ciphers.

We notice that the probability of the left-hand term is the probability of the differential
characteristic, which depends on the key, and we take the expectation over the distribu-
tion of the key. People usually make the hypothesis of Stochastic equivalence, which
says that what holds on average over the keys holds for any key, so that we can remove
the expectations in the above result and have

[yi = ωi , i = 1, . . . , r|y0 = ω0] ≈ r(cid:27)

Pr
x1,x2

DPCi (ωi−1, ωi ).

i=1

114

Chapter 4

Proof. The ratio between the two shifted ﬁrst terms of the equation is

Pr[yi = ωi , i = 1, . . . , r|y0 = ω0]
Pr[yi = ωi , i = 1, . . . , r − 1|y0 = ω0]
= Pr[yr = ωr|yi = ωi , i = 0, . . . , r − 1]
= Pr[yr = ωr|yr−1 = ωr−1]
= E

DPCr (ωr−1, ωr )

(cid:13)

(cid:12)

.

So applying this equality with different r’s, we obtain the result.

(cid:1)

We can add up all characteristics with the same ω0 and ωr and obtain the following

result.
Theorem 4.6. Let C1, . . . , Cr be independent Markov ciphers on {0, 1}m. For any
differential or linear characteristic (ω0, ωr ), we have

(cid:13) =
(cid:12)
DPCr◦···◦C1(ω0, ωr )
(cid:12)
(cid:13) =
LPCr◦···◦C1(ω0, ωr )

E

E

(cid:1)
(cid:1)

r(cid:27)
r(cid:27)

i=1

(cid:12)
(cid:12)

E

E

ω1,...,ωr−1

ω1,...,ωr−1

i=1

(cid:13)
(cid:13)

DPCi (ωi−1, ωi )

LPCi (ωi−1, ωi )

where the expectations are over the distribution of the ciphers.

The above result links the probability of what is called a multipath characteristic
(ω0, ωr ) with single-path characteristics (ω0, . . . , ωr ). There is indeed a cumulative
effect, although one single-path characteristic is often overwhelming.

4.3.3

(cid:1)Theoretical Differential and Linear Cryptanalysis

Classical studies of differential and linear cryptanalysis say that the complexity is
roughly the inverse of the appropriate DP or LP coefﬁcient. Hence a classical (and
heuristic) argument for the security of block ciphers consists of proving that there are
no high DP or LP coefﬁcients. We can prove the equivalence between the DP and LP
coefﬁcients and the complexity of the attacks in a more formal way. For this we need
to have a model of the attacks. Here we use the model in terms of a distinguisher.

As depicted in Fig. 4.10, a distinguisher is an algorithm which plays with a random
oracle and which ultimately outputs 0 or 1. Given a distribution over random oracles,
we can compute the probability that the algorithm outputs 1. We say that the algorithm
distinguishes a distribution from another if the probabilities are far away.

Distinguishers are classical tools for measuring randomness. They are also called

Turing tests.

(cid:1)Conventional Security Analysis

115

∗

C or C

y

x

Distinguisher

0 or 1

Figure 4.10. Distinguisher between C and C

∗

.

We say that a block cipher is secure if it cannot be distinguished from a truly random
permutation. This is actually a quite strong model of security since if we can break a
block cipher by decrypting a fresh ciphertext, we can a fortiori distinguish it from a
truly random permutation by checking that the decryption is correct with the oracle.

Distinguishers are also the basic tool for differential and linear cryptanalysis.
Linear cryptanalysis actually uses an approximation with an unexpectedly large bias
which can be used to distinguish the cipher from a truly random permutation. Similarly,
differential cryptanalysis uses a differential characteristic with an unexpectedly large
probability.

We can modelize a differential distinguisher as depicted in Fig. 4.11.

∗

Theorem 4.7. Given two random permutations C and C
over the same message space
{0, 1}m, where C
) be the difference in the
probability that the above distinguisher outputs 1 when the oracle implements the
distributions of C and C

is uniformly distributed, we let Adv (C, C

. We have

∗

∗

∗

(cid:16)

(cid:12)

(cid:13)(cid:17)

|Adv (C, C

∗

)| ≤ max

n

2m − 1

, n E

DPC (a, b)

.

Therefore the attack is meaningless until the number of chosen plaintext pairs n reaches
the order of magnitude of 1/E(DPC (a, b)).

Parameters: a complexity n, a characteristic (a, b)
Oracle: a permutation c
1: for i from 1 to n do
2:
3:
4: end for
5: output 0

pick uniformly a random X and query for c(X) and c(X + a)
if c(X + a) = c(X) + b, output 1 and stop

Figure 4.11. Differential distinguisher.

116
Chapter 4
Proof. We let pC = DPC (a, b) be the probability of the characteristic (which depends
on the secret key). Given a distribution C, the probability that the distinguisher outputs
1 is

(cid:12)

1 − (1 − pC )n

E

(cid:13)

over the distribution of C. Since we have 1 − (1 − x)n ≤ nx, this is less than n.E( pC ).
Now since | p − p

∗

∗| ≤ max( p, p
∗

Adv (C, C

(cid:13)

∗

E( pC ), E( pC

)

.

Finally, we have

(cid:12)

E

pC

(cid:12)

), we obtain that
) ≤ n. max
(cid:12)
(cid:16)
(cid:16)

(cid:13)
(a, b)
(X + a) = C
∗
(X + a) = C

(cid:14)
(cid:14)

DPC

Pr
X

C

C

∗

∗

∗(cid:13) = E
= E
C∗
= E
=

X

Pr
C∗
1
2m − 1

.

(cid:15)(cid:17)
(X) + b
(cid:15)(cid:17)
(X) + b

∗

∗

This concludes the proof.

(cid:1)

Linear distinguishers can be modelized as depicted in Fig. 4.12. Here is a useful

lemma in order to analyze the advantage.

Lemma 4.8. For the distinguisher of Fig. 4.12 we let pc be the probability that the
output is 1 given an oracle c. We let p0 be the probability that it outputs 1 when the
counter is incremented with probability 1
2 in each iteration instead of querying the
oracle. We have

(cid:21)

| pc − p0| ≤ 2

n.LPc(a, b).

Proof. We ﬁrst express the probability pc that the distinguisher accepts c. Let Ni be
the random variable deﬁned as being 1 or 0 depending on whether or not we have
X · a = c(X) · b in the i-th iteration depending on the random variable X. All Ni ’s

Parameters: a complexity n, a characteristic (a, b), a set
Oracle: a permutation c
1: initialize the counter value u to zero
2: for i from 1 to n do
3:
4:
5: end for
6: if u ∈ , output 1, otherwise output 0

pick a random X with a uniform distribution and query for c(X)
if X · a = c(X)· b, increment the counter u

Figure 4.12. Linear distinguisher.

(cid:1)Conventional Security Analysis

117

Ni = 1. We also deﬁne θ = 2z − 1 = √
are independent and with the same 0-or-1 distribution. Let z be the probability that
LPca, b. We thus mean to prove that | pc −
p0| ≤ 2θ
(cid:18)
(cid:19)
(cid:1)

√
n. We have

pc =

n
u

u∈A

zu(1 − z)n−u

and

(cid:19)(cid:16)

(cid:1)

(cid:18)

u∈A

n
u

pc − p0 =

(cid:17)

.

zu(1 − z)n−u − 1
2n

We would like to upper-bound | pc − p0| over all possible A depending on z. Since
z and 1 − z play a symmetric role, we assume without loss of generality that z ≥ 1
2 .
For z = 1
2 . Since
zu(1 − z)n−u is an increasing function in terms of u we have

2 , the result is trivially true, so from now on we assume that z > 1

| pc − p0| ≤ n(cid:1)

(cid:18)

(cid:19)(cid:16)

n
u

zu(1 − z)n−u − 1
2n

(cid:17)

where k is the smallest integer u such that the difference in parenthesis is nonnegative,
i.e.

u=k

(cid:29)

k = 1 +

− log(1 − z)
log 1
log z − log(1 − z)
2

n

(cid:30)

.

Replacing u by n
Hence k ≥ n+1
to be an increasing function in terms of z, which is 0 for z = 1
that k ≤ (cid:25)n.z(cid:26). Therefore n−1

2 in the same expression in parenthesis we obtain a negative difference.
2 . Similarly, replacing u by n.z, the expression in parenthesis turns out
2 we obtain

≤ k − 1 ≤ (n − 1)z + z.

2 . Since z > 1

2

If n = 1, we have k = 1; thus maxA | pc − p0| = z − 1

2 and so the result holds. If

n = 2, we have k ≥ 3

2 ; thus k = 2 and
(cid:16)
| pc − p0| =

maxA

(cid:17)(cid:16)

(cid:17)

(cid:16)

(cid:17)

z − 1
2

z + 1
2

≤ 3
2

z − 1
2

and so the result holds as well. We now concentrate on n ≥ 3.

We use the following identity.3

n(cid:1)

(cid:18)

(cid:19)

n
u

u=k

zu(1 − z)n−u = k

(cid:18)

n
k

(cid:19)(cid:20)

0

z

t k−1(1 − t)n−k dt.

3 This identity was found in Ref. [155]. We can easily prove it by taking the derivative in terms of z.

Chapter 4

z

t k−1(1 − t)n−k dt

(4.1)

118

We obtain

and thus

maxA

(cid:19)(cid:20)

(cid:18)

n
k

1
2

(cid:17)

| pc − p0| ≤ k
(cid:19)(cid:16)

(cid:18)

| pc − p0| ≤ k

z − 1
2

max
t∈[0,1]

(cid:12)

t k−1(1 − t)n−k

(cid:13)

.

n
k

n
k

The maximum is obtained for t = k−1
(cid:19)(cid:16)

(cid:18)

n−1 ; hence,

(cid:17)

| pc − p0| ≤ k

z − 1
2

(k − 1)k−1(n − k)n−k

(n − 1)n−1

.

Let x = 2 k−1
n−1
0 ≤ x ≤ 1 and

− 1. We have k − 1 = n−1
2 (1 + x) and n − k = n−1
(cid:17)

2 (1 − x). We have
(cid:13) n−1

(1 + x)1+x (1 − x)1−x

2

.

(cid:12)

1
2n−1

(cid:18)
| pc − p0| ≤ k
(cid:13) = n

(cid:12)

n
k

(cid:19)(cid:16)
(cid:18)

z − 1
(cid:19)
2

n−1
Note that by using k
k−1
bound is asymptotically equal to θ
loose.

n
k

and the Stirling approximation, we obtain that this
√
n√
2π and so the bound we want to prove is not so

We can easily prove that (1 + x)1+x (1 − x)1−x ≤ 22x 2. Hence,

(cid:18)

(cid:19)(cid:16)

(cid:17)

| pc − p0| ≤ k

z − 1
2
Since k − 1 ≤ (n − 1)z + z, we have x ≤ θ + θ
(cid:19)
n−1

(cid:18)

(cid:22)

n
k

1

2n−1 2(n−1)x 2 .
= nθ +1
+ 1
(cid:23)
n−1
× 2

(nθ+1)2
n−1

.

n−1 . Thus,

| pc − p0| ≤ θ ×
(cid:13)

(cid:12)

2n ≤ 3

1

n
k

k

n
k

1
2n

For n = 3, we have k

4 ; thus,
√
n × 1
| pc − p0| ≤ 2θ
√
3
2
√
√
n and this remains true even for θ > 1
2

× 3
4

× 2

(3θ+1)2
n−1

.

. We

3

For θ ≤ 1
√
now concentrate on n ≥ 4.

, we obtain | pc − p0| ≤ 2θ

2

3

(cid:1)Conventional Security Analysis

(cid:12)

(cid:13)

The

n
k

term is upper-bounded by

n
r

with r =(cid:31)
(cid:12)
(cid:13)
(cid:17)
(cid:16)
≤ r(cid:27)

1 − 1
2i

i=1

n
2

(cid:18)

(cid:19)

n
r

1
2n

 

. Furthermore, we have

119

with equality when n is even. Then

(cid:16)(cid:18)

(cid:19)

n
r

(cid:17)

1
2n

log

(cid:17)

1 − 1
2i

log

(cid:16)
r(cid:1)
(cid:20)

1
i
r+1

i=1

dt
t

1

log(r + 1)
+ 1

log

n
2

≤ r(cid:1)

i=1
≤ − 1
2
≤ − 1
2
≤ − 1
2
≤ − 1
2
!

(cid:19)

(cid:18)

n
k

≤

1
2n

(cid:19)

(cid:18)

n
k

k

= n

1
2n

(cid:16)

(cid:17)

1
2n

n − 1
k − 1

.

2
n + 2
!

≤ n
2

!

.

n
2

≤

2
n + 1

and therefore

Now we have

We deduce

| pc − p0| ≤ 2θ
√
2 and n ≥ 4 we have (nθ +1)2
√
√
n < 1
n−1
n ≥ 1
n. When θ

< 0, and so we obtain | pc − p0| ≤
When θ
2θ
2 , this also holds since the right-hand side of the inequality is
greater than 1 and the left-hand side is a difference between two probabilities. This
(cid:1)
proves the upper bound.

(nθ+1)2
n−1

− 3
2 .

2

√
n × 2
− 3

Now here is the advantage of linear distinguishers.

Theorem 4.9 (Vaudenay 2003 [183]). Given two random permutations C and C
over
the same message space {0, 1}m, where C
∗
)
be the difference in the probability that the linear distinguisher of complexity n (as

is uniformly distributed, we let Adv(C, C

∗

∗

120

Chapter 4

depicted in Fig. 4.12) outputs 1 when the oracle implements the distributions of C and
C

. We have

∗

"
) ≤ 3 3

Adv (C, C

∗

n.E

LPC (a, b)

!
(cid:13) + 3 3

n

2m − 1

.

Therefore, the attack is meaningless until the number of known plaintexts n reaches
the order of magnitude of

(cid:12)

(cid:12)

(cid:13)

n ≈ 1/E

LPC (a, b)

.

Proof. We ﬁrst notice that the advantage is zero when a = 0 or b = 0, so the bound
holds. Let us now assume that a (cid:4)= 0 and b (cid:4)= 0. We now take a random permutation
C with the corresponding Z and pC as in the previous lemma. Let δ = E((2Z − 1)2).
(Note that δ = E(LCC (a, b)).) When |2Z − 1| ≤ α, we have

| pC − p0| ≤ 2 × α

√

n.

Since (2Z − 1)2 is positive, the probability that |2Z − 1| is greater than α is less than
δ
α2 . Hence

√
n + δ

α2

| p − p0| ≤ 2 × α
(cid:19) 1
√
3 . We obtain | p − p0| ≤ 3 × 3
δn.

(cid:18)

(cid:13)

LPC (a, b)

. Since a (cid:4)= 0 and b (cid:4)= 0 we ﬁnally note that

for any α.

Let us now ﬁx α =
(cid:13) = 1
We recall that δ = E
LPC

(a, b)

∗

δ√
n

(cid:12)

(cid:12)

E

2m−1 and so we can have

| p

!
∗ − p0| ≤ 3 3
2m − 1
∗| ≤ | p − p0| + | p

n

.

∗ − p0|.

Finally, we use the fact that | p − p

(cid:1)

4.3.4

(cid:1)Ad hoc Construction

We deﬁne

(cid:12)
(cid:12)

EDPC

max

ELPC

max

= max
a(cid:4)=0,b
= max
a,b(cid:4)=0

E

E

(cid:13)
(cid:13)

DPC (a, b)

LPC (a, b)

.

(cid:1)Conventional Security Analysis

121

⊕

⊕

⊕

C1

C2

C3

⊕

⊕

⊕

K1

K2

K3

Figure 4.13. Nyberg–Knudsen construction.

Theorem 4.10 (Nyberg–Knudsen 1992 [141]). Given three independent random
≤ p, we let K1, K2, K3 be
permutations C1, C2, C3 such that for all i we have EDPCi
three independent random half-blocks, and we deﬁne ¯Ci (x) = Ci (x + Ki ). We have
max

EDP ( ¯C1, ¯C2, ¯C3)
≤ p for i = 1, 2, 3, we have

max

≤ p2

Similarly, if ELPCi
max

ELP ( ¯C1, ¯C2, ¯C3)

max

≤ p2

This proves that the three-round Feistel scheme of Fig. 4.13 is “provably secure” against
differential and linear cryptanalysis provided that the maximal EDP and ELP of round
functions are low.

Proof. We let

X = V ⊕ ¯C1(W )
Y = W ⊕ ¯C2(X)
Z = X ⊕ ¯C3(Y )

((V , W ) is the input of ( ¯C1, ¯C2, ¯C3) and (Z , Y ) is the output). We have
EDP( ¯C1, ¯C2, ¯C3)(vw , zy) = Pr[Y = y, Z = z|V = v, W = w]

(cid:1)
(cid:1)

x

Pr . . . X = x, Y = y, Z = z|V = v, W = w . . .
EDPC1(w , x ⊕ v)EDPC2(x, y ⊕ w)EDPC3(y, z ⊕ x).

=
=

x

(cid:1)

122
Chapter 4
We consider vw (cid:4)= 0. If w (cid:4)= 0 and y (cid:4)= 0, we can upper-bound EDPC1(w , x ⊕ v) and
EDPC3(y, z ⊕ x) by p. Then we have

EDP( ¯C1, ¯C2, ¯C3)(vw , zy) ≤ p2

EDPC2(x, y ⊕ w)

which is 0 because C2 is a permutation. If w (cid:4)= 0 and y = 0, we have

x

EDP( ¯C1, ¯C2, ¯C3)(vw , zy) = EDPC1(w , z ⊕ v)EDPC2(z, y ⊕ w)

which is less than p2. The w = 0 and y (cid:4)= 0 case is similar. Finally, if w = y = 0, the
x = v term is the only remaining one, and the EDPC2(x, y ⊕ w) term must be zero
because x = v (cid:4)= 0 and y ⊕ w = 0 and C2 is a permutation.
(cid:1)

As an application, Kaisa Nyberg and Lars Knudsen invented the PURE cipher.4

Its core is deﬁned as a three-round Feistel cipher with round functions

FK (x) = f (g(E(x) + K ))

where f : GF(233) → {0, 1}32 consists of discarding one bit, E : {0, 1}32 → GF(233)
is an injective linear mapping, and g : GF(233) → GF(233) is deﬁned by g(x) = x 3.
We can prove that EDPmax ≤ 2

−61 and ELPmax ≤ 2

−61.

The PURE cipher is provably secure against differential and linear cryptanalysis.
This is however an ad hoc construction which is weak against another attack: the
interpolation cryptanalysis. Let us consider r = r
(cid:8) + 1 rounds. Let Fi = FKi and C =
(F1, . . . , Fr(cid:8)). For X = (A1, . . . , A32, c1, . . . , c32) with constant values ci , let Y =
C(X) = (B1, . . . , B64). A classical result of the ﬁnite ﬁeld theory tells us that x (cid:11)→ x 2
is a linear function. Hence, every output bit of g : x (cid:11)→ x 3 can be expressed as a
multivariable polynomial of degree 2 in terms of input bits. For i ≥ 32, Bi is a Boolean
(cid:8) − 2 ≤ 32. (This means
(cid:8)−2 if r
function of A1, . . . , A32 with algebraic degree at most 2r
#
that Bi can be expressed as a polynomial in terms of A j ’s with a total degree at most
(cid:8)−2.) If{X1, . . . , X22r(cid:8)−2} is an afﬁne space with dimension 2r
(cid:8)−2 + 1, and Y j = C(x j ),
2r
Y j = 0 from the property of polynomial functions (see the lemma below). This
then
rounds and a truly random
property enables us to make a distinguisher between r
permutation (see Ref. [95]).

(cid:8)

Lemma 4.11. Let f (x1, . . . , xn) be a polynomial of total degree d over a ﬁeld K. If A
is a subset of Kn with an algebraic structure of afﬁne space of dimension d + 1, then

(cid:1)

(x1,...,xn)∈A

f (x1, . . . , xn) = 0.

This can easily be proved by induction.

4 This is the topic of Exercise 4.3 on p. 132.

(cid:1)Conventional Security Analysis

123
(cid:8) = r − 1 rounds and a random permutation can be
transformed into a key recovery attack against r rounds. For instance, here is an attack
on r = 6 rounds.

A distinguisher between r

1: pick c1, . . . , c32
2: pick an afﬁne space {X j = (A1, j , . . . , A32, j , c1, j , . . . , c32, j )} of dimension
(cid:8)−2 + 1 = 9
#
2r
3: get Y j = C(X j ) = (Y L
j
j Y L
4: ﬁnd Kr such that
j

=#

, Y R
j )

j f (Y R
j

⊕ Kr )

We notice that the afﬁne space consists of 29 = 512 elements. The worst number of
operations in Step 4 is thus 512 times the number of possible round keys which is 233.
We thus have an attack of complexity 242. More generally, an attack against r rounds
would have led to a complexity of 233 × 22r−3+1 elementary operations. When r < 8,
this is better than the codebook attack.

Another way to use the Nyberg–Knudsen Theorem consists of having an iterative
construction. This was the idea of Mitsuru Matsui who invented the MISTY cipher
(see Refs. [126, 127]). A variant of MISTY (KASUMI) is used in the UMTS mobile
telephone network (see Ref. [9]).

4.4 (cid:1)Modern Security Analysis

Although the design of modern block ciphers started in the seventies, all approaches we
have seen so far are essentially empirical or heuristic. We cannot formally guarantee
the security against any attack. Even when we restrict ourselves to differential and
linear attacks, we always need to rely on some heuristic hypothesis in order to estimate
the best characteristics. One exception though: the ad hoc construction based on the
Nyberg–Knudsen result we have seen in Section 4.3.4. This construction however does
not provide so much ﬂexibility for the design, and even introduces other potential
weaknesses. For this reason conventional cryptography seems to be bound to remain
an art which is only accessible to experts. In this section we investigate new ways to
formally address the problem. These results are an attempt to build up a theory for
block ciphers in order to move from an artistic status to a scientiﬁc status.

4.4.1

(cid:1)Distinguishability Security Model

Indistinguishability is a strong notion of security for block ciphers. Formally, a distin-
guisher is a machine which plays with an oracle and ultimately outputs 0 or 1. (The
notion of “machine” depends on the computational model. In Chapter 8, we will deﬁne
the notion of Turing machine.) We measure the ability to distinguish a random oracle
from another by the advantage, which is the difference between the expected output
with both random oracles. The actual power of the machine depends on the security

124

Chapter 4

model. Using the notion of Turing machine, we can limit the time or memory com-
plexity. It can also be deterministic or probabilistic, but this is no real limitation as the
complexity is not bounded.5 Usually, we only limit the number of oracle calls.

In this section we consider two important security models. We consider distin-
guishers which have to prepare all their queries to the oracle at the same time and with
a limited number of oracle calls d. These are called nonadaptive distinguishers. In
practice, they correspond to attacks which can play with an encryption device within a
limited time period so that they have no time to prepare their questions in an adaptive
way. We also consider distinguishers which are limited to d oracle calls, but which can
make their queries depend on the oracle feedback from the previous ones. These are
called adaptive distinguishers. We let Cld
a be the class of all nonadaptive and
adaptive distinguishers, respectively.

na and Cld

We measure the resistance against a class of distinguishers Cl by the best advantage.

Formally, if C and C

are two random oracles, we deﬁne

∗

(cid:12)

(cid:13)

BestAdvCl(C, C

∗

) = max
T∈Cl

E(T C ) − E(T C

∗

)

where T C denotes the output of the distinguisher T when interacting with the random
oracle C. Most of distinguisher classes are symmetric in the sense that if a distinguisher
T is in the class, then the opposite distinguisher 1 − T is also in the class. Therefore,
there is no need for looking at the absolute value of the advantage.

We can motivate the notion of indistinguishability from an ideal function as one
of the strongest security requirements for all conventional cryptographic primitives.
In the case of a block cipher C, for instance, when an adversary is able to decrypt a
ciphertext after having queried d − 1 chosen plaintexts, he can a fortiori use this attack
within d queries: if the
in order to distinguish C from a truly random permutation C
ciphertext decrypts correctly, then the oracle implements C. Hence, if the block cipher
C is indistinguishable from C
within d queries, then the encryption is safe when used
at most d times.

∗

∗

Similarly, if an adversary can forge an authenticated message after d − 1 queries
within
, then the MAC function F is secure

to a MAC algorithm F, he can distinguish F from a truly random function F
d queries. So if F is indistinguishable from F
when used at most d times.

∗

∗

∗

, then we cannot ﬁnd collisions in F

On the hash function side, we can prove that when we are given a truly random
oracle function F
essentially better than by doing a
birthday paradox attack, and we cannot perform ﬁrst or second preimage attacks against
essentially better than by doing exhaustive search. Therefore, under the assumption
F
that a hash function can be simulated by a random oracle which is indistinguishable
from F

, a hash function is also a secure hash function.

∗

∗

∗

5 See Chapter 8.

(cid:1)Conventional Security Analysis

125

Finally, indistinguishability from a truly random source is also a way to deﬁne
randomness: a pseudorandom generator is secure if it is indistinguishable from a truly
random generator. Clearly, if we can predict some nontrivial information about the next
generation of a pseudorandom source, then we can distinguish it from a random one.
The converse is also true: if we can distinguish a pseudorandom source from a random
one with a minimal number d of generations, then from the d − 1 ﬁrst generations
we can predict that the d-th one will be the one for which the distinguisher yields 1.
Indistinguishability is indeed often used as a synonym for randomness.

4.4.2

(cid:1)The Luby–Rackoff Result

In trying to analyze the security of DES, Michael Luby and Charles Rackoff proved
that the Feistel cipher can actually generate a good pseudorandom permutation if the
underlying round functions are random and we have at least three rounds. The result
formally, which was publicly presented in 1986, states the following.

∗
Theorem 4.12 (Luby–Rackoff 1986 [119]). Let F
random functions on {0, 1} m
1

∗
, F
2

∗
3 be three independent

, F

2 with uniform distribution. We have
) ≤ d2.2
) ≤ d2.2

∗
((F
1
∗
((F
1

∗
3 ), F
∗
3 ), C

∗
, F
2
∗
, F
2

− m
− m

, F

, F

∗
∗

2

2

BestAdvCld
BestAdvCld

a

a

∗

∗

(resp. C

where F
) is a uniformly distributed random function (resp. permutation) on
{0, 1}m. The results would still hold if we replaced the XOR of the Feistel schemes by
any (quasi)group operation.
Proof. Following the Feistel scheme F = (F

, F

∗
3 ), we let

∗
, F
2

∗
1

We let E be the event z3
i
thus have [F]d

x,y

i ) and z4
i

= z2

i

+ F

∗
3 (z3

i ) for i = 1, . . . , d. We

We can easily check that Y fulﬁlls the requirements of Lemma 4.14 below. Firstly we
have

, z1
i )
+ F
∗
1 (z1
i )
, z3
i ).

i

xi = (z0
= z0
z2
i
yi = (z4
∗
2 (z2
= Pr[E]. We now deﬁne

+ F

i

i

i

= z1
Y =$

(y1, . . . , yd);∀i < j

%

.

(cid:4)= z3

j

z3
i

(cid:17)

2md

(cid:16)

#Y ≥

1 − d(d − 1)

2

− m

2

2

Chapter 4
2 . Second, for y ∈ Y and any x (with pairwise different
− m
i ’s are pairwise

126
thus we let 1 = d(d−1)
x,y. Let E 2 be the event that all z2
entries), we need to consider [F]d
∗
different over the distribution of F
1 . We have
≥ Pr[E|E 2] Pr[E 2].

[F]d

2

2

x,y

−md. It is then straightforward that Pr[E 2] ≥ 1 − d(d−1)

For computing Pr[E|E 2] we know that z3
i ’s are pairwise different, as for the z2
i ’s. Hence
Pr[E|E 2] = 2
− m
2 , which is
2
) ≤
1 − ε2. We thus obtain from Lemma 4.14 (given later) that BestAdvCld
(F, F
d(d − 1)2
) ≤
− m
∗, F
(C
2 . From Lemma 4.14 it is straightforward that BestAdvCld
2 d(d − 1)2
for d ≤ 21+ m
−m. We thus obtain BestAdvCld
2 . Since
(cid:1)

BestAdv is always less than 1, it also holds for larger d.

) ≤ d22

(F, C

− m

∗
∗

a

a

1

2

∗

a

2

4.4.3

(cid:1)Decorrelation

The notion of decorrelation of a function provides a nice algebraic interpretation of the
best advantage in terms of distribution distance (see Ref. [183]).

Given a random function F from a set A to a set B, we ﬁrst deﬁne the real matrix
[F]d as a Ad × Bd-type matrix (the rows are numbered by input d-tuples, and the
columns are numbered by output d-tuples) for which the ((x1, . . . , xd), (y1, . . . , yd))-
entry is

[F]d

(x1,...,xd ),(y1,...,yd )

= Pr[F(x1) = y1, . . . , F(xd) = yd].

∗

A random function F is aimed at being compared to a canonical ideal random function
. For instance, if F is a block cipher, the canonical ideal random function is a
F
uniformly distributed random permutation. Given a distance D on the vector space of
Ad × Bd-type real matrices, we deﬁne the d-wise decorrelation bias of F by

Decd(F) = D([F]d , [F

∗

]d).

Different distances will deﬁne various decorrelation notions.

We take a simple example with A = {0, 1, 2} and B = {0, 1} with F(x) = (K .x 2 +
(cid:29) + x + 1) mod 2 for K uniformly distributed in {1, 2, 3, 4}. Here is the table

K .(cid:28) K+x
of all possible F functions.

2

K

1
2
3
4

f (0)

f (1)

f (2)

1
1
0
1

0
0
1
0

0
1
1
1

(cid:1)Conventional Security Analysis
For d = 1 we use the following probability table that deﬁnes [F]1.

127

y = 0

1/4
3/4
1/4

y = 1

3/4
1/4
3/4

x = 0
x = 1
x = 2

For d = 2 we use the following probability table which deﬁnes [F]2.

(y1, y2) = (0, 0)

(y1, y2) = (0, 1)

(y1, y2) = (1, 0)

(y1, y2) = (1, 1)

(x1, x2) = (0, 0)
(x1, x2) = (1, 0)
(x1, x2) = (2, 0)
(x1, x2) = (0, 1)
(x1, x2) = (1, 1)
(x1, x2) = (2, 1)
(x1, x2) = (0, 2)
(x1, x2) = (1, 2)
(x1, x2) = (2, 2)

1/4
0
0
0
3/4
1/4
0
1/4
1/4

0
3/4
1/4
1/4
0
0
1/4
1/2
0

0
1/4
1/4
3/4
0
1/2
1/4
0
0

3/4
0
1/2
0
1/4
1/4
1/2
1/4
3/4

Therefore if we write rows and columns in this order we have

⎛
⎝ 1/4 3/4

3/4 1/4
1/4 3/4

⎞
⎠ ,

[F]1 =

[F]2 =

1/4
0
0
0
3/4
1/4
0
1/4
1/4

0
3/4
1/4
1/4
0
0
1/4
1/2
0

0
1/4
1/4
3/4
0
1/2
1/4
0
0

3/4
0
1/2
0
1/4
1/4
1/2
1/4
3/4

We can compare F with a uniformly distributed F

⎛
⎝ 1/2 1/2

1/2 1/2
1/2 1/2

⎞
⎠ ,

∗

]1 =

[F

∗

[F

]2 =

for which

1/2
1/4
1/4
1/4
1/2
1/4
1/4
1/4
1/2

0
1/4
1/4
1/4
0
1/4
1/4
1/4
0

0
1/4
1/4
1/4
0
1/4
1/4
1/4
0

1/2
1/4
1/4
1/4
1/2
1/4
1/4
1/4
1/2

⎛

⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝

⎛

∗

⎞

⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠

⎞

128

Chapter 4

Decorrelation has nice properties which come from its algebraic deﬁnition. For
instance we can use the triangular inequality. When D is deﬁned by a matrix norm,6
decorrelation is multiplicative: if the canonical ideal random function associated with a
random permutation is a uniformly distributed random permutation, then the decorre-
lation of a product of independent random permutations is at most equal to the product
of the decorrelation of each permutation. Let C1 and C2 be two independent random
permutations over a set A. They are compared to a uniformly distributed random per-
mutation over A. Because of the independence between C1 and C2, we have

Then we notice that

[C2 ◦ C1]d = [C1]d × [C2]d .

[C1]d × [C

∗

]d = [C

∗ ◦ C1]d = [C

∗

]d

and

because C

[C
∗ ◦ C1, C2 ◦ C
(cid:12)
[C1]d − [C

∗

∗

]d × [C2]d = [C2 ◦ C
(cid:13) ×(cid:12)

, and C
∗

∗

∗

]d

have exactly the same distribution. Hence
[C2]d − [C

(cid:13) = [C2 ◦ C1]d − [C

]d

]d

∗

∗

]d = [C

∗

]d

which leads us to

Decd(C2 ◦ C1) ≤ Decd(C1) × Decd(C2).

We now show the relationship between best advantage and decorrelation.

Theorem 4.13 (Vaudenay 2003 [183]). We let |||.|||∞ be the matrix norm associated
to the inﬁnity norm:

(cid:1)

|||A|||∞ = max

x1,x2,...,xd

|A(x1,x2,...,xd ),(y1,y2,...,yd )|.

we have

y1,y2,...,yd
∗

For any F and its canonical ideal version F

BestAdvCld

na

(F, F

∗

) = 1
2

Decd|||.|||∞(F).

Similarly, there exists a matrix norm ||.||a which provides the same result for Cld
a:

) = 1
2
6 A matrix norm is a norm such that ||A × B|| ≤ ||A||.||B||.

BestAdvCld

(F, F

∗

a

Decd||.||a (F).

(cid:1)Conventional Security Analysis
This matrix norm ||.||a is deﬁned by
(cid:1)

(cid:1)

||A||a = max

x1

max

x2

y2

y1

129

(cid:1)

yd

··· max

xd

|A(x1,x2,...,xd ),(y1,y2,...,yd )|.

Proof. Since the distinguishers are not computationally bounded, they can be assumed
to be deterministic (equivalently, we take the initial random tape which maximizes
the advantage). For nonadaptive distinguishers, this means that the queries x1, . . . , xd
are constant, and the distinguisher is characterized by an acceptance set A of many
y’s where y = (y1, . . . , yd). The probability that the distinguisher outputs one (over
x,y for x = (x1, . . . , xd). The
the distribution of the oracle) turns out to be
advantage is thus

y∈A[F]d

(cid:26)

(cid:13)

.

[F]d

x,y

− [F

∗

]d
x,y

(cid:1)

(cid:12)

y∈A

This is maximal when A consists of all y’s such that the difference in the parenthesis is
positive, and when x maximizes this sum. Now since the complete sum over all possible
y’s is zero, this sum is exactly half of the sum of all absolute values.

For adaptive distinguishers, we apply the same method by induction on d: we
consider that the distinguisher queries a constant x1, and we deﬁne a distinguisher
limited to d − 1 queries depending on y1. Assuming that the best of these distinguishers
have an advantage of

(cid:1)

y2

1
2

max

x2

··· max

xd

(cid:1)

(cid:28)(cid:28)[F]d

yd

(cid:28)(cid:28)

− [F

∗

]d
x,y

x,y

where x1 and y1 are constants, the advantage of the overall distinguisher will need to
(cid:1)
sum over all y1, and to maximize the quantity over x1.

∗

The following lemma is a tool for bounding the decorrelation. It means that if
]d
x,y for all x and almost all y, then the decorrelation bias of F is
x,y is close to [F

[F]d
small.
Lemma 4.14. Let d be an integer. Let F be a random function from a set M1 to a set
M2. We let X be the subset of Md
1 of all (x1, . . . , xd) with pairwise different entries.
be a uniformly distributed random function from M1 to M2. We know that
We let F
for all x ∈ X and y ∈ Md
−d. We assume
there exist a subset Y ⊆ Md
r #Y p0 ≥ 1 − ε1
r ∀x ∈ X ∀y ∈ Y[F]d

x,y is a constant p0 = (#M2)
]d

2 and two positive numbers ε1 and ε2 such that

2 the value [F

∗

∗

x,y

≥ [F
∗

x,y(1 − ε2).
]d

∗
) ≤ ε1 + ε2.

Then we have BestAdvCld

a

(F, F

130

Chapter 4

∗

Proof. We use the characterization of Decd||.||a in terms of best adaptive distinguisher.
We let A be a distinguisher between F and F
limited to d oracle calls with maximum
advantage. As discussed previously, we can assume without loss of generality that the
distinguisher is deterministic, and that all queries to the oracle are pairwise different (we
can simulate the distinguisher by replacing repeated queries by dummy queries). The
behavior of A is deterministically deﬁned by the oracle responses y = (y1, . . . , yd).
We let xi denote the i-th query deﬁned by y. It actually depends on y1, . . . , yi−1 only.
We let x = (x1, . . . , xd) which is assumed to be in X . We let A be the set of all y for
which A outputs 0. It is straightforward that
(cid:1)

(cid:13)

(cid:12)

AdvA (F, F

∗

) = −

[F]d

x,y

− [F

∗

]d
x,y

.

y∈A

Next we have

AdvA (F, F

∗

) ≤

(cid:1)

y∈A
y∈Y

∗

]d
x,y

+

ε2[F

(cid:1)

y∈A
y(cid:4)∈Y

∗

[F

]d
x,y

.

By relaxing the y in the ﬁrst sum, we observe that it is upper-bounded by 2. (We just
have to add the y j ’s backward, starting by adding all yd’s, then yd−1, etc.) For the second
]d
x,y is always equal to p0. This
sum, we recall that all xi ’s are pairwise different, so [F
(cid:1)
sum is thus less than ε1.

∗

Finally, the following result shows the relationship between the decorrelation of
order 2 and the resistance against differential and linear cryptanalysis. This demon-
strates that although it is hard to construct a cryptographic primitive with proven low
decorrelation bias to a high order d, focusing on the order d = 2 already provides
decent security results.
Theorem 4.15 (Vaudenay 2003 [183]). Let C be a random permutation over {0, 1}m
compared with a uniformly distributed random permutation C

. We have

∗

EDPC

max

ELPC

max

≤
≤

1

2m − 1
2m − 1

1

+ BestAdvCl2
+ 4BestAdvCl2

a

a

(C, C

(C, C

∗

)

∗

)

∗

where C

is a uniformly distributed random permutation.

Proof. By straightforward computations we obtain that

E(DPC (a, b)) = 2

−m

1 x2=x1+a
y2=y1+b

[C]2

(x1,x2),(y1,y2)

(cid:1)

x1 ,x2
y1 ,y2

(cid:1)Conventional Security Analysis

131

for any a and b, thus

E(DPC (a, b)) ≤ 2

−m

(cid:1)

x1 ,x2
y1 ,y2

1 x2=x1+a
y2=y1+b

[C

∗

]2
(x1,x2),(y1,y2)

+ BestAdvCl2

a

(C, C

∗

).

The ﬁrst term is then E(DPC

∗

(a, b)) which is at most

1
2m−1 .

the ELP result, we ﬁrst notice that 2 PrX [X · a = C(X) · b] − 1 =

(cid:12)
For
(−1)X·a+C(X)·b

(cid:13)

E

where X1 and X2 are independent uniformly distributed random variables. We have

(cid:13)

(cid:12)

, and we express LPC (a, b) as
LPC (a, b) = E
(−1)(X1⊕X2)·a+(C(X1)⊕C(X2))·b
(cid:1)

(−1)(x1⊕x2)·a+(y1⊕y2)·b[C]2

E(LPC (a, b)) = 2

−2m

x1 ,x2
y1 ,y2

(x1,x2),(y1,y2)

.

The contribution of terms for which x1 = x2 is equal to 2
−m. Considering that C is a
permutation, we can concentrate on x1 (cid:4)= x2 and y1 (cid:4)= y2. Then we split the remaining
sum into four groups depending on the two bits (x1 · a ⊕ y1 · b, x2 · a ⊕ y2 · b). Let
b1,b2 be the sum of all probabilities for which the two bits are (b1, b2), x1 (cid:4)= x2, and
y1 (cid:4)= y2. We have

E(LPC (a, b)) = 2

−m + 2

−2m 0,0 − 2

−2m 0,1 − 2

−2m 1,0 + 2

−2m 1,1.

As a result of symmetry we have 0,1 = 1,0. Furthermore, the sum of the four sums
is 2m(2m − 1). Hence

E(LPC (a, b)) = 2

−m + 2

−2m × 2m(2m − 1) − 4 × 2

−2m 0,1.

We deduce

E(LPC (a, b)) = 1 − 22−2m

(cid:1)

x1(cid:4)=x2
y1(cid:4)=y2

1 x1·a=y1·b
x2·a(cid:4)=y2·b

[C]2

(x1,x2),(y1,y2)

.

Finally, we obtain
E(LPC (a, b))− E(LPC

∗

(cid:1)

(cid:18)

[C]2

(x1,x2),(y1,y2)

(cid:19)

]2
(x1,x2),(y1,y2)

−[C

∗

(a, b)) =−22−2m

1 x1·a=y1·b
x2·a(cid:4)=y2·b

x1(cid:4)=x2
y1(cid:4)=y2
≤ 4BestAdvCl2

a

(C, C

∗

).

which leads us to the stated result by straightforward computations.

(cid:1)

132

4.5 Exercises

Chapter 4

Exercise 4.1. Given a function f

: {0, 1} p → {0, 1}q we deﬁne

DP f (a, b) = Pr[ f (X + a) = f (X) + b]
LP f (a, b) = (2 Pr[a · X = b · f (X)] − 1)2

where X is uniformly distributed in {0, 1} p. Give an algorithm for computing the whole
table of DP f in O(2 p+q) steps.7 Give an algorithm for computing the whole table of
LP f in O(( p + q)2 p+q) steps.
Exercise 4.2. Prove that if f : {0, 1} p → {0, 1}q, the following bounds hold.

Bound

Name of eq. case

Necessary condition for eq.

max ≥ 2
−q
max ≥ 21− p
max ≥ 2
− p
max ≥ u( p, q)

DP f
DP f
LP f
LP f

PN
APN

B
AB

p ≤ q or ( p, q) = (2, 1)

p ≥ 2q, p even
p ≥ 2q, p even
p = q, p odd

where

(cid:16)

u( p, q) = 21− p

1 + (2q− p − 1)(2 p−1 − 1)

2q − 1

(cid:17)

.

Equality cases are called Perfect Nonlinear (PN), Almost Perfect Nonlinear (APN),
Bent (B), and Almost Bent (AB). In addition, prove that B is equivalent to PN and that
AB is equivalent to APN.8

Exercise 4.3. We deﬁne

f : GF(233) → {0, 1}32
g : GF(233) → GF(233)
E : {0, 1}32 → GF(233)

linear mapping
g(x) = x 3
linear mapping

where f consists of discarding 1 bit, and E is injective, and

FK (x) = f (g(E(x) + K ))

7 This O notation means that there exists a constant c > 0 such that for any p and q, the complexity of
this algorithm is at most c2 p+q elementary operations. The notion of complexity is formally deﬁned in
Chapter 8.

8 This exercise was inspired by Ref. [44].

(cid:1)Conventional Security Analysis
for K ∈ GF(233).

Prove that DPg

max

= LPg

max

= 2

−32.

Deduce that for any K we have DPFK
max

≤ 2

−31 and LPFK

max

133

≤ 2

−31.

−61 and ELPmax ≤ 2

Deduce that for a Feistel cipher with at least three rounds with the above round
−61. (This is used in order to con-

function, we have EDPmax ≤ 2
struct the PURE cipher which was invented by Kaisa Nyberg and Lars Knudsen.)9
Exercise 4.4. Let C : {0, 1}m → {0, 1}m be a random permutation. We compare C to
a uniformly distributed permutation. Show that

matrix space,

1. the property Decd(C) = 0 does not depend on the choice of the distance on the
2. if Dec1(C) = 0, then the cipher C provides perfect secrecy for any distribution
3. if Dec2(C) = 0, then C is a Markov cipher.

of the plaintext,

Exercise 4.5. Let C : {0, 1}m → {0, 1}m be a random permutation. We compare C to a
uniformly distributed permutation. We consider decorrelation deﬁned by the adaptive
norm (||.||a).

1. Prove that Decd−1(C) ≤ Decd(C).
2. Prove that 0 ≤ Decd(C) ≤ 2.

Exercise 4.6. Let C, C
C

is uniformly distributed. Show that

∗

∗

: {0, 1}m → {0, 1}m be random permutations. We assume that

1. C is ε-strongly universal implies 2BestAdvCl2
≤ ε,
2. C is ε-strongly universal implies EDPC
3. C is ε-XOR universal implies EDPC

≤ ε.

max

a

max

∗

) ≤ ε,

(C, C

Exercise 4.7. Let f K : {0, 1}m → {0, 1}m be a function deﬁned by a random key K in
a key space K. We compare f K to a uniformly distributed function.

1. Prove that if Decd( f K ) = 0, then #K ≥ 2md.
2. Show that for f K (x) = x ⊕ K , we obtain Dec1( f K ) = 0.
3. Propose a construction for f K such that Decd( f K ) = 0 and #K = 2md.

Exercise 4.8. Prove that for any independent random function F1, . . . , Fr on {0, 1} m
such that

2

BestAdvCld

a

(Fi , F

∗

) ≤ ε

9 This exercise was inspired by Ref. [141].

134

we have

BestAdvCld

a

((F1, . . . , Fr ), C

(cid:18)

∗

) ≤ 1
2

(cid:19)(cid:28) r

3

(cid:29)

.

2d2.2

2 + 6ε
− m

Chapter 4

By using the previous exercises, propose a construction for a Feistel cipher C such that

BestAdvCld

a

(C, C

∗

) ≤ 2d4.2

−m .

Design a new block cipher.10

10 This exercise was inspired by Ref. [183].

5
Security Protocols with
Conventional Cryptography

Content

Password access control: UNIX passwords, basic HTTP, PAP
Challenge–response protocols: digest HTTP, CHAP
One-time passwords: Lamport scheme, S/Key
Key distribution: Needham–Schroeder, Kerberos, Merkle puzzles
Authentication chains: Merkle signature scheme, timestamps
Case study: GSM network, Bluetooth network

In this chapter we look at several examples of protocols which use conventional
cryptography. As we can see, the most important problems are peer authentication
and key distribution. Once these problems are solved, the conventional cryptographic
primitives that we have seen in previous chapters can be used in order to build up secure
communications, e.g. establish communication sessions between a client and a server
which preserve conﬁdentiality, integrity, authentication, and in which no adversary can
replay, reorder, or erase messages.

5.1 Password Access Control

The most intuitive way to perform access control is to request a password as de-
picted in Fig. 5.1. When a client wishes to connect to a server, they can proceed as
follows.

1. The client ﬁrst sends an access request to the server.
2. The server acknowledges and sends a password request to the client.
3. The client sends his password to the server.
4. The server checks the correctness of the password and either provides or denies

access to the client.

Here, the password does not need to be kept in memory: the server only keeps a
message digest (using a hash function) of the password. This is an interesting property
for security since this protects against potential damages when the memory content is
stolen.

136

Client

Server

−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−
−−−−−−−−−−−−−−−−−−−→ check

login, password

request C to S

authentication?

Chapter 5

Figure 5.1. Password access control.

5.1.1 UNIX Passwords

UNIX passwords are a famous example that we saw in Section 2.4. Here, the client is a
user (or a UNIX process whose permissions are associated to the user) and the server
is a workstation.

Here the server must keep a database of “encrypted” (through a one-way function)
passwords. The one-way encryption is purposely slow in order to slow down access
control attacks.

5.1.2 Basic Access Control in HTTP

Another example of password access control, which is taken from RFC 2617 (Ref. [69]),
is used in the HTTP protocol. Here the client is a browser who wishes to have access to a
protected document called uniform resource identiﬁer (URI) from a Web site. There are
two access control protocols: one is called basic and the other is called digest (the latter is
detailed in Section 5.2). In the basic protocol, the server must keep a database of (realm-
value,userid,password) triplets, where realm-value indicates one “part” of the HTTP
server, userid is the identiﬁcation string of a user, and password is simply the password.

Upon a URI request to a server, the server sends a challenge

WWW-Authenticate: basic realm=”(cid:30)realm-value(cid:31)”

Then the client must send credentials

Authorization: basic (cid:30)basic-credentials(cid:31)

where basic-credentials is the string

(cid:30)userid(cid:31):(cid:30)password(cid:31)

which is encoded according to the base64 algorithm.1 If the (realm-value, userid,
password) triplet is correct, the server can respond to the URI request. Otherwise it

1 This encoding scheme simply consists of encoding bitstrings into byte sequences in which only 6 bits in
every byte are used. This is in order to avoid escape characters which might be interpreted by processes
as special instructions.

Security Protocols with Conventional Cryptography

137

sends an error message

and sends the challenge again.

HTTP/1.0 401 Unauthorized

If the browser tries to connect to the URI and receives an access control request to
which it does not know how to respond, it yields a dialog box urging the user to ﬁll out
the userid and password ﬁelds. Usually, these are kept in memory during the session of
the browser so that the server can send many access control requests during the same
session. This is why users are often requested to close the browser after the session so
that no other user can access the same URI.

5.1.3 PAP Access Control in PPP

A similar example is one of the two access control protocols provided in the Point
to Point Protocol (PPP) which enables the remote connection of a machine to a net-
work. These two access control protocols are PPP Authentication Protocol (PAP) and
Challenge-Handshake Authentication Protocol (CHAP), which will be discussed in
Section 5.2. Both are detailed in RFC 1334 (Ref. [118]).

The PAP protocol is quite similar to the basic access control in HTTP.

5.2 Challenge–Response Protocols

The password access control protocol obviously provides low security since pass-
words may be intercepted by a third party (unless the communication channel pro-
tects conﬁdentiality, which can be the case with the SSL protocol as discussed in
Section 12.3).

In this section we have better access control protocols in which the client never
sends a password in clear to the server. It actually proves that he has the password by
replying to some random challenges as depicted in Fig. 5.2. This is not always feasible
when the client is a human being. It is however quite easy to implement when the client
is a machine.

Client

r = MACpassword(c)

request C to S

−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−− pick c at random
−−−−−−−−−−−−−−−−−−−→ check r = MACpassword(c)

Server

challenge c

response r

Figure 5.2. Challenge–Response access control.

138

Chapter 5

The drawback of this method is that the server database needs to keep the passwords

(i.e. not only the hashed values). It must therefore be strongly protected.

5.2.1 Digest Access Control in HTTP

The digest access control is an alternative to the basic protocol which is also described
in RFC 2617 (Ref. [69]). We describe here the main features of the digest protocol
without giving details for all parameters.

Upon a URI request to a server, the server sends a challenge

WWW-Authenticate: digest

realm=”(cid:30)realm-value(cid:31)”
[domain=”(cid:30)URI(cid:31)”]
nonce=”(cid:30)base64 nonce-value(cid:31)”
[opaque=”(cid:30)base64 opaque-value(cid:31)”]
[stale=true]
[algorithm=MD5]
[qop=”(cid:30)comma-separated list of auth or auth-int or token(cid:31)”]

(Lines between [ ] are optional.) The realm-value works as in the basic access authenti-
cation scheme. The nonce-value is a one-time generated value (preferably encoded with
base64). The opaque-value is only to be returned in the credentials. (It is quite conve-
nient for servers who send several challenges to several clients at the same time: they can
easily ﬁgure out which response corresponds to which challenge.) The stale=true
string indicates that the nonce-value is now stale and that the client must send other
credentials with a new nonce without querying the password again to the user. The
qop-value suggests the quality of protection scheme.

We can use any standard hash function H and MAC KD. Default for H is MD5

and

KD K (x) = H(K||“ : ”||x)

where || denotes the concatenation operation.

The client then must send credentials

Authorization: digest

username=”(cid:30)username-value(cid:31)”
realm=”(cid:30)realm-value(cid:31)”
nonce=”(cid:30)base64 nonce-value(cid:31)”
uri=”(cid:30)digest-uri(cid:31)”
response=”(cid:30)32lhex request-digest-value(cid:31)”
[algorithm=MD5]

Security Protocols with Conventional Cryptography

139

[cnonce=”(cid:30)base64 cnonce-value(cid:31)”]
[opaque=”(cid:30)base64 opaque-value(cid:31)”]
[message-qop=”(cid:30)qop-value(cid:31)”]
[nc=(cid:30)8lhex nc-value(cid:31)]

The algorithm and opaque-value must be the same as in the challenge. The qop-
value must be in the suggested list from the challenge. Here 32lhex and 8lhex means 32
or 8 lowercase hexadecimal digits. Credentials must respond to the challenge following
the computation below.

Computation of request-digest-value: if qop-value is auth (as for authentication)

or auth-int (as for authentication with integrity protection), then
request-digest-value = KDH(A1)((cid:30)nonce-value(cid:31):(cid:30)nc-value(cid:31):(cid:30)cnonce-value(cid:31):

(cid:30)qop-value(cid:31):H(A2))

otherwise

request-digest-value = KDH(A1)((cid:30)nonce-value(cid:31):H(A2))

where A1 and A2 are computed as detailed below.

Computation of A1: if algorithm is MD5 then

A1 = (cid:30)username-value(cid:31):(cid:30)realm-value(cid:31):(cid:30)passwd(cid:31)

and if algorithm is MD5-sess (as for MD5-hashed A1 for the whole session),
then

A1 = H((cid:30)username-value(cid:31):(cid:30)realm-value(cid:31):(cid:30)passwd(cid:31)):(cid:30)nonce-value(cid:31):

(cid:30)cnonce-value(cid:31)

where the hash value is computed once for the session (so that it does not
compromise the conﬁdentiality of passwd).
Computation of A2: if qop-value is auth then

A2 = (cid:30)Method(cid:31):(cid:30)digest-uri-value(cid:31)

and if qop-value is auth-int then

A2 = (cid:30)Method(cid:31):(cid:30)digest-uri-value(cid:31):H((cid:30)entity-body(cid:31))

where Method, digest-uri-value, and entity-body are part of the HTTP/1.1
standard.

As in the basic protocol, the server checks the correctness of the response and provides
or denies access to the client.

140

Chapter 5

5.2.2 CHAP Access Control in PPP

Challenge–Handshake Authentication Protocol (CHAP) is an alternative to the simple
User-Password PPP Authentication Protocol (PAP) in RFC 1334 (Ref. [118]).

While initiating a PPP connection, or at any time during the PPP session, authen-
tication with CHAP is required. Then CHAP packets are exchanged, encapsulated in
PPP Data Link Layer frames. A CHAP packet consists of
Code||Identiﬁer||Length||Data

where Code is a byte equal to 1, 2, 3, or 4, Identiﬁer is a byte, and Length is the length
of Data encoded on two bytes, i.e. it lies between 0 and 65535. The Identiﬁer bytes are
used to identify different simultaneous PPP sessions.

First the authenticator (PPP server) sends a CHAP packet with code 1 (challenge).
Then the peer sends back a CHAP packet with code 2 (response). For the challenge
and response, the Data consists of

Data = ValueSize||Value||Name

where Name is used to identify a Name-secret pair in an access control database and
ValueSize is the size of Value encoded on one byte. The correct answer is deﬁned by

Value2 = H(Identiﬁer||secret||Value1)

where Valuei is the value ﬁeld of the packet with code i for i = 1, 2. Packets with code
3 and 4 indicate success and failure in the access control, respectively.

5.3 One-Time Password

Besides requiring the server to keep the passwords, the challenge-response protocols
still face security problems when a challenge is repeated: if the adversary collected
many challenge-response pairs, she can send multiple parallel service requests until
she gets a challenge for which she knows the answer. We can prevent this by introducing
an artiﬁcial delay for each access (this may substantially slow down this kind of attack),
keeping track of aborted access requests, or having large challenges so that it will never
be repeated. Equivalently, the challenge can be a counter value instead of a random
value. In this case we can talk about one-time passwords: passwords which are used
only once.

5.3.1 Lamport Scheme

One of the ﬁrst one-time password cryptographic schemes was designed by Leslie
Lamport at the time the notion of one-wayness was invented (see Ref. [113]). Basically,
the Lamport scheme consists of several parts.

Security Protocols with Conventional Cryptography

141

The password generation. The client is given a seed password w, and the server
is given a pair ( f n(w), n) associated with this client. Here, f is a one-way
function.
The access control scheme. When the client wants to access the server for the i-th
time, he sends wi = f n−i (w) to the server. The server then checks that f (wi )
is the ﬁrst entry of the pair of the clients, retrieves the second entry which is
necessarily n − i + 1, and replaces this pair by (wi , n − i) in the database.

With this scheme, the number of accesses is limited to n. The server only has to make
one f computation. The client can implement a time–memory tradeoff: either he keeps
all f i (w) in memory and does not have to compute anything (if the client is a human
being, he can keep a sheet of passwords), or he only keeps w and makes on average n
2
f -computations per access, or stores m different passwords and makes on average n
2m
f -computations per access.

5.3.2 S/Key and OTP

The Bellcore company developed a popular one-time software based on the Lamport
scheme: S/Key, which has also been published as an Internet document RFC 1760
(Ref. [85]). This was later transformed into an Internet standard: the one-time password
(OTP) system RFC 2289 (Ref. [86]).

In this standard, the one-way f function is MD5 by default. It can also be SHA-1

or MD4.

In addition to the format of transmission, this standard provides an interesting way
to represent passwords in a humanly readable way: the 64-bit password is ﬁrst expanded
into 66 bits with a checksum. Then it is split into six 11-bit packets. Each packet is
encoded into a word given by a 2048-word dictionary of at most four alphabetical
characters. Thus, a 64-bit password is represented by six humanly readable short words.

In OTP, the user gives his password w (more precisely a secret pass-phrase of at least
10 characters) to the OTP generator. The generator generates a random seed s which
consists of 1–16 lowercase alphanumerical characters. It then hashes w concatenated
with s and reduces it to 64 bits using a standard function (see Ref. [86]). This produces
a string S = H(w , s). (The purpose of the seed is to diversify the pass-phrase, since it
may very well be the case that the same pass-phrase is used in different applications by
a particular individual.) The generator then computes pi = H N−i (S) for i = 0, . . . , N
with a given integer N and gives them to the user together with s. The OTP generator
also sends p0 = H N (S) and s to the server and discards everything from its memory.
The server keeps the last 64-bit one-time password p, a sequence integer i which is
ﬁrst set to 1, and s in memory with integrity protection.

When the user wants to access the server, the server sends a challenge

otp-(cid:30)algorithm(cid:31) (cid:30)sequence integer(cid:31) (cid:30)seed(cid:31)

142

Chapter 5

where seed is s and algorithm speciﬁes on which algorithm H is based. The user
then cross-checks that the sequence integer is correct and sends pi to the server. The
server rejects the user if H( pi ) (cid:4)= p. Otherwise, the server accepts, replaces p by pi ,
and increments i. Note that the user can either recompute pi from the seed and his
pass-phrase or keep a list of all pi ’s.

5.4 Key Distribution

Access control is a typical example of security protocols which involve conventional
cryptography. Another important example is key agreement, key transmission, key
distribution, or more generally key establishment. In many security applications, we
need to share secret keys (as for conventional encryption, MAC, access control) over
an insecure channel. Sometimes, one party needs to transmit its key to the other in a
secure way. But sometimes, both parties only need to agree on a fresh common secret
key.2

When two parties who do not share any common secret want to agree on a secret
key with conventional cryptography, they need to use services from a third party. Many
protocols require a key distribution center (KDC) which share a secret key with every
participant.

5.4.1 The Needham–Schroeder Authentication Protocol

In the Needham–Schroeder protocol, a client C wants to access a server S so S has to
be able to authenticate C (see Ref. [138]). The server does not have a database with all
potential clients though. They use an authentication server (AS),3 which is assumed to
share a secret key with each individual. For instance, C and AS share a key KC , S and
AS share a key K S, etc.

When C wants to access S, he ﬁrst sends a request for authentication with S to
AS in clear with a nonce N . A nonce is a random number which should be used once
(“nonce” is a contraction of “number” and “once”). Then, AS replies to C with a
message encrypted with KC , which includes a fresh key K , the identity IS of S, the
nonce N , and a ticket CK S (K , IC ) which includes K and the identity IC of C. C can
then send the ticket CK S (K , IC ) to S. S can decrypt it, authenticate himself by sending
CK (N ), and C authenticates himself by replying CK (N + 1) (see Fig. 5.3).

Here, the fresh key K is generated by AS in order to secure the communication
between C and S. AS makes sure that only C and S are able to retrieve it by using the
encryption with KC and K S. The ticket sent by C to S must be generated by AS since

2 We refer to Boyd and Mathuria (Ref. [37]) for a complete treatment on authenticated key establishment

protocols.

3 Here, “authentication server” is another terminology for “key distribution center.”

Security Protocols with Conventional Cryptography

143

Server

AS

pick K

Client
←−−−−−−−−−−−−−−−−−−− pick N1
−−−−−−−−−−−−−−−−−−−→

request C to S,N1
(K,IS,N1,CKS

(K,IC))

CKC

(K,IC)

−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−− pick N2
−−−−−−−−−−−−−−−−−−−→

CKS
CK(N2)
CK(N2+1)

Figure 5.3. The Needham–Schroeder authentication protocol.

it is encrypted with K S. (Nobody else should be able to encrypt with this key, so this
seemingly suggests that symmetric encryption is used to provide authentication here.)
So S is sure that C requested the ticket to AS, and that AS generated K for both of
them.

This protocol, made in 1978, is important for historical reasons. It suffers, however,
from the following drawback, and it is no longer recommended. The problem here is
that when K gets leaked to an adversary because of a careless user, she can replay the
ticket message CK S (K , IC ) and thus impersonate C to S. The real problem is that S has
no means to be ensured that K is fresh. Actually, this protocol was originally aimed at
protecting against network outsiders. But the real problem comes from careless insiders.
Several improvements of this protocol have been proposed, including the protocol which
is used in Kerberos.

5.4.2 Kerberos

Kerberos is an improvement of the Needham–Schroeder authentication protocol. Two
versions are currently well spread: Kerberos Version 4 and Kerberos Version 5. The
latter was adopted as the Internet standard RFC 1510 (Ref. [105]). While the basic
Kerberos protocol works as depicted in Fig. 5.4, the complete protocol works as follows.
When a client wants to authenticate to a given server, the protocol proceeds as follows
(see Fig. 5.5).

1. The client sends a request to the authentication server (AS), also called the

Kerberos server.

2. The AS sends a credential—encrypted with the secret key of the client—to
the client, together with a grant—encrypted with the secret key of the TGS.
The TGS is the ticket granting server who issues short-term tickets. Both the
credential and the grant include one selected mid-term key K0, which is assumed
to be fresh, and a timestamp. This key is to be shared by the client and the TGS
only.

3. The client sends a request to the TGS with the grant.
4. The TGS sends a credential—encrypted with the secret key of the client—to
the client, together with a ticket—encrypted with the secret key of the server.

144

AS

pick K

Client
←−−−−−−−−−−−−−−−−−−−−−−−− pick N
−−−−−−−−−−−−−−−−−−−−−−−−→

request C to S,N
(K,IS,N,T,L),CKS

(K,IC,T,L)

CKC

Chapter 5

Server

CKS

(K,IC,T,L),CK(IC,T )

−−−−−−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−−−−−−

CK(T +1)

Figure 5.4. Basic Kerberos key establishment protocol.

This ticket includes a timestamp T and a lifetime period L. In order to avoid a
session key compromise, the timestamp is assumed to be the present time, and
the lifetime is assumed to be short.

5. The client sends the ticket to the server together with an authenticator. This
authenticator is encrypted with the session key and corresponds to the CK (N )
in the Needham–Schroeder protocol.

6. The server replies with another authenticator.

The initial authentication request includes the identity of the client and the identity of
the server. It may also include various options for the ticket, among which the following
are a few:

(long-term tickets are dangerous because they can be stolen);

r Renewable ticket: The ticket has a short validity period but may be renewed
r Postdated ticket: postdated tickets are dangerous (because they can be stolen
r Proxiable ticket: when the client wants to pass a proxy to the server to perform

before they are used), and so they are explicitly marked as postdated, and security
policy of servers may decide to accept or reject it;

a remote request on its behalf;

Of course, the AS may abort the protocol by an error message.

One problem with this protocol is that all individuals must have (relatively loosely)

synchronous clocks.

)

(
flags

,K0,IC,time

request C to TGS, options,N0

−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ AS
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− AS (pick K0)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ TGS
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− TGS (pick K)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ S
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− S

(K0,time,N0,ITGS),grant=CKTGS
IS, options, grant,N,CK0
(K,time,N,IS,T,L),ticket=CKS
ticket,CK(IC,T )

,K,IC,T,L)

CK(T +1)

(IC,time)

(
flags

CKC

CK0

C

C

C

C

C

C

Figure 5.5. The Kerberos authentication protocol.

Security Protocols with Conventional Cryptography

145

5.4.3

(cid:1)Merkle Puzzles

In 1978, Ralph Merkle published a paper (Ref. [129]) which explains how to perform
conﬁdential communication when the two parties do not share any secret, which is
the basic problem of public-key cryptography.4 Here, we assume that the two parties
communicate over a channel which already provides authenticity and that they want to
agree on a common secret key.

We use the notion of puzzle: a puzzle is deﬁned by (y, c) and consists of recov-
(y) is a triplet (n, k, c) where C is an encryption function.
ering a key r such that C
We assume that doing an exhaustive search takes a complexity of (N ) for a given
parameter N . We also use a one-way function g, which is used as a pseudorandom
generator.

−1
r

We assume that N , C, and g are deﬁned. The Merkle key establishment protocol

works as follows between A and B.

1. A takes a random c, s1, s2. A takes N random ri . A computes ni = g(s1, i),

ki = g(s2, ni ), yi = Cri (ni , ki , c). A sends c and all yi to B.

2. B picks a random i and solves the i-th puzzle. He recovers ni and ki . B sends
3. A gets ki = g(s2, ni ). A and B now share the ki secret key.

ni to A.

(See Fig. 5.6.) The time complexity for A and B is O(N ). If an adversary wants to
recover ki without knowing i, she must solve all puzzles until ni is correct, which takes
(N 2).

5.5 (cid:1)Authentication Chains

We put here some applications of conventional cryptography which are a little more
exotic. They illustrate how hash functions can be chained.

5.5.1

(cid:1)Merkle Tree

Before public-key cryptography was invented, researchers tried to invent the notion
of digital signature scheme.5 A digital signature is an appendix to a digital document
which authenticates the signer. The signature is computed by using a private key, and it
is veriﬁed by using a public key. If a digital signature is “valid,” it proves that it has been
computed by someone who knew a given private key. This means that it is impossible
for an adversary to forge a valid signature without knowing the private key.

4 See Chapter 9.
5 The notion of digital signature and public key will be explained in Chapter 9.

146

Alice

generate c,s1,s2, r1, . . . ,rN
ni = g(s1, i), ki = g(s2, ni)
(ni, ki,c)
ki = g(s2, ni)
keep ki

yi = Cri

Chapter 5

Bob

−−−−−−−−−−−−−−−−−−−→ pick i, solve (yi,c), get ni,ki
←−−−−−−−−−−−−−−−−−−−

c,y1,...,yN

ni

keep ki

Figure 5.6. Merkle Key exchange protocol using puzzles.

One of the ﬁrst signature schemes was invented by Ralph Merkle in 1979 (but it

was published 10 years later; see Ref. [131]). Basic principles are twofold:

r Each bit xi of the message is signed by using a one-time secret key ki : a 1-bit
xi = 1 is signed by disclosing the secret key ki , and the corresponding public
key is the image vi = h(ki ) of the secret key through a one-way function;
r A large set of public keys vi is authenticated by using a hash tree: each vi is

attached to a leaf of an oriented binary tree, and if a(cid:9) and ar are attached to the
two subtrees of a node, we attach h(a(cid:9), ar ) to the node where h is a collision-
resistant hash function.

The property of the hash tree is that we can authenticate all public keys by authenticating
the value attached to the root of the tree only. Therefore, assuming that the root value
is authenticated, we sign the message x by disclosing all ki keys for which xi = 1 and
reminding all vi for other i. Signature veriﬁcation simply consists in checking that all
h(ki ) and reminded vi lead to the correct root value when attached to the tree6 (see
Fig. 5.7).

One problem with this basic signature scheme is that only 1-bits are signed: the
signature veriﬁer is ensured that all 1-bits are authentic, but he is not sure for 0-bits
since they could have been substituted to 1-bits. This problem is ﬁxed by preencoding
the message. A message m is ﬁrst encoded into a binary string x in such a way that
replacing one or several 1-bit of x by 0-bits leads to an invalid codeword. As an
example, Merkle proposed to simply append to m the binary representation c of the
number of 0-bits in m: x = m||c. We notice that if c
is the binary representation
of a number which is greater than c, then there must be at least one position i for
which we have ci = 0 and c
= 1. Since the adversary cannot replace a 0-bit by a
1-bit in x, he cannot increase the number of 0-bit in m by replacing a 1-bit by a
0-bit.

(cid:8)
i

(cid:8)

If m has a length of n, then x has a length of n + log2 n. Hence we need n + log2 n =

O(n) keys to sign m, and O(n) h-operation in order to sign or verify the signature.

6 As one can see, the main difference between computer science and botanic is that trees are rooted on the

top in the former.

Security Protocols with Conventional Cryptography

147

v1,8

 I

v1,4

]

v5,8

 ]

v1,2



K

v3,4

v5,6

 K



K

v7,8



K

v1

v2

v3

v4

v5

v6

v7

v8

Value to authenticate

Values to reveal

Figure 5.7. Authentication using a Merkle tree.

5.5.2

(cid:1)Timestamps and Notary

Another problem is how to timestamp a digital document in a notary sense: when
producing the document together with a timestamp, we aim at proving that the document
was deposited as is to a notary at a given date. This problem was addressed by Stuart
Haber who actually made a business out of this problem.7

Provided that the notary is trusted, the problem is trivial: the notary only has to sign
the document and the date. His signature is the timestamp, and verifying the signature
consists of checking the signature.

The problem is more complicated when the notary is not trusted. For this, we
ﬁrst need to realize that the notion of time is society-related. A date is a frontier
in the timescale which separates two classes of events: events which occur before
and events which occur after. We can prove that a document was deposited after a
given date by signing the document together with a proof that past societal events
occurred. Since depositing a document becomes an event itself, we can prove that
it was deposited before another given event by having a linear and linked history of
events. Haber and Stornetta proposed the notion of timestamp linkage (see Ref. [84]).
In this protocol, a notary-like service records sequences of digital documents yn to-
gether with identities (or pseudonyms) of depositors IDn. It computes sequences of
links zn and issues timestamps. Upon reception of yn and IDn, the notary proceeds as
follows.

7 See the activities of the Surety Technologies company http://www.surety.com, which is a kind of

digital notary.

148

Chapter 5

1. It sends IDn to the r previous customers IDn−1, . . . , IDn−r .
2. It computes zn−1 = H(L n−1) where H is a hash function and L n−1 was previ-

3. It sets L n to the sequence ((tn−r , IDn−r , yn−r , zn−r ), . . . , (tn−1, IDn−1, yn−1,

ously computed.

zn−1)).

4. It takes the current date and time tn.
5. It authenticates8 and sends [n, tn, IDn, yn, L n] to IDn. (Note that IDn will later

receive IDn+1, . . . , IDn+r as well.)

6. Once in a while, e.g. when n is a multiple of k, it publishes (n, zn, IDn) in a

newspaper.

When someone wants to verify a timestamp yn, tn from IDn, he asks for L n and
IDn+1, . . . , IDn+r . He can retrieve IDn−i from L n for i = 1, . . . , r. He can thus de-
duce the r depositors before and after tn. By contacting them and collecting answers,
he will be able to check that previous timestamps are embedded by H in L n (thus, they
happened before), and that the timestamp of L n is embedded by H in later timestamps
(thus happened before as well). He can therefore check the right sequence of events.
Going backwards and forwards a sufﬁcient number of times, he will reach published
timestamps and will be able to check that they are indeed embedded in a social event
like the newspaper. It is actually hard to falsify a posteriori a newspaper which has
been widely distributed. It is necessary here to use some data from a social event (see
Ref. [23]).

5.6 Wireless Communication: Two Case Studies

5.6.1 The GSM Network

GSM (Global System for Mobile communications) is a standard developed by ETSI for
mobile wireless communications. It includes security standards. Security objectives are
quite low in the sense that they only prevent attacks against the radio channel between
the terminal and a ﬁxed base. The main threats are

r usage of the GSM network with a fake or stolen identity,
r monitoring of private communications through the radio channel,
r monitoring of positioning information in the radio channel.

Secure charging of networks use and privacy in the wireless link are thus the main
concerns. Neither conﬁdentiality of transmissions in the wired part nor authentication
of the network to the terminal are provided in GSM. The GSM security goals thus
just consist of protecting conﬁdentiality, anonymity, and authentication of terminals in
the wireless part. Two standard interfaces are deﬁned: the A3/8 authentication algo-
rithm and the A5 encryption algorithm. Interfaces are publicly available. As it will be

8 Authentication is performed by a digital signature, which will be studied in Chapter 10.

Security Protocols with Conventional Cryptography

149

MS
(Ki)

VLR

IMSI

RAND

−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−− store
−−−−−−−−−−−−−−→ check
←−−−−−−−−−−−−−−

CKC(TMSI)

SRES

HLR
−−−−−−−−−−−−−−→ (Ki)
n×(RAND,SRES,KC)
←−−−−−−−−−−−−−−

IMSI

...

TMSI

−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−
−−−−−−−−−−−−−−→ check

RAND

SRES

Figure 5.8. GSM authentication.

explained, the A3/8 algorithm itself is not standard: every GSM operator can use its
own algorithm. However, the A5 algorithm is standard but secret.9

A mobile system (MS) is a combination of a terminal and a security module (the
SIM card). The terminal can be manufactured by any company, but the SIM card is
manufactured by the service provider who corresponds to a home network: the Home
Location Register (HLR). Each MS has an identiﬁer called IMSI. When connecting to
a local network, the Visited Location Register (VLR), the IMSI is sent and forwarded
to the HLR (see Fig. 5.8). Then, the HLR sends many triplets, which are used in order
to authenticate the MS to the VLR. After the ﬁrst authentication, the VLR gives a
temporary identity TMSI to the MS in a conﬁdential way in order to protect its privacy.

For the authentication, the mobile and the network share a long-term 128-bit secret
key Ki (integrity key) which is stored in the security module. When a mobile identiﬁes
itself, it sends a TMSI (a temporary identity) which protects the real identity. The
network sends a random 128-bit challenge RAND to the mobile. The mobile uses
A3/8 with inputs Ki and RAND in order to compute SRES and KC. SRES is sent to
the network. It is the response to the challenge. The network can perform the same
computation and compare the SRES values for authentication. At the end, the mobile
is authenticated and both parties have computed a common short-term 64-bit secret
key KC. KC is used for encryption. The three values RAND, SRES, and KC make a
triplet, which is used by the VLR.

We emphasize that Ki is protected by the security module and the HLR, but KC is
a short-term secret key between the device and the VLR. Encryption is performed by
the telephone, whereas authentication is performed by the security module. Therefore,
A5 must be standard for every VLR and telephone manufacturers, but A3/8 can be
speciﬁc to a service provider.

9 The version presented in Section 2.8.3 was disclosed, and then broken in Ref. [32]. Interestingly, another
secret algorithm—COMP128, which was an A3/8 proprietary algorithm—was disclosed and broken. See
http://www.isaac.cs.berkeley.edu/isaac/gsm.html.

150

Chapter 5

For the encryption, the A5/1 algorithm is a standard algorithm for A5. It is actually
a pseudorandom generator which is initialized with a 22-bit counter and the 64-bit
secret key KC. It generates a 114-bit block which is XORed to the plaintext. This is
just a one-time pad with 114-bit block sequences. The heart of A5/1 is explained in
Section 2.8.3.

Once the setup phase is complete, conﬁdentiality is ensured. A counter-based
stream cipher protects against attempts to erase, swap, or replay packets. However,
integrity protection is rather poor. It is easy for an adversary to replace a conﬁdential
message x by a message x ⊕ δ for a δ of her choice (even without knowing x). Hence
it is hard to say that authentication is protected.

5.6.2 The Bluetooth Network

Bluetooth networks10 are other nice examples of security infrastructure based on con-
ventional cryptography only. We brieﬂy outline how it works based on the Bluetooth
version 1.2 standard (Ref. [4]).

Bluetooth uses several conventional cryptographic algorithms. The core ones are
the stream cipher E0 that was described in Section 2.8.4, and the block cipher SAFER+,
which is a successor of the block cipher SAFER K-64. They are used in order to deﬁne a
series of cryptographic algorithms E0, E1, E21, E22, and E3. E0 serves for encryption.
E1 serves for peer authentication. E2 (including E21 and E22) is a key generator for
authentication. E3 is a key generator for encryption.

E0 was described in Section 2.8.4.

The authentication algorithm E1 works like a MAC. No matter who is the master
and who is the slave, authentication can be done in the two directions, so we talk about
a “veriﬁer” and a “claimant.” E1 takes the 48-bit logical address BD ADDR of the
claimant, a 128-bit random challenge AU RAND, which is transmitted by the veriﬁer,
and a 128-bit “link key” K , which is the secret key. E1 produces a 32-bit response
SRES and a 96-bit value ACO (authentication ciphering offset). With this primitive,
authentication is quite trivial: the veriﬁer picks a random challenge and sends it to the
claimant; the claimant computes the response and sends it to the veriﬁer; the veriﬁer
does the same computation and compares the two values. The output of E1 is deﬁned
by using SAFER+ twice in a (strange) hash mode. Note that E1 must keep track of
successful and unsuccessful authentications. Indeed, E1 starts with a waiting interval
whose value is kept in memory. If an authentication is not successful, this value is
multiplied by a factor greater than 1. Otherwise, it is divided by a factor greater than
1. The value must further be bounded.

The authentication challenge is generated either by E21 or by E22 depending on
what kind of authentication is required. E21 takes a 128-bit random value and a 48-bit

10 Following the Bluetooth folklore, “network” should be replaced by “piconet.”

Security Protocols with Conventional Cryptography

151

logical address and produces a 128-bit value. E22 takes an additional PIN code of L
bytes where 1 ≤ L ≤ 16. Note that E21 is a kind of SAFER+ encryption of the address
by using the random value as a key, and that E22 is a kind of SAFER+ encryption of
the random value by using a combination of the PIN and the address as a key.

Finally, the encryption key is generated by E3. E3 takes as input the 128-bit link
key K , a 128-bit random value EN RAND, and a 96-bit value COF. This value called
ciphering offset is either the concatenation of the two logical addresses of the master
and the slave, or the ACO value coming from the authentication, depending on what
kind of link key is used. It produces a 128-bit value which is linearly shrunk to the
required length for giving the encryption key Kc. E3 is deﬁned from SAFER+ in the
same hash mode as that of E1.

We can now describe how these primitives are used in order to set up the encrypted

channel between two peers.

First of all, the peers negotiate who is the master and who is the slave. Then they
have to do some key management through a “pairing protocol,” which leads to a secret
128-bit “link key.” Typically, this link key is semipermanent and will be used in a future
connection so that key management will not be required. Indeed, if the peers are already
paired, then the link key already exists on both sides. When a session starts they perform
a bidirectional authentication based on this key using E1 as depicted in Fig. 5.9. When
they need to start encryption they agree on a key length, derive an encryption key using
E3, and encrypt using E0.

When two devices want to set up a pairing, a human user typically has to securely
type a random ephemeral PIN code on both devices as depicted in Fig. 5.10. If one
device does not have any keyboard, a PIN code can be built in by a manufacturer (on
Bluetooth headsets, the PIN code is usually 0000). The key management consists of
generating an initialization key, generating a link key, and exchanging the link key.
The initialization key is generated using E22 using a PIN code and a 128-bit random
value IN RAND which is communicated by the master to the slave. Then both peers
pick a fresh random value LK RAND and exchange the XOR of this value with the
initialization key. They can then compute two keys LK K with E21 using the two

Master A

pick AU RANDB
check SRESB

compute SRESA

(cid:1)

Slave B

AU RANDB

SRESB

−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−− compute SRESB
←−−−−−−−−−−−−−−−−−−− pick AU RANDA
−−−−−−−−−−−−−−−−−−−→ check SRESA

AU RANDA

SRESA

SRESB = E1(K,BD ADDRB,AU RANDB)
SRESA = E1(K,BD ADDRA,AU RANDA)

Figure 5.9. Bidirectional authentication protocol in bluetooth.

(cid:2)

152

Chapter 5

Human user

SECURE

?

Device A

Radio link

SECURE

?

-

Device B

Figure 5.10. Bluetooth pairing.

random values and their addresses and deduce a combination key which is the XOR
of both LK K keys. This combination key serves as the link key. The overall (typical)
pairing protocol is depicted in Fig. 5.11. If one device has low memory capabilities, it
uses its unit key which is generated once for all as a link key and sends its XOR with
the initialization key to the other device which deduces it (see Fig. 5.12).

Obviously, if an adversary listens to the communication in the pairing and authen-
tication protocols and if the PIN code can be found by exhaustive search, then she can
easily recover the link key. Since the whole security infrastructure is built on the con-
ﬁdentiality of the link key, security is void in this case. However, peer authentication
and key establishment is safe assuming that the pairing is run through a conﬁdential
channel.

Like in GSM, once the setup phase is complete, conﬁdentiality is ensured. A clock-
based stream cipher protects against attempts to delay, swap, or replay a frame, but not

−−−−−−−−−−−−−−−−−−−→ user inputs PIN code

IN RAND

Slave B

Kinit = E22(IN RAND,PIN)
pick LK RANDB
CB = LK RANDB ⊕ Kinit

LK RANDA = CA ⊕ Kinit
compute LK KA,LK KB,K

Master A

user inputs PIN code
pick IN RAND
Kinit = E22(IN RAND,PIN)
pick LK RANDA
CA = LK RANDA ⊕ Kinit

LK RANDB = CB ⊕ Kinit
compute LK KA,LK KB, K

−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−

CA

CB

⎛
⎝ LK KA = E21(LK RANDA,BD ADDRA)
LK KB = E21(LK RANDB,BD ADDRB)

K = LK KA ⊕ LK KB

⎞
⎠

Figure 5.11. A typical pairing protocol in bluetooth.

Security Protocols with Conventional Cryptography

153

Master A

user inputs PIN code
pick IN RAND
Kinit = E22(PIN,IN RAND)
K = CB ⊕ Kinit

Slave B

IN RAND

−−−−−−−−−−−−−−−−−−−→ user inputs PIN code (or not)
Kinit = E22(PIN,IN RAND)
←−−−−−−−−−−−−−−−−−−− CB = Kunit ⊕ Kinit

CB

K = Kunit

Figure 5.12. A pairing protocol with a device with low capacities.

against attempts to erase frames. Like in GSM, the integrity protection is very poor so
frame authentication is not guaranteed.

5.7 Exercises

from a ﬁnite set C of C
Exercise 5.1. We consider an arbitrary secret function f
challenges to a ﬁnite set R of R responses. We consider an access control scheme in
which a server picks a challenge uniformly at random in C and the client sends the
corresponding response after one f application. Explain how the chance of cheating
increases when looking at access control communications. Describe an attack and
compute its probability of success.

Exercise 5.2. The Kerberos authentication protocol is presented using secret-key en-
cryption function CK . Identify the accurate security property which is required here.
(Conﬁdentiality? Integrity? Authentication?)

6
Algorithmic Algebra

Content

Group theory: isomorphism, construction
The ring Zn: Euclid algorithm, exponentiation, Chinese Remainder Theorem
Finite ﬁelds: generators, construction
(cid:1)Quadratic residuosity
Elliptic curves

Basic notions of number theory are brieﬂy exposed in this chapter, as well as a
number of useful algorithms on number theory. We encourage the reader to experiment
with the algorithms using a symbolic computing software (e.g. Maple, from the Univer-
sity of Waterloo,1 or Pari/GP from the University of Bordeaux2). While conventional
cryptography uses simple operations on bitstrings which are built in all microproces-
sors, public-key cryptography uses computation in algebraic structures. These classical
structures are reviewed here.

6.1 Basic Group Theory

6.1.1 Basic Set Theory

We brieﬂy remind here some basic notions and notations from set theory.

A set consists of a collection of elements. If an element x is in a set A we write
x ∈ A. Two sets are equal if they have exactly the same elements. We let ∅ be the empty
set, i.e. the set which has no element. If A and B are two sets, their intersection is the
set denoted A ∩ B of all x which are elements of both A and B. The union of A and B
is the set denoted A ∪ B of all x which are elements of either A or B. If all elements of
A are systematically elements of B we say that A is a subset of B and write A ⊆ B. In
order to denote the set of all elements x of A which satisfy a predicate P(x), we write
{x ∈ A; P(x)}, or simply {x; P(x)} when A is clear from the context.

A function f may map an element x of A to an element f (x) of a set B. We write
f : A → B to denote that f takes elements of A and maps them into elements of B.
We write f : x (cid:11)→ y to denote that f (x) = y. In this case we say that y is the image
of x by f and that x is a preimage of y by f . If I ⊆ A, we let f (I ) denote the set of

1 See http://www.maplesoft.com.
2 See http://pari.math.u-bordeaux.fr.

156
Chapter 6
all f (x) for x ∈ I . If J ⊆ B, we let f
−1(J ) denote the set of all x such that f (x) ∈ J .
To denote the set of all images by f of elements that satisfy the predicate we write
{ f (x); x ∈ A, P(x)} or f ({x ∈ A; P(x)}).

A function can also be considered as a family, but for this we use sequence-like
notations. A family (xi )i∈I is essentially a function from I to another set which maps
i into an element xi . The set I is called the index set of the family. As an example a
sequence is a family whose index set is the set of all integers (which may start from zero
or one). A pair can be viewed as a family of index set {1, 2}. In this case we just denote
(x1, x2) instead of (xi )i∈{1,2}. Similarly we write (x1, x2, x3) for a triplet (xi )i∈{1,2,3}.
Given two sets A and B we deﬁne the Cartesian product A × B as the set of all pairs
(x, y) for which x ∈ A and y ∈ B. We similarly deﬁne the set A × B × C of all triplets
consisting of elements in A, B, and C respectively. For simplicity we consider × to be
associative in the sense that we do not make any difference between (A × B) × C and
A × (B × C). (Formally we would consider (A × B) × C as the set of all pairs whose
ﬁrst component is a pair belonging to A × B and the second one is an element of C.)

Given a family (Ai )i∈I of sets we can deﬁne the intersection

&

and the union

Ai

Ai

i∈I

(cid:24)

i∈I

of all Ai . We also deﬁne the Cartesian product(cid:27)

Ai

i∈I

as the set of all families (xi )i∈I for which xi ∈ Ai for all i ∈ I . When all Ai are equal
to a single set A we write this product AI . We also write A2 instead of A
, and more
generally An instead of A

{1,...,n}

.

{1,2}

A binary relation R between elements of a set A and elements of a set B is formally
a subset of A × B. If (x, y) is in this subset, we say that x and y are related by R and
denote this by x Ry. When A = B, we say that R is reﬂexive if x Rx for all x ∈ A. We
say that it is symmetric if x Ry is equivalent to y Rx for all x, y ∈ A. We say that it is
transitive if x Ry and y Rz imply x Rz for any x, y, z ∈ A. R is an equivalence relation
if it is reﬂexive, symmetric, and transitive. In this case, denoting Cl(x) = {y ∈ A; x Ry}
as for the “class of x,” we have

Cl(x) = Cl(y) ⇐⇒ Cl(x) ∩ Cl(y) (cid:4)= ∅ ⇐⇒ x Ry

for all x, y ∈ A. Since x ∈ Cl(x) we notice that all Cl(x) build a partition of A, i.e. a
collection of subsets Cl(x) of empty pairwise intersections whose union covers all of

Algorithmic Algebra

157

A. In this case we formally call this partition the quotient of A by R and denote it by
A/R.

6.1.2 Groups
Formally, a group is any set G associated to a group law which is a mapping from G × G
to G. It thus maps two operands from G onto an element of G. This law can be denoted
either additively (with the + symbol) or multiplicatively (with ×, the dot symbol, or
even nothing). Using multiplicative notation, this law must fulﬁll the following group
properties.

1. Closure: For any a, b ∈ G, ab is an element of G.
2. Associativity: For any a, b, c ∈ G, we have a(bc) = (ab)c.
3. Neutral element: There exists a distinguished element e in G such that for any
4. Invertibility: For any a ∈ G, there exists an element b ∈ G such that ab and ba

a ∈ G, we have ae = ea = a.

are neutral elements.

(cid:8)

(cid:8)

because e is neutral, and ee

e = ee
is neutral, therefore e = e
(cid:8)

It is further easy to see that the neutral element e is necessarily unique: if e and e
are
e = e because
(cid:8) = e
neutral, we must have e
(cid:8)
. We usually denote this neutral element by 1 (or 0 if
e
the group law is additively denoted). This implies that if b is an inverse of a, then
ab = ba = 1. It is not more difﬁcult to see that the inverse of an element is unique: if
a = 1. We can
are both inverse of a, we must have ab = ba = 1 and ab
(cid:8) = b
b and b
a)b = 1.b = b which
(cid:8)
then multiply the latter equality by b to the right and obtain (b
(ab) = b by associativity, and then b
(cid:8)
−1
implies b
the inverse of a (or −a if the group law is additively denoted, in which case the term
opposite is more appropriate).

(cid:8) = b. We usually denote by b = a

(cid:8)

(cid:8)

(cid:8) = e

(cid:8)

(cid:8)

Commutativity means that for any a, b ∈ G, we have ab = ba. Usually, additively
denoted groups are commutative. When a group is commutative, we say it is an Abelian
group.

We notice that all products made of a single element a are uniquely deﬁned by
their number of terms: for instance (a(a(aa)))a = a(((aa)a)a). We can thus uniquely
−n as the inverse
denote it by a5 (or 5a with additive notations). We can also deﬁne a
of an. (With additive notations, this is denoted n.a as a multiplication of an integer by
a group element.)

Here are a few fundamental examples.

r The trivial group G = {0} deﬁned by 0 + 0 = 0. There is not much to say about
r The Abelian group Z of relative integers with the usual addition law. We notice
this group.
that this is the free Abelian group generated by a single element g = 1: all
elements can be uniquely written ng.

158

r The symmetric group SA over a given set A. This is the set of all permutations over
A with the composition law which is usually denoted◦. When A = {1, 2, . . . , n},
we denote it Sn. Note that this is not an Abelian group when n ≥ 3.
r The group Zn of residues modulo n. This is the group {0, 1, . . . , n − 1} with the

Chapter 6

additive law
We do not encourage denoting it as a + b in cryptography since this may lead
to confusions with other modular additions, or even the regular one.

(a, b) (cid:11)→ a + b mod n.

We recall that x mod n denotes the remainder in the Euclidean division of x by
n: computing the division of x by n, we obtain an integral relation x = qn + r with
0 ≤ r < n. The x mod n integer is simply r. The quotient q is the greatest integer
which is smaller than x

. We can thus write

’

n . We can denote it

x
r

(
)

*

x
n

x mod n = x −

× n.

6.1.3 Generating a Group, Comparing Groups

If we take a subset A of a group G, we say that A generates G, or is a generator set,
if any element of G can be written as a ﬁnite product (or sum for additive notations)
of elements which are taken only from A or the inverse (or opposite) of elements of
A. For instance, {1} generates Z as a group since all relative integers are ﬁnite sums of
terms all equal to 1 or to −1, the inverse of 1. It also generates Zn with the modulo n
addition.

A group (homo)morphism is a function f from a group G (whose neutral element

is 1G) to a group G

(whose neutral element is 1G(cid:8)) with the following properties:

(cid:8)

1. If 1G is the neutral element of G, then f (1G) is neutral in G
2. For all a, b ∈ G, we have f (ab) = f (a) f (b) (here, ab is a G-product and
-product);
3. For all a ∈ G, we have f (a
−1 is a G-inverse and f (a)
−1

−1) = f (a)

−1 (here, a

f (a) f (b) is a G

;

(cid:8)

(cid:8)

(cid:8)

is a G

-inverse).

We notice that the ﬁrst and third properties are consequences of the second one.

We can deﬁne the kernel f

−1({1G(cid:8)}) of f as the reciprocal image of the set {1G(cid:8)} by
f : it is the set of all a ∈ G such that f (a) is neutral in G
. We can also deﬁne the image
f (G) of f as the set of all elements reached by f . We have the following properties:

(cid:8)

r f is injective if and only if f
r f is surjective if and only if f (G) = G

−1({1G(cid:8)}) = {1G};

.

(cid:8)

When a group morphism is injective and surjective, it is bijective, thus called a group
isomorphism. In that case we say the groups are isomorphic, and considered as having
the same group structure.

Algorithmic Algebra

159

A subgroup of a group G

(cid:8)

is a subset G of G

(cid:8)

such that

1. G is nonempty,
2. for all a, b ∈ G, we have ab

−1 ∈ G.

Therefore it is a nonempty subset stable by multiplication and inversion. As an example,
{1G} and G are subgroups of G. They are the trivial subgroups. Kernels and images of
group morphisms are subgroups. Interestingly, any group G is isomorphic to a subgroup
of SG: we let f be a mapping from G to SG, deﬁned by f (a) as the permutation over G
which maps x onto ax. f is a group morphism which is injective. G is thus isomorphic
to the image of this group morphism which is hence a subgroup of SG. Therefore, any
ﬁnite group of cardinality n is isomorphic to a subgroup of Sn.

6.1.4 Building New Groups
We can make new groups from others. We can make the group product G × G
two groups G and G
law:

as the set of pairs of elements of G and G

(cid:8)

(cid:8)

(cid:8)

of any
with the product

(a, b).(c, d) = (ac, bd).

We notice that the notion of product of group is associative since G × (G
) is
isomorphic to (G × G
. We can make the product of more groups. We can also
deﬁne Gn. More generally, for any set A, we can deﬁne the group G A as the set of all
functions from A to G.

) × G

(cid:8) × G

(cid:8)(cid:8)

(cid:8)(cid:8)

(cid:8)

We can also obtain the quotient of an Abelian group G by a subgroup H. With
additive notations, we deﬁne a congruence in G in the following way: we say that
a, b ∈ G are congruent if a − b ∈ H. We denote a ≡ b (mod H). The congruence
class of a ∈ G is the set of all b ∈ G which are congruent to a. We can easily check
that this is a + H, the set of all a + h terms for h ∈ H. We then deﬁne G/H as the
set of all congruence classes. We can easily deﬁne additions in G/H by saying that the
sum of the class of a and the class of b is the class of a + b which is uniquely deﬁned:
if a ≡ a

(mod H), then a + b ≡ a

(mod H) and b ≡ b

(mod H).

(cid:8) + b

(cid:8)

(cid:8)

(cid:8)

The set nZ of all multiples of n is a subgroup of Z. We can thus deﬁne Z/nZ. As
we will see, this group is actually isomorphic to the group Zn of residues modulo n that
was deﬁned in Section 6.1.2, and we denote a ≡ b (mod n) instead of a ≡ b (mod nZ).

6.1.5 Fundamentals on Groups

We say that an element a of a ﬁnite group G has an order n if n is the smallest positive
integer such that an = 1. Note that the order can be inﬁnite, but when the group has

160

Chapter 6

a ﬁnite number of elements, all the orders of its elements are necessarily ﬁnite. We
also call order of a group its cardinality. We should thus avoid confusion between
the order of a group and the order of an element. We recall the famous Lagrange
Theorem.

Theorem 6.1 (Lagrange). In a ﬁnite group, the order of an element always divides
the order of its group.

How to compute orders in practice will be addressed in Chapter 7.

It is worth mentioning that we can characterize all ﬁnite Abelian groups by the

following result.

Theorem 6.2 (Reduction of ﬁnite Abelian groups). Let G be a ﬁnite Abelian group.
There exists a unique sequence n1, . . . , nr of natural integers such that for all i, ni is
a factor of ni+1, n1 > 1, and G is isomorphic to
× ··· × Znr

Zn1

.

6.2 The Ring Zn

6.2.1 Rings

Formally, a ring is an additively denoted Abelian group R with a second law which is
multiplicatively denoted and which fulﬁlls the following ring properties.

1. Closure: For all a, b ∈ R, a × b is in R.
2. Associativity: × is associative.
3. Neutral element: There exists a neutral element. Since it is necessarily unique,
4. Distributivity: For any a, b, c ∈ R, we have a × (b + c) = ab + ac and (a +

we denote it by 1.
b) × c = ac + bc.

We notice that distributivity implies that a × 0 = 0 × a = 0 for any a: we have a × 0 =
a × (0 + 0) = a × 0 + a × 0, which can be simpliﬁed by a × 0 to yield a × 0 = 0. We
thus notice that unless R is the trivial group, 0 must be different from 1: if 1 = 0, for any
a we have a = a × 1 = a × 0 = 0, thus the group is trivial. We notice that elements
are not always invertible with respect to ×. 0 is actually not invertible since 0 × a
cannot be equal to 1. We can however deﬁne the multiplicative group denoted R
as
the set of all invertible ring elements. When the multiplicative group consists of R with
0 removed, we say that R is a ﬁeld.

∗

In this book we only consider commutative rings: rings for which the multiplication

is also commutative.

Algorithmic Algebra

6.2.2 Deﬁnition of Zn

161

We have already mentioned the group Zn. The structure of this group is however richer.
It actually has a ring structure.

We already mentioned its additive structure of an Abelian group. We can similarly

deﬁne the multiplication by

(a, b) (cid:11)→ a × b mod n.

Once again, we do not suggest writing it a + b since it may introduce confusion.

This is actually a “pedestrian way” to deﬁne Zn. A more intellectual one consists
of saying that Zn is the quotient ring of ring Z by the principal ideal nZ generated by n.
We have seen that we can make a quotient of an Abelian group by one of its subgroups.
For rings, we quotient by ideals. An ideal is a subset I such that

1. I is a subgroup for the addition;
2. for any a ∈ I and any b ∈ R, we have ab ∈ I .

In this case, quotients are deﬁned similarly as for groups. The multiplication of the
class of a by the class of b is simply the class of ab. An arbitrary set A can generate
an ideal (cid:30)A(cid:31): it is simply the set of all ring elements which can be written as a ﬁnite
combination a1x1 + ··· + an xn with a1, . . . , an ∈ R and x1, . . . , xn ∈ A. When A is
reduced to a single element, the ideal is principal. The principal ideal generated by n
is thus simply the set of all integers for which n is a factor. Making a quotient by this
ideal simply means that we further consider all multiples of n as zero elements: we
thus reduce modulo n. Classes of residues modulo n are uniquely represented by their
representative in {0, 1, . . . , n − 1}.

Since we will later make an extensive use of Zn, it is important to get familiar with

it. We will use examples with n = 35. As a computation example, we can see that

27 + 19 mod 35 = 11

because 27 + 19 = 46 and for 46 mod 35, we make the Euclidean division of 46 by
35. We get a quotient of 1 with a remainder of 11 (indeed, 46 = 1 × 35 + 11), thus
46 mod 35 = 11. As another example, we have

17 × 22 mod 35 = 24

because 17 × 22 = 374 and the quotient of 374 by 35 is 10 with a remainder of 24
(indeed, 374 = 10 × 35 + 24).

162

Chapter 6

For completeness we need to realize that the “pedestrian” and “intellectual” ways

really deﬁne the same thing! This can be checked by the following properties

(a + nZ) + (b + nZ) = ((a + b mod n) + nZ)
(a + nZ) × (b + nZ) = ((a × b mod n) + nZ)

where a + nZ is the residue class of a, (a + nZ) + (b + nZ) means the addition of
two residue classes in the “intellectual” sense, and a + b mod n is the “pedestrian”
addition.

6.2.3 Additions, Multiplications, Inversion

We can then realize that it is computationally easy to make simple operations on large
numbers. Let us ﬁrst look at how additions can be carried out.

We assume that large numbers are represented as bitstrings. For instance, if a and

b are (cid:9)-bit numbers, we can represent them as

a = (cid:9)−1(cid:1)

i=0

ai 2i

and b = (cid:9)−1(cid:1)

bi 2i

i=0

with ai = 0 or 1 and bi = 0 or 1 for i = 0, 1, . . . , (cid:9) − 1. Fig. 6.1 is an example of
an algorithm computing the addition of a and b. This program computes the addition
by managing carries from right to left. We can prove it by induction by showing that
when entering into loop i, the addition of the truncated parts of a and b is equal to the
truncated part of c plus r.2i and that r = 0 or 1, namely

i−1(cid:1)

j=0

a j 2 j + i−1(cid:1)

b j 2 j = i−1(cid:1)

j=0

j=0

c j 2 j + r2i .

Input: a and b, two integers of at most (cid:9) bits
Output: c, an integer of at most (cid:9) + 1 bits representing a + b
Complexity: O((cid:9))
1: r ← 0
2: for i = 0 to (cid:9) − 1 do
d ← ai + bi + r
set ci and r to bits such that d = 2r + ci

3:
4:
5: end for
6: c(cid:9) ← r

Figure 6.1. Addition of big numbers.

Algorithmic Algebra
163
It is trivial for i = 0. Assuming that this holds for i, we can prove it for i + 1 by noticing
that

a j 2 j + ai 2i + i−1(cid:1)

j=0

b j 2 j + bi 2i

c j 2 j + r2i + ai 2i + bi 2i

c j 2 j + (ai + bi + r)2i .

i(cid:1)

j=0

a j 2 j + i(cid:1)

j=0

j=0

b j 2 j = i−1(cid:1)
= i−1(cid:1)
= i−1(cid:1)

j=0

j=0

The parenthesis ai + bi + r corresponds to the new d value. Since ai , bi , and r are not
greater than 1, the new d is at most 3. Thus, we can compute the Euclidean division of
d by 2:

d = 2r + ci .

Thus, with the new r value, we have

i(cid:1)

j=0

a j 2 j + i(cid:1)

b j 2 j = i−1(cid:1)

j=0

j=0

c j 2 j + ci 2i + r2i+1

and r = 0 or 1. This completes the induction. Then, the induction equation for i = (cid:9)
raises

a + b = (cid:9)−1(cid:1)

j=0

c j 2 j + c(cid:9)2(cid:9)

which means a + b = c.

This program uses elementary operations (like the addition of bits). Its complexity

is O((cid:9)). We can thus compute a + b within a linear time in the sizes of a and b.

If n is a number of exactly (cid:9) bits, we can compare a + b with n within O((cid:9))
operations by comparing bits from left to right until we have a difference. Then, if
a + b is greater than n, we can subtract n within O((cid:9)) operations. Therefore, if a and
b are smaller than n, we can compute a + b mod n within O((cid:9)) operations.

We can generalize this method for the multiplication. We have indeed two possible
iterative algorithms for multiplying a by b. One looks at all bits of b iteratively from
the rightmost to the leftmost (see Fig. 6.2), the other does the same from the leftmost
to the rightmost (see Fig. 6.3). For multiplication in Zn, we just have to replace the
regular additions by the addition in our group: the addition modulo n.

164

Chapter 6

Input: a and b, two integers of at most (cid:9) bits
Output: c = a × b
Complexity: O((cid:9)2)
1: x ← 0
2: y ← a
3: for i = 0 to (cid:9) − 1 do

if bi = 1 then
x ← x + y
end if
y ← y + y

4:
5:
6:
7:
8: end for
9: c ← x

Figure 6.2. Multiplication from right to left.

Division is harder but generalizes as well.

Finally, Fig. 6.4 depicts a program performing the Extended Euclid Algorithm.
This enables us to compute the greatest common divisor (gcd) of a and b together with
an integral relationship (called Bezout relationship)
au + bv = gcd (a, b).

The program manipulates three-dimensional vectors !x = (x1, x2, x3) and !y =
(y1, y2, y3). We can prove by induction that the above algorithm works. We ﬁrst have
to notice that we have

x1 = ax2 + bx3

and

y1 = ay2 + by3

at any time. Thus, if the program halts, we have d = au + bv. Then, we notice that |y1|
decreases at every step 4, since the new y1 is the remainder of the division of x1 by y1,

Input: a and b, two integers of at most (cid:9) bits
Output: c = a × b
Complexity: O((cid:9)2)
1: x ← 0
2: for i = (cid:9) − 1 downto 0 do

x ← x + x
if bi = 1 then
x ← x + a

3:
4:
5:
end if
6:
7: end for
8: c ← x

Figure 6.3. Multiplication from left to right.

Algorithmic Algebra

165

Input: a and b, two integers of at most (cid:9) bits
Output: d, u, v such that d = au + bv = gcd(a, b)
Complexity: O((cid:9)2)
1: !x ← (a, 1, 0), !y ← (b, 0, 1)
2: while y1 > 0 do
3: make an Euclidean division x1 = qy1 + r

do !x ← !x − q!y and exchange !x and !y

4:
5: end while
6: (d, u, v) ← !x

Figure 6.4. Extended euclid algorithm.

thus lesser than |y1|. Thus y1 eventually reaches zero and the program halts. Finally, we
notice that the gcd of x1 and y1 remains unchanged throughout the computation since

gcd(x1, y1) = gcd(y1, x1 − qy1).

When the computation starts, the gcd of x1 and y1 is the gcd of a and b. When the
computation ends, the gcd of x1 and y2 is the gcd of d and 0 which is d. Therefore
the computation eventually halts with d = gcd (a, b) = au + bv. Finally, we prove that
the complexity is O((cid:9)3). For this we notice that every loop is of complexity O((cid:9)2) (the
complexity of the Euclidean division). We thus have to prove that the maximal number
of iterations is O((cid:9)). For this, let us consider the sequence (z0, z1, . . . , zi ) of all values
taken by y1. We know that it is a decreasing sequence such that z0 < 2(cid:9), zi = 0, and
z j+1 = z j−1 − q j z j with

(cid:10)

(cid:11)

.

q j =

z j−1
z j

Note that q j can never be zero, but for the ﬁrst iteration, thus we have z j+1 + z j ≤ z j−1
for j = 1, 2, . . . , i − 1. Let us deﬁne ti = zi = 0, ti−1 = zi−1 = d, and t j−1 = t j+1 +
t j for j = i − 1, i − 2, . . . , 1. We have z j ≥ t j for j = 0, 1, . . . , i, thus t0 ≤ 2(cid:9). This
t j sequence is a Fibonacci sequence which resolves into

⎛
⎝(cid:2)

1 + √

5

(cid:2)

(cid:3)i− j −

(cid:3)i− j

⎞
⎠ .

1 − √

5

2

t j = d√
5

2

(cid:2)

⎛
⎝

(cid:2)

(cid:3)i −

1 + √

5

2

1 − √

5

2

(cid:3)i

⎞
⎠

Hence

2(cid:9) ≥ t0 ≥ 1√
5

which ends up with i = O((cid:9)).

166

Chapter 6

For completeness we mention that a more complicated analysis shows that the
complexity is actually O((cid:9)2). The proof of this result is out of the scope of this course
(see Ref. [112]).

As an important application of the Extended Euclid Algorithm, we can now com-
pute the inversion in Zn: if we want to invert an element x in Zn, we run the algo-
rithm with a = x and b = n. If the obtained gcd is 1, we get the Bezout relationship
1 = xu + nv, which means u ≡ x
−1 (mod n). If the gcd is not 1, x and n have a com-
mon factor greater than 1, so xu − qn also shares this common factor for any q ∈ Z,
therefore xu mod n cannot be equal to 1: x is not invertible modulo n.

As an example, we can invert 22 modulo 35. We run the algorithm with a = 22

and b = 35. We obtain the following sequence of vectors.

Iteration

!x

!y

0
1
2
3
4
5
6

(22, 1, 0)
(35, 0, 1)
(22, 1, 0)
(13,−1, 1)
(9, 2,−1)
(4,−3, 2)
(1, 8,−5)

(35, 0, 1)
(22, 1, 0)
(13,−1, 1)
(9, 2,−1)
(4,−3, 2)
(1, 8,−5)
(0,−35, 22)

q

0
1
1
1
2
4

Thus 1 = 22 × 8 − 35 × 5, and the inverse of 22 modulo 35 is 8.

6.2.4 The Multiplicative Group Z

∗
n

∗
n denote the multiplicative group of all invertible elements and let ϕ(n) denote its
Let Z
∗
cardinality (called Euler totient function). We can prove the following properties of Z
n.

Theorem 6.3. Given an integer n, we have the following results.

⇐⇒ gcd(x, n) = 1.

1. For all x ∈ Zn we have x ∈ Z
∗
n
2. Zn is a ﬁeld if and only if n is prime.
3. For all x ∈ Z
4. For any e such that gcd(e, ϕ(n)) = 1, then x (cid:11)→ x e mod n is a permuta-
−1 mod ϕ(n) mod n is the only e-th root of y

n we have x ϕ(n) ≡ 1 (mod n).
∗

n, and for all y ∈ Z
∗
∗
n, ye
tion on Z
modulo n

We already proved the ﬁrst property. For the second one, we recall that Zn is a ﬁeld
∗
n consists of all nonzero elements of Zn, which means that for any
if and only if Z
x = 1, . . . , n − 1, we have gcd(x, n) = 1. This holds if and only if n is a prime. For
∗
the third property, we notice that if x is a group element of Z
n, the Lagrange theorem
says that its order is a factor of the cardinality of the group which is ϕ(n). Hence
x ϕ(n) ≡ 1 (mod n). The last property consists of solving the x e ≡ y (mod n) equation

Algorithmic Algebra
167
in x. If gcd(e, ϕ(n)) = 1, we know that e is invertible in Zϕ(n), so we can compute
−1 mod ϕ (n). Now we can raise the equation to the power d and get x ed ≡ yd
d = e
(mod n). But since ed ≡ 1 (mod ϕ(n)), x ed mod n can be written x 1+kϕ(n) mod n
which is equal to x. Thus x ≡ ye
−1 mod ϕ(n) mod n. Conversely, we check that this x is
a solution of the equation by raising it, modulo n, to the power e.

6.2.5 Exponentiation

We notice that a modular exponentiation x e mod n does not necessarily require to per-
form e multiplications. We have much more efﬁcient algorithms. Note that x e mod n
can be deﬁned by x × x × ··· × x (e times) where the multiplications are performed
modulo n, so we can adapt the multiplication algorithm of Figs. 6.2 and 6.3 to the expo-
∗
nentiation in Z
n by replacing the regular addition by a multiplication modulo n. Fig. 6.5
is an algorithm which computes the exponentiation “from right to left” because it reads
all exponent bits in this direction. Conversely, Fig. 6.6 computes the exponentiation
“from left to right.” Both algorithms require O(log e) modular multiplications, thus a
complexity of O((cid:9)2 log e). We have many possible improvements of these algorithms.
For instance we can read the exponent from left to right in basis B instead of a binary
basis. We then need to precompute all a j mod n for j = 0, . . . , B − 1 prior to the loop.

6.2.6 Zmn: The Chinese Remainder Theorem

Exponentiation in Zmn can also be accelerated by using the following theorem.

Theorem 6.4 (Chinese Remainder Theorem). Let m and n be two integers such that
gcd(m, n) = 1. We have

f : Zmn → Zm × Zn deﬁned by f (x) = (x mod m, x mod n) is a ring isomor-
phism
−1(a, b) ≡ an(n

1.
2. ϕ(mn) = ϕ(m)ϕ(n)
3.

−1 mod m) + bm(m

f

−1 mod n) (mod mn)

Input: a and n, two integers of at most (cid:9) bits, an integer e
Output: x = ae mod n
Complexity: O((cid:9)2 log e)
1: x ← 1
2: y ← a
3: for i = 0 to (cid:9) − 1 do
x ← x × y mod n
end if
y ← y × y mod n

if ei = 1 then

4:
5:
6:
7:
8: end for

Figure 6.5. Exponentiation from right to left (Square-and-Multiply).

168

Chapter 6

Input: a and n, two integers of at most (cid:9) bits, an integer e
Output: x = ae mod n
Complexity: O((cid:9)2 log e)
1: x ← 1
2: for i = (cid:9) − 1 downto 0 do

x ← x × x mod n
if ei = 1 then

x ← x × a mod n

3:
4:
5:
end if
6:
7: end for

Figure 6.6. Exponentiation from left to right (Square-and-Multiply).

To prove the ﬁrst property, we ﬁrst observe that Zm × Zn is a product ring with unit
element (1, 1). The f function then fulﬁlls

f (x y mod mn) = f (x) f (y)

and f (1) = (1, 1), which are the ring morphism properties. Finally, f (x) = (0, 0) im-
plies x mod m = 0 and x mod n = 0, which means that x is a multiple of m and n
simultaneously. Since m and n have no common prime factor, this means that x is a mul-
tiple of mn, so that x mod mn = 0. Thus the preimage of zero by f in Zmn is {0}, which
means that f is injective. Now since the cardinalities of Zmn and Zm × Zn are equal, f
must be a bijection, hence a ring isomorphism. For the second property, we can see that
invertible elements of Zm × Zn are elements of Z
∗
∗
n, so there are ϕ(m)ϕ(n) many.
We have ϕ(mn) invertible elements in Zmn. Since Zmn and Zm × Zn are isomorphic
m
rings, the number of invertible elements must be the same, hence ϕ(mn) = ϕ(m)ϕ(n).
Finally, we check the last property by computing the f of the right-hand term. We ﬁrst
reduce it modulo m. We know that for any x, (x mod mn) mod m = x mod m, thus
we can remove the ﬁnal reduction modulo mn. Next, the modulo m reduction cancels
−1 mod n) which is a factor of m. The remainder is
the second term of the sum bm(m
−1 mod m) mod m which is a. The modulo n reduction similarly outputs b. Hence
an(n
the right-hand term of the last property is actually the preimage of (a, b).

× Z

As an example in Z35, we can let m = 5 and n = 7. If we want to solve x mod 5 = 3

and x mod 7 = 4 simultaneously, we compute

3 × 7 × (7

−1 mod 5) + 4 × 5 × (5

−1 mod 7)

mod 35.

−1(3, 4) =(cid:12)

f

(cid:13)

The Extended Euclid Algorithm gives us the following Bezout relationship.

1 = (−2) × 7 + 3 × 5.

Thus 5

−1 mod 7 = 3 and 7

−1 mod 5 ≡ −2 which gives us 7

−1 mod 5 = 3. Hence

−1(3, 4) = (3 × 7 × 3 + 4 × 5 × 3) mod 35 = 123 mod 35 = 18.

f

Algorithmic Algebra
We can check that 18 mod 5 = 3 and 18 mod 7 = 4.

169

This theorem can be used to compute exponentiation in Zmn faster. Since Zmn
is isomorphic to the Zm × Zn product structure, instead of computing ae mod mn,
we can compute ae mod m and ae mod n which gives ae in Zm × Zn. Then we can
use the Chinese Remainder Theorem to recover ae mod mn. Since the complexity of
exponentiation is cubic in the size of the modulus, assuming that m and n are half of
the size of mn, exponentiation in Zm costs 1/8 of the exponentiation price in Zmn,
as well as exponentiation in Zn. Since applying the Chinese Remainder Theorem is
quadratic, we speed up the exponentiation by a factor of 4. This method needs however
to use the factorization of mn, which is not always available as we will see in the next
sections.

For instance, if we want to compute 223 mod 35, we ﬁrst compute 223 mod 5 and
∗
223 mod 7. For 223 mod 5, we notice that Z
5 has a cardinality of 4 since Z5 is a ﬁeld
(see Section 6.3). Hence the Lagrange theorem implies that 24 mod 5 = 1. We can
reduce 23 modulo 4, by 23 = 4 × 5 + 3, and obtain 223 mod 5 = 23 mod 5. It is
now easy to compute 23 mod 5 which gives 223 mod 5 = 3. Similarly, 223 mod 7 =
25 mod 7 = 4. Now we apply the Chinese Remainder Theorem for computing f
−1(3, 4)
and obtain that 223 mod 35 = 18. If we apply the algorithm of Fig. 6.6, we ob-
tain the following sequence of x values in step 2 with the corresponding bits of
e = 23

e

x

1

1

0

4

1

16

1

9

1

9

and we terminate with x = 18 as well.

We summarize all the above algorithms by the following complexities for compu-

tations in Zn. We assume that all operands are lesser than n.

r Addition: linear in the size of n.
r Multiplication, inversion, gcd: quadratic in the size of n.
r Exponentiation: cubic in the size of n.

6.3 The Finite Field Zp

6.3.1 Basic Properties of Z p

We recall that a ﬁeld is a ring K in which the multiplicative group K
consists of
all nonzero ring elements. This means that all elements but zero are invertible for the
multiplication.

∗

170

Chapter 6

In Theorem 6.3 we saw that a ring element x of Zn is invertible if and only if the
gcd of x and n is 1. When n is prime, all nonzero elements of Zn fulﬁll this property.
But if n is composite, we may have a nontrivial factor x, and for all integers y we know
that x y mod n is a multiple of x, so it cannot be 1. This means that Zn is a ﬁeld if and
only if n is a prime integer. In the following, we let p denote a prime integer and focus
on the ﬁeld Z p.

It is important to emphasize that Z p is a ﬁnite ﬁeld: we have a ﬁnite number of
elements in it. This is not the case for classical ﬁelds: the ﬁeld of real numbers R, the
ﬁeld of complex numbers C, the ﬁeld of rational numbers Q, etc.

Here are a few properties allowing to get familiar with Z p.

= {1, . . . , p − 1} since Z p = {0, 1, . . . , p − 1} and zero is the only nonin-

r Z
r ϕ( p) = p − 1 since Z
r for any x ∈ Z
p, we have x p−1 ≡ 1 (mod p) since the order of x is a factor of the
∗
order p − 1 of Z

p contains p − 1 elements;
∗

∗
p

vertible element;

∗
p.

Theorem 6.5. Let p be a prime number. Z

We have the following important result.
p is a cyclic group with ϕ( p − 1) generators.
∗
We recall that a cyclic group is a group which has at least one generator, namely an
= {1, g, g2, . . . , g p−2}.
∗
element g such that all group elements are powers of g, i.e. Z
∗
p
Assuming that Z
p is cyclic, it is fairly easy to prove that the number of possible
generators is actually ϕ( p − 1). For this we can pick a generator g and ﬁnd conditions
on i = 1, . . . , p − 2 for gi mod p to be a generator of Z
∗
p. It is a generator if and only
if there exists a j such that gi j mod p = g: it is necessary because a generator must
generate g, and it is sufﬁcient because if it generates g, we know that we can generate
all elements from g only. Since the exponents of g “live” in Z p−1, gi j mod p = g is
equivalent to i j ≡ 1 (mod p − 1). Hence such a j exists if and only if i is invertible
modulo p − 1. We already know that we have exactly ϕ( p − 1) invertible elements
modulo p − 1, which completes the proof.

(cid:1)Quadratic Residues

6.3.2
In Theorem 6.3 we saw how to extract e-th roots modulo p of an element x ∈ Z
∗
p when e
−1 mod p − 1.
is invertible modulo ϕ( p) = p − 1: we just have to raise x to the power e
How can we proceed when e is not invertible modulo p − 1? In particular, how can we
proceed with e = 2 which will never be invertible since p − 1 is even?3 As for real num-
bers, square roots do not always exist (e.g. for negative numbers), and when they do, they
come in couples which are the opposite of each other. In ﬁnite ﬁelds, we call quadratic
residues the elements which can be written as the square of some other element.
3 We omit the p = 2 case here which is trivial.

Algorithmic Algebra

171

(mod p)

p where p ≥ 3 is prime
Input: a quadratic residue a ∈ Z
∗
Output: b such that b2 ≡ a
1: repeat
choose g ∈ Z
∗
p at random
2:
3: until g is not a quadratic residue
4: let p − 1 = 2st with t odd
5: e ← 0
6: for i = 2 to s do
e ← 2i−1 + e

2i mod p (cid:4)= 1 then

−e)

p−1

if (ag

7:
8:
end if
9:
10: end for
11: b ← g
−t e

2 a t+1

2 mod p

∗
Figure 6.7. The Tonelli algorithm for square roots in Z
p.

Let us ﬁrst state the following theorem.

Theorem 6.6. Let p be an odd prime number. We have
1. x ∈ Z
∗
p is a quadratic residue if and only if x p−1
2. p−1
3. that if p ≡ 3 (mod 4), for any quadratic residue x ∈ Z
∗
p, the two square roots

2 mod p = 1

∗
p

2 quadratic residues in Z
of x modulo p are x p+1

4 mod p and −x p+1

4 mod p

The last property holds only for p ≡ 3 (mod 4), i.e. when p + 1 is a multiple of 4. For
other primes p, we generalize this formula by using the Tonelli algorithm in Fig. 6.7.

Proof. We ﬁrst notice that since Z p is a ﬁeld, the number of possible solutions to the
polynomial equation y2 − x = 0 in y is limited to two. For x (cid:4)= 0, we can even notice
that if y is a root, then −y is another different root. Thus the number of solutions for
x (cid:4)= 0 is zero or two. Due to the pigeonhole principle, we infer that all elements of
∗
Z
p will be mapped onto the set of all quadratic residues by the square operation as
a two-to-one mapping. This proves the second property, namely we have exactly p−1
∗
2
quadratic residues in Z
p. To prove the ﬁrst property, we notice that if x is a quadratic
residue, we can write x = y2 mod p. Then we have

p−1

2 ≡ y p−1 ≡ 1 (mod p).

x

Thus all quadratic residues are roots of the equation x p−1
quadratic residues and at most p−1

2 ≡ 1. Since we have exactly p−1
roots, this shows that all roots are quadratic residues.

2

2

The third property is straightforward: if x is a quadratic residue, let y = ±x p+1

4 mod

p. We have

y2 ≡ x

p+1

2 ≡ x

p−1
2 x

(mod p).

172

Since x is a quadratic residue, we know that x p−1
root of x.

Chapter 6
2 mod p = 1. This shows that y is a
(cid:1)

6.4 Finite Fields

Finite ﬁelds are useful in communication systems, and in cryptography in particular. In
addition to all Z p ﬁelds, we have other ﬁnite ﬁelds. We give without proof the following
theorem which characterizes the ﬁnite ﬁelds.4

Theorem 6.7. We have the following results.

1. The cardinality of any ﬁnite ﬁeld is a prime power pk.
2. For any prime power pk, there exists a ﬁnite ﬁeld of cardinality pk. p is called

the characteristic of the ﬁeld.

3. Two ﬁnite ﬁelds of same cardinality are isomorphic, so the ﬁnite ﬁeld of car-
dinality pk is essentially unique. We denote it by GF( pk) as Galois ﬁeld of
cardinality pk.

4. GF( pk) is isomorphic to a subﬁeld of GF( pk×(cid:9)).
5. GF( pk) can be deﬁned as the quotient of the ring Z p[x] of polynomials with
coefﬁcients in Z p by a principal ideal spanned by an irreducible polynomial of
degree k: Z p[x]/(P(x)).

The last property suggests a way to represent ﬁnite ﬁelds: in order to deﬁne GF( pk),
we ﬁrst ﬁnd an irreducible polynomial P(x) of degree k in Z p[x].5 Then we repre-
sent an element of GF( pk) as a polynomial in Z p[x] of degree at most k − 1. Ad-
ditions are then performed as regular additions modulo p. Multiplications are per-
formed modulo P(x) and modulo p. Then all algorithms of this section generalize to
GF( pk).

As an example, let us consider GF(4) where p = 2 and k = 2. We must get
an irreducible polynomial of degree 2 in Z2[x]. We have only four polynomials of
degree 2:

r x 2 which is equal to x × x
r x 2 + 1 which is equal to (x + 1) × (x + 1) (remember that 1 + 1 = 0 in Z2)
r x 2 + x which is equal to x × (x + 1)
r x 2 + x + 1 which is irreducible.

4 For a more complete treatment on ﬁnite ﬁelds, we suggest the textbook by Lidl and Niederreiter (Ref. [117]).
5 Picking irreducible polynomials is quite easy. We can for instance pick random polynomials and check
irreducibility. Factorization of polynomials is actually easily feasible, so instead of making irreducibility
tests, we can directly try to factorize. More elaborate ways are also possible. They can be found in any
textbook on ﬁnite ﬁelds.

Algorithmic Algebra
173
Let P(x) = x 2 + x + 1 in Z2[x]. We consider GF(4) as Z2[x]/(P(x)). Any polynomial
over Z2 can be reduced modulo P(x) into a polynomial of degree at most 1, so we have
only four classes of polynomials:

r the class c0 of polynomials congruent to 0
r the class c1 of polynomials congruent to 1
r the class c2 of polynomials congruent to x
r the class c3 of polynomials congruent to x + 1

Therefore GF(4) = {c0, c1, c2, c3}. Addition is quite straightforward, as well as mul-
tiplication by c0 or c1. We however have to explain how to compute c2 × c2, c2 × c3,
and c3 × c3. c2 × c2 is x 2 which is equal to x + 1 after reduction modulo P(x) since
x 2 = x + 1 + P(x) thus c2 × c2 = c3. Similarly, we have x 2 + x = 1 + P(x), thus
c2 × c3 = c1, and x 2 + 1 = x + P(x), thus c3 × c3 = c2. Here are the addition and
multiplication tables.

+ c0
c0
c0
c1
c1
c2
c2
c3
c3

c1
c1
c0
c3
c2

c2
c2
c3
c0
c1

c3
c3
c2
c1
c0

× c0
c0
c0
c0
c1
c2
c0
c0
c3

c1
c0
c1
c2
c3

c2
c0
c2
c3
c1

c3
c0
c3
c1
c2

We notice that the additive group is isomorphic to Z2 × Z2 (and not to Z4!), that c0
and c1 are neutral elements for the addition and the multiplication respectively, that all
elements but c0 are invertible, and that GF(4)

is isomorphic to Z3.

∗

6.5 (cid:1)Elliptic Curves over Finite Fields
Elliptic curves are odd structures. They are curves of equations like y2 = x 3 + ax + b.
This looks like the curve in Fig. 6.8 when x and y are real numbers, depending on the
parameters a and b. They are used to deﬁne new types of groups for cryptography. For
completeness we give all necessary material to implement algorithms on elliptic curves
over ﬁnite ﬁelds.

6.5.1

(cid:1)Characteristic p > 3

Let us ﬁrst deﬁne an elliptic curve and point addition.
Deﬁnition 6.8. Given a ﬁnite ﬁeld K of characteristic p > 3 and given a, b ∈ K such
that 4a3 + 27b3 (cid:4)= 0, we let

Ea,b = {O} ∪ {(x, y) ∈ K2; y2 = x3 + ax + b}.

174

Chapter 6

P2

P1 + P2

P1

Figure 6.8. Elliptic curve example over the real numbers.

Given P = (x,y), we deﬁne −P = (x,−y) and −O = O. Given P1 = (x1, y1) and
P2 = (x2, y2), if P2 = −P1, we deﬁne P1 + P2 = O. Otherwise, we let

+

if x1 (cid:4)= x2
if x1 = x2

y2−y1
x2−x1
+a
3x 2
1
2y1

λ =
x3 = λ2 − x1 − x2
y3 = (x1 − x3)λ − y1
P3 = (x3,y3)

and P1 + P2 = P3. In addition, P + O = O + P = P and O + O = O.

We further deﬁne the discriminant  = −16(4a3 + 27b2) and the j-invariant j =

−1728(4a)3/, which can also be expressed as
4

j = 1728

4 + 27b2/a3

when a (cid:4)= 0.
(Note that the x1 = x2 case in the λ formula implies that y1 = y2, thus P1 = P2, and
y1 (cid:4)= 0. Otherwise we would have had P2 = −P1.) The deﬁnition of point addition
may look quite odd. It can be geometrically illustrated as depicted in Fig. 6.8. Actually,
y − y1 = λ(x − x1) is the equation of a straight line which contains P1. It is the chord
which contains P2 when P1 (cid:4)= P2. It is the tangent to the elliptic curve when P1 = P2.
Since Ea,b is an algebraic curve of degree 3, it usually intersects straight lines on three
points. The intersection between this line and the curve is deﬁned by

(y1 + λ(x − x1))2 = x 3 + ax + b

Algorithmic Algebra

175

by expanding in terms of x we obtain

x 3 − λ2x 2 + (a − 2λ(y1 − λx1))x + (b − (y1 − λx1)2) = 0.

This polynomial equation of degree 3 clearly has three roots whose sum is λ2. Since
x1 and x2 are known roots, the third one is simply

x3 = λ2 − x1 − x2.

It corresponds to the third intersection point. Due to the equation of the line, we notice
that (x3,−y3) is this third point (see Fig. 6.8). (x3,−y3) is therefore on the curve, so
P3 is as well.

We summarize important facts on elliptic curves.

Theorem 6.9. Given a ﬁnite ﬁeld K of characteristic p > 3 and given a, b ∈ K such
that 4a3 + 27b3 (cid:4)= 0, we let Ea,b be the elliptic curve as deﬁned in Def. 6.8.

1. Ea,b together with the point addition forms an Abelian group where O is the

2. For any a

neutral element.
and b
if there exists some u ∈ K
∗

(cid:8)

(cid:8)

, the group Ea,b is isomorphic to the group Ea(cid:8),b(cid:8) if and only

such that a

(cid:8) = au4 and b

(cid:8) = bu6.

3. Two isomorphic elliptic curves on K have the same j-invariant. The converse

is true when K is algebraically closed.

Proof (sketch). To prove the ﬁrst property, we notice that addition is trivially commu-
tative, with a neutral element O and that every P point has an opposite −P point such
that P + (−P) =O. We already saw that addition is internal in Ea,b. What remains
to prove is associativity. This part can be proven in a sophisticated way or through an
exhausting computation.

For the second property we notice that (x, y) (cid:11)→ (u2x, u3 y) deﬁnes a mapping
from Ea,b to Eu4a,u6b. We further notice that it is a group isomorphism. The converse
is true as well.

For the third property, we notice that if Ea,b and Ea(cid:8),b(cid:8) are isomorphic, then we can
)3 = b2/a3 and the two curves have
(cid:1)

write a
the same j-invariant. We omit the proof for the converse result.

(cid:8) = bu6. Therefore (b

(cid:8) = au4 and b

)2/(a

(cid:8)

(cid:8)

(cid:8) = av2 and b

(cid:8) = bv3 for some v ∈ K
∗

Let a

. Obviously, when v is a quadratic
residue, it can be written v = u2 and we deﬁne an isomorphic group Ea(cid:8),b(cid:8). One may
wonder what happens when v is a not a quadratic residue. Obviously, we obtain another
curve whose isomorphism class depends on the isomorphism class of Ea,b only. It is
actually called the twist of the curve. Note that although a curve and its twist share
the same j-invariant, they are usually not isomorphic. Their cardinality is actually
complementary in the sense of the following theorem.

176
Chapter 6
Theorem 6.10. Given a ﬁnite ﬁeld K of characteristic p > 3 and given a, b ∈ K such
that 4a3 + 27b2 (cid:4)= 0, we let Ea,b be the elliptic curve deﬁned in Def. 6.8. The twist of
Ea,b is Eau2,bu3 for u ∈ K
∗

which is not a quadratic residue. We have
#Ea,b + #Eau2,bu3 = 2#K + 2.

Proof. For any x ∈ K, we notice that

quadratic residue, or

a quadratic residue, or

r either x 3 + ax + b is a nonzero quadratic residue, or and u3(x 3 + ax + b) is not
r x 3 + ax + b is not a quadratic residue and u3(x 3 + ax + b) is a nonzero
r x 3 + ax + b = u3(x 3 + ax + b) = 0.
f (x) be the number of afﬁne points on Ea,b (i.e. all points except O). When
Let
x 3 + ax + b is a nonzero quadratic residue, then f (x) = 2. When x 3 + ax + b is not a
quadratic residue, then f (x) = 0. When x 3 + ax + b = 0, then f (x) = 1. Let g(x) be
the number of points on Eau2,bu3 which can be written (ux , y). We notice that g(x) is
respectively 0, 2, and 1 in the three cases. Therefore f (x) + g(x) = 2 for all x. Since
(cid:1)
this counts all afﬁne points on Ea,b and Eau2,bu3, we obtain the result.

Note that since we have a group law, we can deﬁne m P for any integer m and any
point P and compute m P by the square-and-multiply algorithms which were deﬁned
(with multiplicative notations) in Figs. 6.5 and 6.6.

6.5.2

(cid:1)Characteristic Two

Finite ﬁelds of characteristic two are important in practice. For completeness we provide
here the deﬁnitions and properties related to elliptic curves over these ﬁelds.

Let us ﬁrst recall that given a ﬁnite ﬁeld K of cardinality 2m we can deﬁne the trace

Tr2m ,2 function by

Tr2m ,2(x) = m−1(cid:1)

i=0

x 2i

which is a linear form of K over GF(2).
Deﬁnition 6.11. Given a ﬁnite ﬁeld K of characteristic two and given a6 ∈ K
∗
a2 ∈ {0, γ} with γ such that Tr#K ,2(γ ) = 1, we let

and

= {O} ∪ {(x, y) ∈ K2; y2 + x y = x 3 + a2x 2 + a6}

Ea2,a6

Algorithmic Algebra
177
be a non-supersingular elliptic curve. Given P = (x, y), we deﬁne −P = (x, x + y)
and −O = O. Given P1 = (x1, y1) and P2 = (x2, y2), if P2 = −P1, we deﬁne
P1 + P2 =O. Otherwise, we let
+
y2+y1
x2+x1
+
+y1
x 2
1
x1
y1x2+y2x1
x2+x1
x 2
1

if P1 (cid:4)= P2
otherwise
if P1 (cid:4)= P2
µ =
otherwise
x3 = λ2 + λ + a2 + x1 + x2
y3 = (λ + 1)x3 + µ = (x1 + x3)λ + x3 + y1
P3 = (x3, y3)

λ =

and P1 + P2 = P3. In addition, P + O = O + P = P.

We further deﬁne the discriminant  = a6 and the j-invariant j = 1/.

We have similar results for group structures.
Theorem 6.12. Given a ﬁnite ﬁeld K of characteristic two, let γ ∈ K
∗
Tr#K ,2(γ ) = 1. Given a6 ∈ K
∗
deﬁned in Def. 6.11.

be such that
and a2 ∈ {0, γ}, we let Ea2,a6 be the elliptic curve as

(cid:8)
6

neutral element.
∈ K
∗

2. For any a

(cid:8)
6 if and only if a2 = a
2

1. Ea2,a6 together with the point addition is an Abelian group of which O is the
∈ {0, γ}, the group Ea2,a6 is isomorphic to the group
2 and a6 = a
(cid:8)
(cid:8)
6.
+ #Eγ ,a6

3. E0,a6 and Eγ ,a6 are called twist of each other. We have
= 2#K + 2.

#E0,a6

and a

Ea

(cid:8)
2

(cid:8)

,a

6.5.3

(cid:1)General Results

We mention an important result that will be used later.*

Theorem 6.13 (Hasse 1933). Let K be a ﬁnite ﬁeld and E be an elliptic curve on K.
We have #E = #K + 1 − t where |t| ≤ 2

#K. t is called the trace of Frobenius.

√

Computing #E is quite technical (but feasible in polynomial time).

For some technical reasons, we deﬁne special elliptic curves.

* See, e.g. Ref [171]

178

Chapter 6

is equivalent to the fact that the algebraic equation which deﬁnes the elliptic
curve has a singular point. We notice that this is excluded by our deﬁnitions.

r An elliptic curve is singular if its discriminant  is zero. We can prove that this
r An elliptic curve is supersingular if its trace of Frobenius is multiple of the
characteristic of the ﬁeld. For the characteristic two case, we can prove that it is
equivalent to j = 0, which is excluded by our deﬁnition. For a characteristic p >
3, we can prove that it is equivalent to t = 0, which implies that #E = #K + 1.
r An elliptic curve is anomalous if its trace of Frobenius is one. This implies that
#E = #K.

According to the state of the art of research, these special curves (except anomalous
curves of characteristic two) should be avoided for cryptographic use.

6.6 Exercises

Exercise 6.1. Show that the Caesar cipher is “isomorphic” to the addition of 3 in Z26.
Similarly, what is ROT13?

Exercise 6.2. Show that the Vigen`ere cipher can be considered as a block cipher in
ECB mode deﬁned by addition in Zm
26.
Exercise 6.3. Let a, b, and n be three integers of at most (cid:9) bits and n ≥ 2(cid:9)−1. Prove
that we can compute a × b mod n within a time complexity of O((cid:9)2).

Exercise 6.4. Let a and b be two integers of at most (cid:9) bits. Prove that we can perform
an Euclidean division a = bq + r within a complexity of O((cid:9)2).

Exercise 6.5. We deﬁne a new cipher. The message space is Z26 (we encrypt arbitrary
× Z26 with a uniform
∗
long alphabetical messages in ECB mode). The key space is Z
distribution. Given a key K = (a, b), we deﬁne C(x) = ax + b mod 26.
26

How many possible keys do we have?
Show that we have perfect secrecy.
How can we break it with a known plaintext attack?

Exercise 6.6 (Hill cipher). We deﬁne a new cipher. The message space is Zm
space is the set of all m × m invertible matrices in Z26.

26. The key

How many keys do we have?
How can we break it with a known plaintext attack?

Exercise 6.7. Let p be an odd prime number. Prove that the algorithm in Fig. 6.7
∗
computes square roots in Z
p.

Algorithmic Algebra
179
Exercise 6.8. Let G be the set of all x ∈ Z p2 such that x ≡ 1 (mod p). Show that G
is a multiplicative group, that L : G → Z p deﬁned by L(x) = x−1
is an isomorphism,
that p + 1 is a generator of G, and that L is the logarithm in base p + 1 in L. Infer
that ϕ( p2) = p( p − 1).6

p

Exercise 6.9. Considering the reduction modulo p as a group homomorphism on
pα → Z
p, prove that ϕ( pα) = ( p − 1) pα−1.
∗
∗
Z

Infer that if p1, . . . , pr are distinct prime integers and if α1, . . . , αr are nonzero

positive integers, then

ϕ( pα1
1

× ··· × pαr

r ) = r(cid:27)

i=1

( pi − 1) pαi−1

i

.

6 This exercise was inspired by the Okamoto–Uchiyama cryptosystem. See Ref. [143].

7
Algorithmic Number Theory

Content

Primality: Fermat test, Miller-Rabin test
(cid:1)Primality: Carmichael numbers, Solovay–Strassen test
(cid:1)Factorization: rho method, p − 1 method, elliptic curve method
(cid:1)Discrete logarithm: baby steps – giant steps, Pohlig–Hellman

This chapter is a continuation of the previous one. Here we see that prime numbers
can be efﬁciently generated, while factorization is intractable to date. We also study the
discrete logarithm problem. These are basic tools in public-key cryptography.

7.1 Primality

This section deals with the prime number generation problem. We ﬁrst try to distinguish
prime numbers from composite ones by primality tests.

We ﬁrst recall the intuitive method depicted in Fig. 7.1. This algorithm tries to
divide the input n by every integer. At each iteration, we perform all possible divisions
by integers up to i − 1 and obtain x, thus the remaining factors are between i and x.
2 and i − 1, we know that the remaining factors lie between i and b = (cid:28)√
This method has been optimized: since we know that there is no factor of x between
x(cid:29). Thus
we can stop as soon as i > b. The worst case occurs when n is prime or a product
n iterations, which is enormous for
of two primes of same size, for which we need
typical numbers in cryptography. We notice however that this algorithm does more than
expected since it prints the factorization of the input instead of just checking whether
or not it is prime. Primality tests are nevertheless easier than factorization as will be
shown in Section 7.2.

√

7.1.1 Fermat Test

A ﬁrst important primality test is the Fermat test. We ﬁrst notice that all prime numbers
p are such that for any b with 0 < b < p, we have b p−1 ≡ 1 (mod p). This property
is known as the Little Fermat Theorem. We can thus check the primality behavior of a
number n by picking a random b such that 0 < b < n, and checking whether bn−1 ≡
1 (mod n). For all prime numbers, this veriﬁcation will always succeed. But how about
composite numbers?

182

Chapter 7

Input: an integer n
√
Output: a list of prime numbers whose product is n
Complexity: O(
1: b ← (cid:28)√
n) arithmetic operations
n(cid:29), x ← n, i ← 2
2: while x > 1 and i ≤ b do

while i divides x do

print i
x ← x/i
b ← (cid:28)√
x(cid:29)
end while
i ← i + 1

3:
4:
5:
6:
7:
8:
9: end while
10: if x > 1 then print x

Figure 7.1. Trial Division Algorithm.

One deﬁciency in the Fermat test is that it checks a necessary, but not sufﬁcient,
condition: there are some composite numbers for which the above test has the same
behavior as for prime numbers, unless we pick p dividing n. For example, n = 561 =
3 · 11 · 17 is such that for all b’s which are prime with n, we have bn−1 ≡ 1 (mod n).
This can be proven easily: we notice that n − 1 = 560 = 24 · 5 · 7 which is a multiple of
3 − 1, 11 − 1, and 17 − 1. Therefore, if b is prime with 3, we have bn−1 ≡ 1 (mod 3)
and the same for 11 and 17. Hence, from the Chinese Remainder Theorem we obtain
that if b is prime with n we have bn−1 ≡ 1 (mod n). We study these numbers in more
detail in Section 7.1.2.

7.1.2

(cid:1)Carmichael Numbers

Theorem 7.1. We call Carmichael number any integer n which is a product of at least
two pairwise different prime numbers p such that p − 1 is a factor of n − 1. An integer
n is a Carmichael number if and only if it fulﬁlls the Fermat property: for any b that is
prime with n, we have bn−1 ≡ 1 (mod n).

It follows that the Fermat test cannot be used in order to distinguish prime numbers
from composite ones.
Proof. We can easily show that a Carmichael number n = p1 ··· pr fulﬁlls the Fermat
property: if b is prime with n, it is prime with any pi , we have b pi−1 ≡ 1 (mod pi ).
Then since pi − 1 is a factor of n − 1 we obtain that bn−1 ≡ 1 (mod pi ). Finally,
thanks to the Chinese Remainder Theorem we obtain that bn−1 ≡ 1 (mod n). This
means that unless we pick a b that is already a factor of some unknown prime factor
of n, the Carmichael numbers will behave just like a prime number in the Fermat
test.

Algorithmic Number Theory

183

Proving converse result is a bit more technical. We assume that n is such that
r be the factorization of n

bn−1 ≡ 1 (mod n) for any b ∈ Z
n. Let n = pα1
∗
where the pi ’s are pairwise different prime numbers. We use two lemmata.

× ··· × pαr

1

Lemma 7.2. Let G be a group of order n. If p is a prime factor of n, there exists an
element of order p in G.

Lemma 7.3. Given a ﬁnite group G, we call exponent of G the smallest integer µ such
that x µ = 1 for all x ∈ G. The exponent is equal to the lcm of all element orders in G.
If n is such that x n = 1 for all x ∈ G, then µ must be a factor of n.

From this we infer that the exponent and the group order have exactly the same prime
n (whose order is ϕ(n)) and bn−1 ≡ 1
∗
factors. In our case, G is Z
(mod n) for all
b ∈ G, and thus all prime factors of ϕ(n) are factors of n − 1. If for some i we had
αi > 1, then pi would be a factor of ϕ(n), and therefore a factor of n − 1 as well. But
a pi cannot simultaneously be a factor of n and n − 1. Therefore we have that αi = 1
for all i.

∗
∗
n is isomorphic to Z
We know from the Chinese Remainder Theorem that Z
× ··· × Z
pi is a cyclic subgroup of order pi − 1, there must be an element
∗
p1
b of order pi − 1, but bn−1 ≡ 1 (mod n) implies that pi − 1 is a factor of n − 1 for
(cid:1)
any i.

∗
pr . Since Z

Proof (Lemma 7.2). We prove this by induction on the order of G. This is trivial when
the order is 1. If x is an element of G of order k > 1, we ﬁrst assume that p is not a
factor of k. x generates a subgroup H of G of order k, and G/H is a group of order
n
< n. Since this is a factor of p, there must be an element yH of order p. It is then
k
easy to see that the order of y must be a multiple of p. This shows that we must have
an element x of G of order multiple of p. It is then easy to see that x
p is of order
(cid:1)
p.
Proof (Lemma 7.3). Let H be the set of all integers µ for which x µ = 1 for all x ∈ G.
It is quite easy to notice that H is a subgroup of Z: it is not empty (it contains the
order of G), it is stable with respect to addition and subtraction. Due to a structure
property1 of Z, H must be generated by a single integer µ which is the exponent of
G.

k

We also prove by induction that we can compute the smallest integral power which
makes all elements of a subset A vanish by computing the lcm of the orders of all
(cid:1)
elements in A.

1 When considering the smallest nonnegative element µ of H, we can prove that µ spans the whole subgroup
H: clearly, all elements spanned by µ are in H; conversely, if x ∈ H, the Euclidean division x = qµ + r
of x by µ tells us that r = x − qµ is in H as well and 0 ≤ r < µ, but since µ is the smallest nonnegative
element of H, then we must have r = 0, which means that x = qµ is spanned by µ.

184
Chapter 7
Finally, Carmichael numbers exist (we have seen n = 561 as an example). We further
know that there exist an inﬁnite number of such numbers.2

7.1.3

(cid:1)Solovay–Strassen Test

Prime numbers have other properties which can be used by primality tests. For instance,
the Euler test says that for primes p and 0 < b < p, we have

(cid:18)

(cid:19)

2 ≡
p−1

b

(cid:16)

(cid:17)

b
p

(mod p)

b
p

is the Legendre symbol. The following theorem tells us that this necessary
where
property is also sufﬁcient (when considering the Jacobi symbol). The Solovay–Strassen
primality test is based on it.

Theorem 7.4. Let n be an odd number. n is prime if and only if for any b such that
0 < b < n we have b n−1

2 ≡(cid:12)
is the Jacobi symbol which is deﬁned as follows. Let n = pα1

··· pαr
We recall that
r
be the factorization on n into pairwise different odd prime numbers p1, . . . , pr . We
deﬁne

(mod n).

(cid:13)

(cid:12)

(cid:13)

b
n

b
n

1

(cid:16)

(cid:17)

b
n

(cid:16)

=

b
p1

(cid:16)

(cid:17)α1 ···

(cid:17)αr

b
pr

(cid:18)

(cid:19)

b
pi

where

is the Legendre symbol deﬁned by

(cid:16)

(cid:17)

=

b
pi

⎧⎨
⎩ 0
1
−1

if b mod pi = 0
if b is a quadratic residue in Z
∗
if b is not a quadratic residue in Z
pi

∗
pi

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

It is important to emphasize that the Jacobi symbol can be efﬁciently computed. This
can be done using an algorithm which is very similar to the Euclid algorithm. We
actually use the following properties:

(cid:13)
(cid:13)

(cid:13) =(cid:12)
(cid:13) =(cid:12)
(cid:13) = 1 if a ≡ ±1 (mod 8) and
(cid:13) = −(cid:12)

for b odd,
for c odd,

(cid:13)(cid:12)
(cid:13)

a
c

b
c

b

(cid:12)

a mod b

1.
2.
3.
4.

a
b
ab
c
2
a
a
b

(cid:13) = −1 if a ≡ ±3 (mod 8) for a odd,
(cid:12)

(cid:13) =(cid:12)
otherwise for a and b odd.
As an example, we can compute both terms of the test for b = 362 and n = 561
2 mod n = 362280 mod 561. We have
(a Carmichael number). First we compute b n−1

if a ≡ b ≡ 3 (mod 4) and

(cid:13)

2
a

b
a

b
a

a
b

2 For more information we recommend Ref. [53].

Algorithmic Number Theory
= 280 = 23 × 35; thus
n−1
2

185

(cid:17)35
2 mod n = 36223×35 mod 561
b n−1

mod 561

=

=

3622

(cid:16)(cid:18)(cid:12)
(cid:18)(cid:12)
= (cid:12)
= 67 ×(cid:12)
= 6735 mod 561

(cid:19)2
(cid:13)2
(cid:19)35
(cid:13)2
(cid:13)35 mod 561
(cid:13)17 mod 561

mod 561

1662

3312

672

= 67 × 117 mod 561
= 67.

Here we do not even have to compute the Jacobi symbol since 67 is neither 1 nor −1.
We could have computed it without any factorization as follows.

(factor 2 isolation) =
(multiplicativity) =
(561 ≡ 1 (mod 8)) =
(quadratic reciprocity) =
(modular reduction) =
(factor 2 isolation) =
(multiplicativity) =

(181 ≡ −3 (mod 8)) = −
(quadratic reciprocity) = −
(modular reduction) = −
= −1

(cid:17)

(cid:16)

(cid:17)

b
n

=

(cid:17)
(cid:16)

(cid:17)

181
561

(cid:16)

(cid:17)

9
181

(cid:16)
(cid:16)
(cid:16)
(cid:16)
(cid:16)
(cid:16)
(cid:16)
(cid:16)

×

362
561
2 × 181
(cid:17)
561
(cid:17)
2
561
(cid:17)
181
561
(cid:17)
561
181
18
181
2 × 9
(cid:17)
181
2
181
9
181
181
9

(cid:16)
(cid:16)
(cid:16)

×
(cid:17)
(cid:17)

(cid:17)

(cid:17)

1
9

186

Chapter 7

Parameter: k, an integer
Input: n, an integer of (cid:9) bits
Output: notiﬁcation of non-primality or pseudo-primality
Complexity: O(k(cid:9)3)
1: if n = 2 then

output “prime” and stop

output “composite” and stop

2:
3: end if
4: if n is even then
5:
6: end if
7: repeat
pick a random b such that 0 < b < n
8:
if b n−1
9:
10:
11:
12: until k iterations are made
13: output “pseudo-prime” and stop

output “composite” and stop

(mod n) then

(cid:4)≡(cid:12)

end if

(cid:13)

b
n

2

Figure 7.2. The Solovay–Strassen Primality Test.

We can also verify the deﬁnition with the factorization of n by

2 mod 3 = 2, 362 11−1

2 mod 11 = 10, 362 17−1

and 362 2−1
(−1)3 = −1.

Instead of proving the previous theorem, we will prove an actually much stronger

theorem which is stated as follows.

Theorem 7.5 (Solovay–Strassen 1977 [174]). Let n be an odd integer. If n is prime,
for any b ∈ Z
(mod n). Conversely, if n is composite, this equality
holds for no more than half of all the possible b values.

∗
n we have b n−1

b
n

2 ≡(cid:12)

(cid:13)

This shows that when n is prime, the primality test in Fig. 7.2 always answers “prime”
or “pseudo-prime”; otherwise, it answers “composite” with probability greater than
1 − 2

−k, k being the maximum number of times the test iterates.

Proof. To prove the theorem, we ﬁrst let n be an odd prime number. The Jacobi symbol
∗
reduces to the Legendre symbol, and all b such that 0 < b < n are in Z
n. We thus only

(cid:16)

(cid:17)

b
n

(cid:16)
(cid:16)
(cid:16)

=

=

=

(cid:17)

362

(cid:17)

362
561
(cid:16)
3 × 11 × 17
362
362
3
11

(cid:17)

×

(cid:16)

(cid:17)

×

(cid:17)

362
17
2 mod 17 = 16,

(cid:13) =

(cid:12)

b
n

thus

Algorithmic Number Theory

have to prove that b is a quadratic residue if and only if b n−1
did it for Theorem 6.6.

187
2 ≡ 1 (mod n). We already

The converse is a little more technical.

We assume that the test holds for more than half of the b is in Z

∗
n. We notice that
∗
the set of all b for which the test holds is actually a subgroup of Z
n. (The test predicate
is stable by multiplication.) Therefore the probability that the test holds for a random
b is one over the index of the subgroup. If the probability is greater than 1
2 , this means
that the subgroup is Z

∗
n itself. Therefore the test must hold for all b.

By squaring the test predicate, we notice that this implies bn−1 ≡ 1 (mod n) for all
b ∈ Z
∗
n. Thus n is either prime or a Carmichael number. All we have to do is show that
the test predicate cannot hold with Carmichael numbers.

(cid:18)

(cid:19)

∗
p1

We already know that a Carmichael number is a product of pairwise different
prime numbers. Let n = p1 ··· pr be the factorization of n into primes. Seeing Z
∗
n
× ··· × Z
pr , we can consider all the bi =
∗
like Z
as independent bits indicating
pi element) a quadratic residue (+1) or not (−1). Obviously,
∗
whether b is (as a Z
is the
product of all these independent bits. Due to Theorem 7.1, we know that pi − 1 divides
n − 1. If pi − 1 divides n−1
2 ≡ bi (mod pi ).
2 ≡ 1 (mod pi ). Otherwise, b n−1
2 , then b n−1
2 ≡ b
= ±1 such that b n−1
(cid:8)
(cid:8)
We deﬁne b
i (mod pi ). We know that depending on i only,
(cid:8)
(cid:8)
i
i is equal to bi or to 1. Hence b
i can be written fi (bi ). Thus for all i we have
b

b
pi

(cid:12)

(cid:13)

b
n

fi (bi ) ≡ r(cid:27)

j=1

b j (mod pi ).

But both sides are +1 or −1 and pi is an odd prime. Therefore this must be an equality.
Thus for any b we have

f1(b1) = ··· = fr (br ) = r(cid:27)

b j .

j=1

Clearly this cannot happen unless r = 1: once we have at least two factors, we know
that the bi ’s are totally independent, so we can just ﬁx bi for i > 1 and make b1 change,
and we obtain that f2(b2) must depend on b1, which is a contradiction. Hence r = 1,
(cid:1)
which means that n is a prime number.

7.1.4 Miller-Rabin Test

We conclude this section with the Miller-Rabin test which generalizes the Fermat test
and the Solovay–Strassen test. It is depicted in Fig. 7.3. We can prove that it always

188

Chapter 7

Parameter: k, an integer
Input: n, an integer of (cid:9) bits
Output: notiﬁcation of non-primality or pseudo-primality
Complexity: O(k(cid:9)3)
1: if n = 2 then output “prime” and stop
2: if n is even then output “composite” and stop
3: write n = 2st + 1
4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: until k iterations are made
16: output “pseudo-prime” and stop

pick a random b such that 0 < b < n
x ← bt mod n, i ← 0
if x (cid:4)= 1 then
while x (cid:4)= n − 1 do

x ← x 2 mod n, i ← i + 1
if i = s or x = 1 then

output “composite” and stop

end if

end while

end if

Figure 7.3. The Miller-Rabin Primality Test.

detects prime numbers, and detects composite ones with a probability greater than
1 − 4

−k.
The Miller-Rabin test consists of successively dividing n − 1 by 2 until the result
becomes some odd t. Then, we raise b to the power of t, and square it. If we eventually
ﬁnd 1 (as in the Fermat test), we check that the previous value was −1. If it was not,
we have found a square root of 1 which is neither +1 nor −1, which is a proof of
compositeness (see Fig. 7.4).

Theorem 7.6 (Miller-Rabin 1976, 1980 [134, 154]). Let n be an integer, and let us
consider the algorithm in Fig. 7.3 with input n and parameter k. If n is prime, then the
algorithm always outputs “prime” or “pseudo-prime.” Conversely, if n is composite,
the algorithm outputs “composite” with probability greater than 1 − 4

−k.

This result is proven in Section 7.1.5.

(cid:7)
-(cid:10)= 1

bt mod n

-(cid:10)= 1

SQ

-(cid:10)= 1

SQ

at most s

(cid:8)(cid:9)
···

-(cid:10)= 1

SQ

(cid:10)

SQ

- 1

-(cid:10)= 1
6
?

is it ≡ −1?

Figure 7.4. The Miller-Rabin test.

Algorithmic Number Theory

189

7.1.5

(cid:1)Analysis of the Miller-Rabin Test

Proof. Since the case of even integers is trivial we assume that n is odd and we write
n = 2st + 1.

n. We know that b2s t ≡
∗
When n is prime, any b such that 0 < b < n is in Z
1 (mod n). Since Zn is a ﬁeld, we know that the only square roots of 1 are +1 and
−1 mod n. Hence either bt ≡ 1 (mod n) or there exists i such that 0 ≤ i < s and
b2i t ≡ −1 (mod n). Therefore none of the k iterations can lead to “composite.”

For a composite n, the result is quite technical to prove. We simply mention the

following two results.

r If n passes the Miller-Rabin test for b, then n passes the Solovay–Strassen test
r One can prove that the probability that the test passes with b ∈ Z

for b, and so the Miller-Rabin test is at least as efﬁcient as the Solovay–Strassen
test.

∗
n is less than

1
2 .

We simply prove the second result. We refer to Ref. [102] for more details. One can
∗
easily notice that the set G of b’s in Z
n for which the test passes is a subgroup. If we have
n, then the result holds since #G is a factor of ϕ(n). Otherwise, n = p1 ··· pr
G (cid:4)= Z
∗
is a Carmichael number. Let i j be the smallest i such that p j−1
divides 2i t. We know
that for any b ∈ Z
n we have b2i j +1t mod p j = 1 and b2i j t mod p j = 1 if and only if b
∗
is a quadratic residue modulo p j . Let j be the index for which i j is the greatest and
let k be the index different from j for which i j is the greatest. We know that for any
b ∈ Z
n we have b2i j +1t mod n = 1. If j > k, for any quadratic nonresidue b modulo p j ,
∗
b2i j t mod n is neither +1 nor n − 1 (which is −1 modulo n) and so the test cannot pass.
If j = k, for any b with different quadratic residuosity modulo p j and pk, b2i j t mod n
is neither +1 nor n − 1 and so the test cannot pass. In both cases, we cannot have
G = Z
∗
(cid:1)
n.

2

7.1.6 Prime Number Generation

It should be noticed that prime numbers can be randomly generated with ease: we can
iteratively pick a random number until one primality test responds positively. Due to
the following Prime Number Theorem, we know that we will eventually end up with a
prime number of n bits within O(n) trials.

Theorem 7.7 (Prime Number Theorem). Let p(N ) denote the number of prime num-
bers in {2, 3, . . . , N}. We have p(N ) ∼ N

log N when N increases toward inﬁnity.

Thus the number of prime numbers of at most (cid:9) bits is asymptotically equal to
which means that the probability that a random (cid:9)-bit number is prime is (1/(cid:9)).

2(cid:9)
(cid:9) log 2 ,

190

Chapter 7

(cid:12)

(cid:13)c, which is
We pick a random (cid:9)-bit n until it passes a primality test with k iterations. The
probability that we do not ﬁnd a prime number after c trials is
arbitrarily small with c = ((cid:9)). The probability pw that this algorithm lets a composite
−k. Therefore we can pick k = (log (cid:9)). We
number pass after c trials is lesser than c4
notice that the k factor vanishes in the complexity O(c(cid:9)3) because k is negligible with
respect to c and most composite numbers are ruled out by the very ﬁrst iteration.

1 − ( 1
(cid:9) )

This algorithm will succeed after c = O((cid:9)) trials, which leads to a complexity of
O((cid:9)4). In practice, we even make sure that the randomly picked (cid:9)-bit number n has no
small prime factors by construction (e.g. we do not pick even numbers). Hence, the
running time is indeed smaller in practice.

7.2 (cid:1)Factorization

As we saw in Section 7.1, it is easy to recognize prime numbers, and therefore, composite
numbers as well. Given a composite number n, it is easy to get a “proof of composite-
ness” (for instance by exhibiting a number b such that 0 < b < n and bn−1 mod n (cid:4)= 1).
Here “easy” means within a time polynomial in the size of n (namely log n). It is how-
ever quite hard to get a nontrivial factor of n in general: no polynomial algorithms (in
terms of log n) are known for that.

√
The ﬁrst algorithm we think of is based on the trial division algorithm depicted in
Fig. 7.1: we try to divide n by all integers i from 2 to
n until a factor is found. This
algorithm will pull a factor out of n within a complexity of O( p) arithmetic operations
where p is the smallest prime factor of n.3

In this section we list a few exponential algorithms which have a better complexity.

7.2.1

(cid:1)Pollard Rho Method

√
Pollard Rho algorithm (named after the Greek ρ character) lowers the complexity of
trial division from O( p) down to O(
p) where p is the smallest prime factor of n (see
Ref. [149]). The basic idea is the following.

r We take a “random” function f which can be “factorized” by a mod p function
f (x) ≡ f (x mod p) (mod p) for any
for any factor p of n: namely such that
factor p of n. For example we can take any polynomial function. In practice
one always uses f (x) = x 2 + 1 which behaves “like a random function” from a
heuristic viewpoint.

3 Note that we should consider the complexity of arithmetic operations. But the overhead is negligible in
comparison to p since it is polynomial in log n. We will omit it for simplicity, but recall that the complexity
unit is an “arithmetic operation” and not an elementary binary operation.

Algorithmic Number Theory

= f (x

(cid:8)
i ) mod p with x

(cid:8)
0

191

r Since Z p is a ﬁnite set, we expect that the x

r We take a random x0, and iterate xi+1 = f (xi ) mod n. Due to the property
of the f function, this modulo n computation actually “hides” the modulo p
= x0, we obtain that
(cid:8)
i+1
computation: if we deﬁne x
= xi mod p for any i > 0.
(cid:8)
x
i

(cid:8)
i sequence will at one point enter
into a loop. The shape of the sequence will then look like the Greek character
Rho (ρ): with a tail and a loop. If f is random, there is no reason for this loop
modulo p to correspond to a loop modulo n: for instance, unless n is not a
power of p, there exists another prime factor q, and we know from the Chinese
Remainder Theorem that the modulo p and modulo q computations are totally
independent. So a loop modulo p will rarely synchronize with a loop modulo
q.

r This loop is easy to detect by gcd computations. Furthermore, random mappings
√
analysis shows that we can expect the total length of the Rho (the tail and the
loop) to be O(

p). This is similar to the birthday paradox effect.

In order to detect a loop, we can just iterate the following process:
1: a ← x0, b ← x0
2: while a (cid:4)= b do
a ← f (a)
b ← f ( f (b))

3:
4:
5: end while

The a register contains xt and the b register contains x2t in the t-th iteration where
x0, x1, . . . is the sequence obtained by iterating f on x0. Eventually we will ﬁnd xt = x2t
when t is greater than the tail length and a multiple of the loop length.

In order to do the same with a “hidden computation” in Z p, we notice that we only
have to check equalities. If computations are performed in Zn, we can check hidden
equalities in Z p between a and b by computing the gcd of a − b and n. In case of
equality, the gcd will be a multiple of p. Hence the algorithm in Fig. 7.5 eventually
stops after a number of iterations which is a big O of the rho length.

Input: n, an integer with a prime factor smaller than B
√
Output: a nontrivial factor of n
Complexity: O(
1: x0 ← random, a ← x0, b ← x0
2: repeat
3:
4:

a ← f (a) mod n
b ← f ( f (b) mod n) mod n

B) arithmetic operations

5: until gcd(a − b, n) (cid:4)= 1
6: output the gcd

Figure 7.5. Pollard Rho Factorization Algorithm.

192

Chapter 7

The ﬁrst key argument is now to realize that equalities in Z p are independent from
equalities in Zn, which means that the case of full equality in Zn is negligible. This
means that the gcd will be a nontrivial factor of n which contains p.

√
The second key argument is the complexity analysis: we can prove that if x0 and
f are random, then the rho length is O(
√
p). Hence the algorithm works within a
complexity of O(
p) arithmetic operations where p is the smallest factor of n. In
numbers, we have B ≤ √
Fig. 7.5 the complexity in terms of an upper bound B on p is given. For all composite

√
n so we factorize a general integer n in O( 4
n).

We use the following result for the complexity analysis.

element. We iteratively deﬁne xt+1 = f (xt ) and let t = 1 + (cid:28)√

Lemma 7.8. Let f be a random function in a set of p elements, and let x0 be a random
2λp(cid:29) for a given real λ.

The probability that x0, x1, . . . , xt are pairwise different is less than e

−λ.

Proof. Since f is random, this probability is

(cid:16)

p = t(cid:27)

i=1

1 − i
p

(cid:17)

.

Since log(1 − x) < −x for 0 < x < 1, log p is lesser than − t(t+1)
this is smaller than −λ.

2λp,
(cid:1)
As an example, we try to factorize n = 18923. We let f (x) = x 2 + 1 and x0 =
2347. The iterations yield the data given in Fig. 7.6 and ends by pulling out the factor
127. Then we can see that, indeed, 18923 = 127 × 149.

2 p . Since t ≥ √

7.2.2

(cid:1)Pollard p − 1 Method

The Pollard Rho algorithm works well for numbers n featuring a relatively small factor.
We can ﬁnd another method when n has a prime factor p such that all prime factors of

t = 1
t = 2
t = 3
t = 4
t = 5
t = 6
t = 7
t = 8
t = 9
t = 10

a = 1817
a = 8888
a = 11943
a = 12599
a = 8678
a = 13068
a = 11473
a = 1342
a = 3280
a = 10137

b = 8888
b = 12599
b = 13068
b = 1342
b = 10137
b = 7978
b = 8232
b = 16487
b = 11407
b = 11280

gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 127

Figure 7.6. Example of Pollard Rho factorization.

Algorithmic Number Theory

193

Input: n
Output: a nontrivial factor of n
Complexity: O(B) arithmetic operations
1: pick x at random in {2, . . . , n − 1}
2: if gcd(x, n) (cid:4)= 1 then

output this gcd and stop

3:
4: end if
5: i ← 1
6: while gcd(x − 1, n) = 1 do

x ← x i mod n
7:
i ← i + 1
8:
9: end while
10: if x = 1 then

fail
output gcd(x − 1, n) and stop

11:
12: else
13:
14: end if
Figure 7.7. Pollard p − 1 Factorization Algorithm.

p − 1 are small. We call it the p − 1 method (see Ref. [148]). When all prime factors
of p − 1 are smaller than B, we say that p − 1 is B-smooth.

Concretely, we assume that there exists some threshold B such that there exist two
prime factors p and q of n such that p − 1 is B-smooth and q − 1 is not B-smooth.
√
n) works.) Fig. 7.7 shows the Pollard p − 1 algorithm.
(In most examples, B = O( 4
It simply consists of computing x B! mod n. The modulo p part of this number will
be congruent to 1 since p − 1 is a factor of B!, and therefore (x B! mod n) − 1 has a
common factor p with n which can be found by a gcd computation.

1

··· pαr

Conversely, if n contains a B-smooth factor p, then for some “B-small” t the
t! integer is a multiple of p − 1, hence x t! ≡ 1 (mod p), which can be detected by
computing the gcd of (x t! mod n) − 1 and n as in the rho method. More precisely, if
is the prime factorization of p − 1, we take αi pi = max j α j p j .
p − 1 = pα1
For t ≥ αi pi , we know that for all j we have at least α j multiples of p j which are
j divides t! for all j, so p − 1 divides t!. We notice that αi pi lies
lesser than t, so p
between B and B log2 p since αi < log2 p. In typical cases, α j ’s corresponding to large
p j ’s are all equal to 1, so αi pi is actually the largest prime factor of p − 1 which is
approximated by B. So t only needs to be slightly larger than B to ensure the property.
This is what we meant by “B-small.”

r

α j

It is a bit more tricky to know when the equality modulo p does not imply a full
equality modulo n in order to make the algorithm succeed. This is quite easy when
n has a prime factor p such that p − 1 is B-smooth and a prime factor q such that
q − 1 contains a large prime factor r: when t becomes slightly greater than B, p − 1
is a factor of t!, but r is not, so unless the multiplicative order of the original x is zero

194
Chapter 7
modulo r (which is the case for a negligible fraction of x’s), x t! − 1 mod n will be a
multiple of p but not a multiple of q, so the gcd is nontrivial. More precisely, this will
always work for integers n such that

r there exists a prime factor p of n such that all prime factors of p − 1 are lesser
r there exists a prime factor q of n such that one prime factor r of q − 1 is larger

than B,

than B log2 n.

In other cases, the algorithm may never work. For instance, when all prime factors p
of n are such that the largest prime factor r of p − 1 is the same, x t! will certainly
simultaneously vanish modulo all prime factors of n. For instance, for n = 133 ×
331 = 44023, p = 133 is such that p − 1 = 22 × 3 × 11 and q = 331 is such that
q − 1 = 2 × 3 × 5 × 11. It is easy to see that the smallest factorial number which is
a multiple of p − 1 is 11!, just like for q − 1. This means that for any initial x whose
∗
multiplicative orders in Z
q are both multiple of 11 (which is the case for most
of the original x), x t! will simultaneously vanish modulo p and modulo q, which gives
us no chance to split n with this algorithm. These cases are however quite pathological.
We give the same example as for the rho method: n = 18923. (Note that p = 127
and q = 149 are such that p − 1 = 2 × 32 × 7 and q − 1 = 22 × 37, so r = 37 is
large, and p − 1 is B-smooth for B = 7.) We pick x = 2347. We obtain

∗
p and Z

i = 1
i = 2
i = 3
i = 4
i = 5
i = 6
i = 7

x = 2347
x = 1816
x = 4072
x = 14891
x = 18431
x = 7247
x = 13590

gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 1
gcd = 127

which yields p.

(cid:1)The Elliptic Curves Method (ECM)

7.2.3
The Pollard p − 1 algorithm consists of making hidden computations in a Z
∗
p group
∗
of smooth order. Computations in this group are hidden by computations in Z
n. We
can generalize this algorithm by using other groups, following an idea due to Hen-
drik Lenstra (see Ref. [114]). For instance, by picking a, b ∈ Zn, we can consider the
elliptic curve Ea,b in Z p by doing all computations modulo n since the modulo p
computations are just “hidden” in the modulo n ones. The order of this group is a
random number within the p + 1 ± 2
p range. If this order happens to be B-smooth,
we can apply exactly the same algorithm within a complexity of O(B) arithmetic op-
erations. The success of this approach thus corresponds to the probability that the
order of the Ea,b mod p group is B-smooth for random a and b, which is estimated to

√

Algorithmic Number Theory

195

be u

of O(cid:12)

−u where u = log p
√
(1+o(1)) log n log log n
e

(cid:13)

log B . Heuristic analysis show that this algorithm has a complexity
, which is asymptotically smaller than all the factorization

complexities that we have seen so far.

√
The Pollard p − 1 algorithm may not work for B = 4
n. In practice, the elliptic
curve version will work, because the probabilistic structure of the used group will hide
any ill-designed properties of n.

As an example, we have seen that n = 44023 could not be factorized with the
p − 1 method because 44023 = 133 × 331 and 133 − 1 and 331 − 1 are factors of
exactly the same factorial numbers. We can thus try to factorize n with the ECM
method.

Let us pick a random a and b. We consider Ea,b mod n:

y2 ≡ x 3 + ax + b

(mod n).

The ﬁrst problem is to pick a random point X on this curve. It is not quite trivial because
we must solve an algebraic equation modulo n whose factorization is assumed to be
unknown. So, instead of picking a, b, then X, we pick a, X, then b.

Let us pick a = 13 and X = (x, y) = (23482, 9274). We thus have

b = y2 − x 3 − ax mod n = 21375.

We now have to compute Xi = i! · X = (xi , yi ) for i = 1, 2, . . . until we notice that the
“hidden point” Xi mod p in Ea,b mod p is the inﬁnity point while Xi is not. This nec-
essarily comes from an illegal operation, like a division by a noninvertible element. But
∗
note that noninvertible elements in Z
n yield a nontrivial factor of n by gcd computation,
which is exactly what we are looking for.

Computing X1 is easy: X1 = X. To compute X2, we need to compute X2 = 2 · X1.

For this we compute

+ a
λ = 3x 2
1
2y1

mod n = 31095

and

x2 = λ2 − 2x1 mod n = 18935

y2 = λ(x1 − x2) − y1 mod n = 21838.

Next we compute X3 = 3 · X2 = (2 · X2) + X2. We compute 2 · X2 by

+ a
λ = 3x 2
2
2y2

mod n = 41645

196

and

Chapter 7

x = λ2 − 2x2 mod n = 26093

y = λ(x2 − x) − y2 mod n = 7008

and (x, y) + X2 by

and

λ = y2 − y
x2 − x

mod n = 5816

x3 = λ2 − x − x2 mod n = 15187

y3 = λ(x − x3) − y mod n = 29168.

Next we compute X4 = 4 · X3 = 2 · (2 · X3). We obtain
X1 = (23482, 9274)
X2 = (18935, 21838)
X3 = (15187, 29168)
X4 = (10532, 5412)

i = 1
i = 2
i = 3
i = 4

In order to compute X5 = 5 · X4 = (2 · (2 · X4)) + X4, we ﬁrst compute

then

2 · X4 = (30373, 40140)

2 · (2 · X4) = (27556, 42335)

but when we add up this point and X4, we need to compute

λ = 42335 − 5412
27556 − 10532

mod n

but the denominator 27556 − 10532 = 17024 is not invertible modulo n. When trying
to invert it with the Euclid algorithm, we end up with the gcd between 17024 and n
which is 133, indeed a factor of n.

This kind of algorithm illustrates how computation in an ill-designed structure
(namely an elliptic curve over a structure Zn which is not a ﬁeld) can be performed
until we stumble upon a computation error which yields an interesting result, here the
factorization of n.

(cid:1)Fermat Factorization and Factor Bases

7.2.4
The Fermat factorization algorithm is used to factorize n = pq where p ≈ q. The
purpose is to ﬁnd a representation of n as the difference of two perfect squares

Algorithmic Number Theory
n = t 2 − s2. Obviously, t = p+q
is one such solution, and we notice
that s is small when p ≈ q. Conversely, if n = t 2 − s2, we have n = (t + s)(t − s),
which factorizes n unless t = s + 1 = n+1
2 . But in this case, s is not small at all
since s ≈ n
from (cid:25)√
2 . The algorithm works as follows: we try all possible t values starting
n(cid:26) until t 2 − n is a perfect square s2. The complexity is O( p − q) arithmetic

and s = p−q

197

2

2

operations.

The Fermat method is important because of the following observation: if we happen
to ﬁnd s and t such that s2 ≡ t 2 (mod n) and s (cid:4)≡ ±t (mod n), then gcd(s − t, n) is a
nontrivial factor of n: we notice that (s − t)(s + t) is a multiple of n. So if gcd(s −
t, n) = 1, it means that s + t is a multiple of n in which case we have s ≡ −t (mod n),
but this is not the case. If gcd(s − t, n) = n, we have s ≡ t (mod n), but this is not the
case either. Hence, gcd(s − t, n) is a proper factor of n. So we can try to factorize n by
looking for nontrivial s2 ≡ t 2 (mod n) relations.

··· pαi,r

Modern methods use factor bases. A factor base is a set B of numbers p1, . . . , pr .
We say that a number b is a B-number if it can be factorized into a product of numbers
which are all in B. This factorization is assumed to be easy to perform. The goal of
factor base methods is to get several B-numbers a2 mod n. We obtain several relations
i mod n = pαi,1
a2
. Once we get a little more than r equations, we consider the
vectors !Ai = (αi,1, . . . , αi,r ). Since we have a little more than r vectors of r coordinates,
(cid:13)2 is congruent to a
(cid:26)
they must be linearly dependent modulo 2. Hence we obtain a linear combination
perfect square B-number. This relationship is likely to be a nontrivial s2 ≡ t 2 (mod n)
relation which leads to a nontrivial factor of n. It is important to emphasize the three
phases of this method:

!Ai , the coefﬁcients of which are all even. Therefore

(cid:12)/

i∈I ai

i∈I

1

r

≡ pαi,1

··· pαi,r

1. get several a2
i
2. solve a linear system in GF(2) in order to get an even linear combination of
3. from the corresponding s2 ≡ t 2 (mod n) relation, get a nontrivial factor of n.

(αi,1, . . . , αi,r ) vectors,

(mod n) relations,

1

r

The way the ﬁrst step is done highly depends on the factorization method.

7.2.5

(cid:1)The Quadratic Sieve

√
The Quadratic sieve is a factor base method. Let m be the integer part of
n. We pick
at random A = X + m, where X is a small integer. We notice that A2 − n = X 2 +
2m X + m2 − n. When X << m, the most important term in this sum is 2m X which is
√
n. So this is relatively small when compared to n. In particular we
approximately 2X
may have A2 mod n = A2 − n. Assuming that A2 − n is b-smooth (namely all prime
factors p are lesser than b), we can factorize A2 − n with the factor base which consists
of−1 and all prime numbers lesser than b. (−1 is necessary in order to factorize negative
numbers.)

198

Chapter 7
We notice that if p is prime and divides A2 − n, then A2 ≡ n (mod p), therefore
p ) = 1 when p is odd. Therefore we can
n is a quadratic residue modulo p, hence ( n
just pick B equal to −1, 2, and all prime numbers p lesser than b and for which
p ) = 1.
( n

−u where u = log n

We have already mentioned that a random integer within the order of magnitude of n
is b-smooth with probability u
log b . So by picking a totally random A mod
−u. The trick here (due to Carl Pomerance)
n, we obtain a relation with probability u
consists of increasing this probability by decreasing the order of magnitude of A mod n
by picking A = X + m. A mod n is now within an order of magnitude of
n, so u is
decreased by one half.

√

(cid:18)

√
(1+o(1)) log n log log n

(cid:19)

when

We can heuristically prove that the complexity is O

e

we carefully choose b, as for the ECM method (see Ref. [152]).

As an example, we factorize n = 527773. We let m = 726, the nearest integer to

the square root of n. The list of prime numbers of our factor basis is

2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67.

But n is a quadratic residue modulo

3, 7, 11, 13, 17, 41, 53, 61, 67

only. Therefore we set B = {−1, 2, 3, 7, 11, 13, 17, 41, 53, 61, 67}. We now try A =
X + m with

X = 0,+1,−1,+2,−2,+3,−3, . . .

and we test which A2 − n values are b-smooth. We have

m2 − n = −1 · 17 · 41
(m + 1)2 − n = 22 · 33 · 7
(m − 1)2 − n = −1 · 22 · 3 · 179
(m + 2)2 − n = 3 · 11 · 67
(m − 2)2 − n = −1 · 3 · 11 · 109

...

(m + 5)2 − n = 22 · 33 · 61

...

(m + 7)2 − n = 22 · 3 · 13 · 61
(m − 7)2 − n = −1 · 22 · 3 · 17 · 53

Algorithmic Number Theory

199

...

(m + 10)2 − n = 32 · 7 · 13 · 17
(m − 13)2 − n = −1 · 22 · 32 · 72 · 11
(m + 17)2 − n = 22 · 3 · 7 · 172
(m − 17)2 − n = −1 · 22 · 32 · 17 · 41
(m + 20)2 − n = 3 · 11 · 13 · 67
(m + 24)2 − n = 7 · 112 · 41
(m − 24)2 − n = −1 · 112 · 172

and so we obtain the following vectors

X

0
1
2
5
7
−7
10
−13
17
−17
20
24
−24

m + X

(m + X)2 − n

726
727
728
731
733
719
736
713
743
709
746
750
702

−697
756
2211
6588
9516
−10812
13923
−19404
24276
−25092
28743
34727
−34969

Factors
−1 · 17 · 41
22 · 33 · 7
3 · 11 · 67
22 · 33 · 61
22 · 3 · 13 · 61
32 · 7 · 13 · 17
22 · 3 · 7 · 172
3 · 11 · 13 · 67
7 · 112 · 41
−1 · 112 · 172

−1 · 22 · 3 · 17 · 53
−1 · 22 · 32 · 72 · 11
−1 · 22 · 32 · 17 · 41

Vector

(1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0)
(0, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0)
(0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1)
(0, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0)
(0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0)
(1, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0)
(0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0)
(1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0)
(0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 0)
(1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 0)
(0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1)
(0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0)
(1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0)

We can now reduce the vectors modulo 2 and try to ﬁnd linear dependencies. Here,
we can simply notice that the vectors for X = 1 and for X = 17 coincide modulo 2.
Indeed, we have

7272 · 743

−2 ≡ 32 · 17

−2

(mod n).
−1 mod n = 186273, we

3 · 17

727 · 743

−1 mod n = 223754

have
Since
2237542 ≡ 1862732 mod n and we notice that 223754 − 186273 = 37481 and
gcd(37481, n) = 1013, which is a nontrivial factor of n. This leads us to the
factorization n = 521 · 1013.

and

7.2.6

(cid:1)Factorization Nowadays

The previous methods have been extended to give birth to the more sophisticated
number ﬁeld sieve method (NFS), due to Hendrik Lenstra and Arjen Lenstra, which is
quite beyond the scope of this book (see Ref. [115]). We simply mention that the NFS

200

complexity is

(cid:18)

O

e

(log n)

1
3 (log log n)

2
3

(cid:19)

.

Chapter 7

This method becomes substantially better than the previous ones as n increases. (In
practice, it becomes better when n reaches 100 to 150 decimal digits.)

As we will see in the next chapters, the ability to factorize large numbers enables
to break some cryptosystems. To monitor public progress in this area, factorization
challenges were issued by the company RSA Data Security. Every challenge is a large
number which is given a tag featuring its length. For instance, RSA155 is the following
155-decimal digits integer.
RSA155 = 10941738641570527421809707322040357612003732945449
20599091384213147634998428893478471799725789126733
2497625752899781833797076537244027146743531593354333897.

This number was factorized on August 22, 1999 by a team of scientists from six different
countries, led by Herman te Riele of CWI (Amsterdam). They implemented a network
of computers that provided about 8000 mips.year (8000 millions of instructions per
second during one year), to discover that RSA155 is equal to

102639592829741105772054196573991675900716567808038066803341933521790711307779×
106603488380168454820927220360012878679207958575989291522270608237193062808643

which is a product of two prime numbers. A lot of efforts for a two-line conclusion!

7.2.7

(cid:1)Factorization Tomorrow

At this time, researchers are investigating a new computing model which no longer relies
on traditional microprocessors and memories. Instead, it relies on quantum mechanics.
This model or quantum computing assumes that the state of a computing machine
can be so isolated from an electromagnetic viewpoint that it can be in a quantum
state. Namely, instead of being on a well-deﬁned state, it can be on a superposition of
(inﬁnitely) many states associated with a complex weight. The probability that freezing
the machine ends up on a given state is the squared norm of the complex weight. Since
most people have been educated with well-deﬁned deterministic state machines, this
model is clearly against the common intuition. Nevertheless, some experiments show
that such machines can be built, but the current state of the art limit them to a dozen
of quantum bits of memory. The question whether this model can scale or not is still
open today since the more quantum bits we have, the less stable states are.

Algorithmic Number Theory

201

One advantage of this computing model is the notion of free massive parallelism.
With a single quantum bit of memory we can basically do 2n computations in parallel
for any n. One problem is that we cannot collect all results. With some special cases,
we can make the results interact (for instance through a discrete Fourier transform)
to derive a single result. This case is very well suited to the problem of factoriza-
tion. Indeed, the Shor algorithm can factorize any integer N within a complexity of
O((log N )2(log log N )(log log log N )) on a quantum computer. The only remaining problem
is to construct a machine with log N quantum bits of memory.4

7.3 Computing Orders in Groups

7.3.1 Finding the Group Exponent

As explained in Section 7.1.2, we recall that the exponent of a group is the smallest
nonnegative integer λ such that x λ = 1 for all x in the group (assuming that the group
is multiplicatively denoted). For cyclic groups, the exponent is obviously the order of
the group. For instance in Zn (which is additively denoted), the exponent is n since n
is the smallest x such that x.1 ≡ 0 (mod n) and we have n.x ≡ 0 (mod n) for any x ∈ Zn.

In the case of Z
··· pαr

that if n = pα1

r

1

∗
n, λ is denoted λ(n) as the Carmichael function. We can easily prove
is the factorization of n, then
λ(n) = lcm

, . . . , ( pr − 1) pαr−1

( p1 − 1) pα1−1

(cid:18)

(cid:19)

r

1

which should be compared to

1

.

r

··· ( pr − 1) pαr−1

ϕ(n) = ( p1 − 1) pα1−1
∗
Finding the exponent of Z
n is not easy. It is actually as hard as factoring n: obviously,
the factorization of n allows to compute λ(n) by the above formula. The opposite is a
little more subtle. Let us assume that we can compute λ = λ(n) and let us factorize n.
Let us ﬁrst take an example with n = pq with p and q different primes such that p ≡
q ≡ 3 (mod 4). This way we have λ = 2.lcm
and the lcm is odd. We know that
2 mod q = 1
x p−1
2 − 1
if and only if x is a quadratic residue modulo q. Hence the two reductions of x λ
modulo p and modulo q are equal with probability 1
2 , namely if the quadratic residuosity
is the same for both modulo p and modulo q. This means that we can ﬁnd factors of n
2 − 1 mod n) with a random x.
by computing gcd(n, x λ

2 mod p = 1 if and only if x is a quadratic residue modulo p, and that x q−1

, q−1

p−1
2

(cid:18)

(cid:19)

2

More generally, let us assume without loss of generality that n is odd (we can
get rid of even factors by using the Chinese Remainder Theorem) and has at least

4 For more information, see Ref. [140].

202

Chapter 7

(cid:8) with k

(cid:8)

two different primes (otherwise, the factorization is easy). Let i be such that λ can be
(cid:8) odd. There is at least a prime factor p of n such that 2i divides
written as 2i k
p − 1 and ( p − 1)/2i is odd. Obviously, x ∈ Z
∗
n is a quadratic residue modulo p if and
only if x 2i−1k
mod p is equal to 1. Let q be another prime factor of n. If 2i does not
mod q = 1 for all x ∈ Z
divide q − 1, then we have x 2i−1k
∗
n. Otherwise this holds if and
only if x is a quadratic residue modulo q. From these two facts (modulo p and q) and
the Chinese Remainder Theorem we infer that x 2i−1k
modulo q are
is a square root of 1. This means that there is
different with probability 1
(cid:8) − 1 mod n, n) yields a nontrivial factor of n for a random
a probability of 1
x ∈ Z
∗
n. We can iterate this process until we ﬁnd all factors of n.

modulo p and x 2i−1k

2 , but that x 2i−1k

2 that gcd(x 2i−1k

(cid:8)

(cid:8)

(cid:8)

(cid:8)

Finding an element order is a separate problem. We can ﬁrst notice that ﬁnding
element orders implies ﬁnding the group exponent. Indeed, the exponent is equal to
the lcm of all element orders by Lemma 7.3. Therefore, computing the lcm of orders
of a few elements quickly reaches the exponent as the number of elements grows.
Therefore, ﬁnding the order of an element is at least as hard as computing the group
∗
exponent which is equivalent to the factorization problem in the case of Z
n.

7.3.2 Computing Element Orders in Groups

As we saw in the previous section, computing element orders is at least as hard as
computing the group exponent.

In some particular groups, computation is easy. For instance, in Zn, we can compute
the order of x even though we do not know how to factorize n. The order is the smallest
nonnegative k such that kx ≡ 0 (mod n), namely such that n divides kx. The order is
simply given by the formula k = lcm(n,x)

.

x

When the complete factorization of the exponent of a group G is known, it is also
easy to compute the order of any group element x: let λ be the exponent of G and let
··· pαr
pα1
r be the complete factorization of λ (all pi are pairwise different primes, and
all αi are nonnegative integers). We want to compute the order of x ∈ G. We know that
1
it is a factor of λ. We set k = λ as a ﬁrst approximation for the order of x. For any i
pi = 1 in G. (We assume the group to be
from 1 to n, we replace k by k/ pi as long as x
multiplicatively denoted.) At the end we are ensured that k is the smallest nonnegative
power of x which is such that x k = 1.

k

∗
In the case of Z
n we obtain that

knowledge of the factorization of λ(n)
=⇒ ability to compute element orders in Z
=⇒ knowledge of λ(n)
⇐⇒ knowledge of the factorization of n

∗
n

Algorithmic Number Theory

203

but it is not quite clear whether one of the ﬁrst two reductions is actually an equivalence
or not. We can still suspect that knowledge of λ(n) and knowledge of the factorization
of λ(n) are not equivalent given the hardness of the factorization problem, but this is
not so obvious since the integer has a particular form and we have a hint with the extra
information of the knowledge of n.

√
In Section 7.4 we will present an algorithm computing element orders in O(
#G)

time when we do not know #G.

7.4 (cid:1)Discrete Logarithm

Another problem similar to factorization and widely used in cryptography is the discrete
logarithm problem: in a multiplicative group G generated by some g, compute an integer
x such that y = gx from y ∈ G. We summarize this by saying that we want to compute
logg y in G. There are a few variants.

r The order #G of the group can be available or not.
r Since the logarithm is in unique modulo #G, we can ask for one possible logarithm
r The y elements may not necessarily be in G and in that case, the problem consists
r and so on.

of distinguishing elements of G from other elements.

if #G is not available.

Here are some formal problem speciﬁcations.

DLP (Discrete Logarithm Problem):

Parameters: a cyclic group G generated by an element g ∈ G
Input: an element y in G
Problem: compute the least integer x such that y = gx
Parameters: a cyclic group G generated by an element g ∈ G, and the order

DLKOP (Discrete Logarithm with Known Order Problem):

#G

Input: an element y in G
Problem: compute x such that y = gx

Note that if the order of G is known, then computing the least discrete logarithm and
computing one representative are two equivalent problems. We can also consider the
problem when the factorization of the order of G is known.

DLKOFP (Discrete Logarithm with Known Order Factorization Problem):

Parameters: a cyclic group G generated by an element g ∈ G,

the order

#G and its factorization into prime numbers

Input: an element y in G
Problem: compute x such that y = gx

As an example, we can consider the multiplicative group G generated by some
g ∈ Z
∗
n. If ϕ(n) is unknown, it is quite hard to compute the order of g in general. It is

204
Chapter 7
also hard to check if a given y ∈ Z
n is in G or not. If y ∈ G, it is still hard to compute
∗
one x such that y = gx mod n.

In many cases, we can adapt factorization algorithms to solve all these problems.
√
Here we ﬁrst see that when #G and its factorization are known (which is the DLKOFP
problem) and when #G is B-smooth, we can have a simple algorithm within O(
B)
arithmetic operations.

7.4.1

(cid:1)Pollard Rho Method

The heuristic Pollard Rho factorization algorithm, described in Section 7.2.1, can be
adapted to solve the discrete logarithm problem when we are given the order of the
group (i.e. the above DLKOP) (see Ref. [150]).

Let G denote the (multiplicatively denoted) group, n denote its order, g and y be
two elements of G such that we look for some integer c such that y = gc. The idea
consists in managing a sequence of (x, α, β) triplets such that x = gα yβ. The sequence is
√
obtained by some kind of random walk. We expect to loop on the x term of the triplet
in O(
n) iterations, so that we obtain an equation of the kind

which leads us to

x = gα yβ = gα(cid:8)

yβ(cid:8)

c = α − α(cid:8)

β − β(cid:8) mod n.

The random walk is deﬁned by a function f (x, α, β) by

⎧⎪⎨
⎪⎩ (x × g, α + 1 mod n, β)

(x × y, α, β + 1 mod n)
(x 2, 2α mod n, 2β mod n)

f (x, α, β) =

if h(x) = 1
if h(x) = 2
if h(x) = 3

where h is a random balanced function from G to {1, 2, 3}. We can easily see that the
property x = gα yβ is preserved by replacing (x, α, β) by f (x, α, β). Analysis shows that
√
this is indeed a random walk so that we are expected to loop on the ﬁrst term in O(
n).
The complete algorithm is detailed in Fig. 7.8.

7.4.2

(cid:1)Shanks Baby Steps – Giant Steps Algorithm

√
We now describe the Shanks baby step – giant step algorithm which solves the DLKOP
problem within a complexity of O(
√
#G). It even solves the DLP problem when we
have an upper bound B for #G, within a complexity of O(
B). It is also convenient for
ﬁnding element orders (which are a kind of discrete logarithm of unity).

Algorithmic Number Theory

205

Input: g and y in a group G of order n
√
Output: the logarithm of y in basis g
Complexity: O(
n) group operations
1: pick a random function h : G −→ {1, 2, 3}
2: !a, !b ← (1, 0, 0) ∈ G × Zn × Zn

3: repeat
4:
5:

!a ← f (!a)
!b ← f ( f (!b))
6: until a1 = b1
7: output (a2 − b2)/(a3 − b3) mod n {fail if not possible}

Figure 7.8. Pollard Rho Discrete Logarithm Algorithm.

Let B be #G (if #G is not available, B can be any upper bound of #G). We proceed
as shown in Fig. 7.9. The algorithm eventually succeeds: if x is the unknown logarithm
modulo #G, we know that x ∈ {0, . . . , (cid:9)2 − 1}. It can thus be written x = i (cid:9) + j, for which
√
we get the match in the above algorithm. Therefore this algorithm computes the discrete
logarithm within O(
B)
memory space as well.)

√
B) group operations. (One problem is that it requires O(

7.4.3

(cid:1)Pohlig–Hellman Algorithm

Here we describe the Pohlig–Hellman algorithm (see Ref. [145]). It reduces the com-
putation of a discrete logarithm within a group G (the factorization of the order of
which #G = pa1
n is known) into computing discrete logarithms in groups of order
p1, . . . , pn. Combining this reduction with the Shanks baby steps – giant steps algo-
rithm we obtain an algorithm which solves the DLKOFP problem, i.e. which enables
the computation of discrete logarithms in a B-smooth ordered group of known factor-

··· pan

1

Input: g and y in a group G, B an upper bound for #G
√
Output: the logarithm of y in basis g
Complexity: O(
1: let (cid:9) = (cid:25)√
Precomputation
2: for i = 0, . . . , (cid:9) − 1 do

B(cid:26) be the size of a “giant step”

B) group operations

insert (gi (cid:9), i) into a hash table

3:
4: end for
Algorithm
5: for j = 0, . . . , (cid:9) − 1 do

− j

compute z = yg
if we have a (z, i) in the hash table then
yield x = i (cid:9) + j and stop {we get yg

− j = gi (cid:9)}

6:
7:
8:
end if
9:
10: end for

Figure 7.9. Baby Steps – Giant Steps Algorithm.

206
√
ization, within a complexity which is essentially O(
extra logarithmic complexity factors).

Chapter 7

B) group operations (plus some

1

n .
. . . pan

Let N = #G of given factorization N = pa1
We ﬁrst reduce the discrete logarithm computation to the case of n = 1 with the
following argument. If x is a discrete logarithm of y, then x is a discrete logarithm
of yαi in the subgroup generated by gαi . With αi = N p
−ai , the later discrete logarithm
i generated by gi = gαi , thus a number modulo pai
is in a subgroup of G of order pai
i .
−ai modulo pai
Conversely, xi is a discrete logarithm of y N p
in the subgroup generated by
−ai , and from the Chinese Remainder Theorem we can reconstruct x mod N such that
g N p
x ≡ xi (mod pai
−x is equal to 1. This way we reduce the problem
of computing x in G into n problems of computing discrete logarithms in groups of
order pai
i

i ). We then notice that yg

for i = 1, . . . , n.

i

To go further we can thus assume that n = 1 and N = pa. We will reduce
the computation by a computations of discrete logarithms in subgroups of order
p.

We proceed by computing x by more precise approximations of x by x j = x mod p j
for j = 1, . . . , a. Note that x0 = 1 is known. Assuming that we know x j−1, we notice that
−x j−1 ) pa− j is a power of g pa−1, hence of order p. Its discrete logarithm is indeed the
(yg
next base- p digit of x and enables the computation of x j. We thus compute the discrete
logarithm of (yg

−x j−1 ) pa− j and get x j.

To summarize, we have reduced the computation of x into a1 + ··· + an discrete
logarithm problems in subgroups of orders which are the prime factors of N. Since
a1 + ··· + an = O(log N ), we can combine this reduction with Shanks algorithm and
√
obtain a full algorithm of complexity O(
B log N ) group operations. The algorithm is
shown in Fig. 7.10.

As an example, we want to compute the logarithm of y = 123 in base g = 6 modulo
p = 125651. We ﬁrst check that p is a prime, so Z
∗
p is a cyclic group in which 123
is included. We have p − 1 = 125650 = 2 × 52 × 7 × 359. We ﬁnally check that 6 is a
q mod p (cid:4)= 1 for q = 2, 5, 7, 359. So 123 must be a power of 6
generator of Z
modulo p.

∗
p because 6

p−1

Applying the Pohlig–Hellman algorithm, we have to compute the following loga-

rithms.

log

p−1
2 mod p

g

(y p−1
2 ),

log

p−1
52 mod p

g

log

p−1
7 mod p

g

(y p−1
7 ),

p−1
52 ),
(y
(y p−1
359 )

log

g

p−1
359 mod p

Algorithmic Number Theory

207

1

. . . pan

Input: g and y in a group G, #G = N and the complete factorization
n such that pi is prime, p1 (cid:4)= p j , and ai > 0 for 1 ≤
N = pa1
i, j ≤ n and i (cid:4)= j
√
√
Output: the logarithm of y in base g
p1 + ··· + an
Complexity: O(a1
1: for i = 1, . . . , n do
(cid:8) ← g N / pai
g
(cid:8) pai −1
(cid:8)(cid:8) ← g
g
(cid:8) ← y N / pai
y
(cid:8)(cid:8) ← y
y
xi ← 0
for j = 0 to ai − 1 do

pn) group operations

3:
4:
5:
6:
7:

2:

(cid:8)

i

i

i

(cid:8)(cid:8)

in the subgroup of order

8:
9:

i

(cid:8) pai − j−1

(cid:8)(cid:8) ← y
y
compute the discrete logarithm u of y
pi which is spanned by g
(cid:8) ← y
(cid:8)u. p j
y
xi ← xi + u. p j

(cid:8)/g

(cid:8)(cid:8)

i

10:
11:
12:
13: end for
14: reconstruct and yield x such that x ≡ xi (mod pai
i )

end for

i

Let us ﬁrst compute log

p−1
2 mod p

g

Figure 7.10. Pohlig–Hellman Algorithm.

(cid:18)

(cid:19)

y p−1

2

. We have

p−1

2 mod p = 662825 mod p = 125650,

g

p−1

2 mod p = 12362825 mod p = 125650.

y

Therefore

Let us now compute log

p−1
7 mod p

g

(cid:18)

log

p−1
2 mod p

g

(cid:19)

y p−1

7

. We have

(cid:19)

(cid:18)

p−1
2

y

= 1.

p−1

7 mod p = 617950 mod p = 21153,

g

p−1

7 mod p = 12317950 mod p = 91649

y

so we need to compute log21153 mod p(91649) in a group of order 7. Since 7 is quite small,
we can exhaust all powers.

211530 mod p = 1
211531 mod p = 21153
211532 mod p = 6198

208

Therefore

Let us now compute log

p−1
52 mod p

g

Chapter 7

211533 mod p = 52301
211534 mod p = 91649

(cid:19)

(cid:18)

p−1
7

y

= 4.

log

p−1
7 mod p

g

(cid:18)

(cid:19)

p−1
52

y

. We have

p−1

52 mod p = 65026 mod p = 45194,

g

p−1

52 mod p = 1235026 mod p = 34726.

y

We know that this logarithm is in a group of order 52. We can have a ﬁrst approximation
(compute the modulo 5 part) by computing log451945 mod p(347265). We have 451945 mod
p = 10770 and 347265 mod p = 55981, so we need to compute log10770 mod p(55981) in a
group of order 5. Since 5 is quite small, we make a logarithm table.

107701 mod p = 10770
107702 mod p = 17027
107703 mod p = 55981
107704 mod p = 41872
107705 mod p = 1.

log10770 mod p(55981) = 3. Therefore

Thus
can take 34726/451943 mod p = 10770, and we notice that
base 10770 is 1. So we can check that
Therefore

So we
logarithm in
log45194 mod p(34726) = 3 + 1 × 5 = 8.

log45194 mod p(34726) mod 5 = 3.

its

log

g

(cid:18)

Let us ﬁnally compute log

p−1
359 mod p

g

p−1
52 mod p

(cid:19)

(cid:18)

(cid:19)

p−1
52

y

= 8.

y p−1

359

. We have

p−1

359 mod p = 6350 mod p = 19903,

g

p−1

359 mod p = 123350 mod p = 101887.

y

Shanks algorithm. Let (cid:9) = (cid:25)√
We need to compute log19903 mod p(101887) in a group of order 359. For this we need the
359(cid:26) = 19 be the “giant step.” We compute the table of

powers of 19903(cid:9) mod p for powers up to 18:

Algorithmic Number Theory

209

i

19903i (cid:9) mod p

0

1

1

2

3

4

5

6

7

8

9

24783

15001

94125 114711 28838 114917 108096 63848 21941

i

10

11

12

13

14

15

16

17

18

19903i (cid:9) mod p

71926 56972 122440 84521

81773

80931

71711

5969

38500

We sort this table into

(1, 0) (5969, 17) (15001, 2) (21941, 9) (24783, 1) (28838, 5) (38500, 18) (56972, 11)
(63848, 8) (71711, 16) (71926, 10) (80931, 15) (81773, 14) (84521, 13) (94125, 3)
(108096, 7) (114711, 4) (114917, 6) (122440, 12)

Next we compute 101887 × 19903

− j mod p until we hit a value in this table. We compute

101887 × 19903
101887 × 19903
101887 × 19903
101887 × 19903

−0 mod p = 101887
−1 mod p = 7139
−2 mod p = 114597
−3 mod p = 28838

which is in the table, corresponding to i = 5. So we can check that log19903 mod p
(101887) = 3 + 5 × 19 = 98.

Therefore we obtain

(cid:19)

(cid:18)

p−1
359

y

= 98.

log

p−1
359 mod p

g

Finally, we obtain that logg mod p(y) is congruent to 1 modulo 2, to 8 modulo 52, to 4
modulo 7, and to 98 modulo 359. We can now apply the Chinese Remainder Theorem.
We let

x = 1 · r2 + 8 · r52 + 4 · r7 + 98 · r359 mod ( p − 1)

where

(cid:2)(cid:16)

(cid:17)−1

p − 1
q

(cid:3)

mod q

rq = p − 1

q

×

and we obtain that logg mod p(y) = x = 23433.

210

Chapter 7

7.4.4

(cid:1)Factor Base and Index Calculus Algorithm

Factorization algorithms using factor bases can be adapted to the DLKOP problem,
which makes some researchers believe that the two problems might have the same
intrinsic complexity. This is the index calculus algorithm.

To compute the discrete logarithm of y in base g in a group G of known order
#G, we ﬁrst take a factor base B = { p1, . . . , pr}. The ﬁrst step consists of getting several
B-numbers of the form gk and to collect gki = pα1,1
random relations. When we
have r + c relations for a small constant c, we can solve the linear system in Z#G, which
leads to the discrete logarithms of all pi. Now we can pick a random ygk until it is a
B-number and get the discrete logarithm of y:

··· pα1,r

1

r

··· pαi,r

1. obtain several gki = pαi,1
2. solve the linear system deﬁned by all ki = α1,1x1 + ··· + αi,r xi in Z#G,
3. obtain ygk = p
··· p
,
4. infer logg y = β1x1 + ··· + β

r xr − k.

relations,

β 1
1

β r
r

1

r

(cid:18)
With a judicious choice of B the complexity of this algorithm in G = Z
∗
p
O

log p log log p

(cid:19)

√

e(c+o(1))

.

is

As an example, let us compute the discrete logarithm of y = 123 in base g =
34631. We use the factor base B = {2, 3, 5, 7, 11, 13}. We ﬁrst collect the
∗

7777 in Z
relations

g9 ≡ 53 × 111 × 131
g109 ≡ 22 × 35 × 51 × 71
g130 ≡ 31 × 52 × 112
g131 ≡ 25 × 31 × 73
g138 ≡ 33 × 51 × 131
g161 ≡ 25 × 111 × 131
g174 ≡ 23 × 37
g185 ≡ 21 × 53 × 71 × 131
⎞
⎛

then we write

⎛

⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝

⎞

⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠

9
109
130
131
138
161
174
185

=

⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝

5
1
1
3

7

2

5

5
3
1

⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠

⎞

⎟⎟⎟⎟⎟⎟⎟⎠

⎛

⎜⎜⎜⎜⎜⎜⎜⎝

x1
x2
x3
x4
x5
x6

×

1

1

2

1

1
1

1

3
1
2

1

3

1

3

1

Algorithmic Number Theory
211
If we extract the sixth and eighth rows, we obtain a matrix with determinant −133 which
is invertible modulo p − 1. By inverting the matrix, we obtain through computations in
Z p−1 that

⎞

⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠

=

⎛

⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝

x1
x2
x3
x4
x5
x6

⎞

⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠

⎛

⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝

10918
5240

18146

3187

13929

902

which leads to

2 = g10918 mod p
3 = g5240 mod p
5 = g18146 mod p
7 = g3187 mod p
11 = g13929 mod p
13 = g902 mod p

We can ﬁnally ﬁgure out that

yg30 ≡ 21 × 51 × 71 × 132

(mod p)

hence

y = g

−30+10918+18146+3187+2×902 mod p = g34025 mod p.

7.5 Exercises
Exercise 7.1. We call strong prime an odd prime n such that n−1
can (heuristically) generate (cid:9)-bit strong primes within a time complexity of O((cid:9)5).

2

is prime as well. Prove that we

Hint: Make the heuristic assumption that m and 2m + 1 behave like independent random odd

numbers when m is a random odd number.

Exercise 7.2. Assume we have a table S of n Boolean elements. We consider the ﬁve following
algorithms.

212

Chapter 7

for j ← 2i to n by step of i do

S[ j] ← false

Algorithm B:
1: for i = 1 to n do
S[i] ← true
2:
3: end for
4: S[1] ← false
5: for i = 2 to n do
6:
7:
8:
9: end for
10: for i = 1 to n do
if S[i] then
11:
12:
13:
14: end for

end for

end if

print i

for j = 2 to i do

if j divides i then
S[i] ← false

Algorithm D:
1: for i = 1 to n do
S[i] ← true
2:
3: end for
4: S[1] ← false
5: for i = 2 to n do
6:
7:
8:
9:
10:
11: end for
12: for i = 1 to n do
if S[i] then
13:
14:
15:
16: end for

end if
end for

end if

print i

for j = 2i to n by step of i do

S[ j] ← false

Algorithm A:
1: for i = 1 to n do
S[i] ← true
2:
3: end for
4: S[1] ← false
5: for i = 2 to n do
if S[i] then
6:
7:
8:
9:
10:
11: end for
12: for i = 1 to n do
if S[i] then
13:
14:
15:
16: end for

end for

end if

end if

print i

for j = i + 1 to n do
if i divides j then
S[ j] ← false

Algorithm C:
1: for i = 1 to n do
S[i] ← true
2:
3: end for
4: S[1] ← false
5: for i = 2 to n do
if S[i] then
6:
7:
8:
9:
10:
11:
12:
13: end for
14: for i = 1 to n do
if S[i] then
15:
16:
17:
18: end for

end for

end if

end if

end if

print i

Algorithm E:
1: for i = 1 to n do
S[i] ← true
2:
3: end for
4: S[1] ← false
5: for i = 2 to n do
√
for j = 2 to
i do
6:
if j divides i then
7:
S[i] ← false
8:
9:
10:
11: end for
12: for i = 1 to n do
if S[i] then
13:
14:
15:
16: end for

end if
end for

end if

print i

Algorithmic Number Theory

213

Each of these algorithms provides the same result. What is this result? What is the complexity

of each of these algorithms?

Hint: We admit that the number of prime integers smaller than n is equivalent to n/ log n. Infer
that the k-th prime is equivalent in magnitude to k log k, and that the sum of the inverses of prime
numbers smaller than n is equivalent to log log n.
Exercise 7.3. Factor 232 − 1, 264 − 1, 332 − 1.
Show that 28 + 1 and 216 + 1 are prime.
Factor 232 + 1 and 264 + 1 by any method.

Exercise 7.4. Using the paradigm which extends the p − 1 factorization method into the elliptic
curve method, propose an algorithm for ﬁnding a prime factor p of n when p + 1 is B-smooth.

∗
Hint: Use the GF( p2)

group structure.

Exercise 7.5. Factorize 530899, 509017, 539377 using the quadratic sieve method.

8
(cid:1)Elements of Complexity Theory

Content

(cid:1)Formal computation: languages, automata, Turing machines
(cid:1)Ability frontiers: computability, decidability
(cid:1)Complexity reduction: intractability, NP-completeness, oracles

In Chapter 1 we saw how to formalize secrecy based on information theory
with the notion of perfect secrecy. The Shannon Theorem says that secrecy cannot
be achieved unless we can afford the technical cost of the Vernam cipher, which is
not very practical. The Shannon Theorem was however formulated in the prehistory
times of computer science, and the notion of computation complexity did not exist.
The security of cryptographic algorithms always relies on a given frontier of com-
putational capability. Shannon implicitly explored the frontier based on information
availability. By looking at the foundations of computer sciences, we explore other fron-
tiers in this chapter. We will see that a frontier based on Turing complexity better ﬁts
cryptography.

8.1 (cid:1)Formal Computation

8.1.1

(cid:1)Formal Languages and Regular Expressions

Formally, a language is a set of words. A word is a ﬁnite sequence of characters taken
from an alphabet. An alphabet is a ﬁnite set . The basic operation deﬁned on words
is concatenation. Given two words u and v, we let u||v denote the concatenation of u
and v. We can thus let word denote the word (w, o, r, d) as the concatenation of four
elementary words which consist of single characters. We also deﬁne the length of a
word which is its number of characters. The length is additive with concatenation (the
length of a concatenated word is the sum of the lengths of concatenatees). A special
word is the null word ε of length zero. We have the property that u||ε = ε||u = u for any
word u. Concatenation in the set of all words over the alphabet  is thus an associative
law for which ε is a neutral element. It is also regular: u||v = u||w or v||u = w||u
implies v = w for any words u, v, w.

Alphabets of cardinality 1 are used in order to represent integers: if  = {1},
ﬁve is represented in unary by 11111. In many cases, we use the binary alphabet
 = {0, 1}.

216

Chapter 8

We can manipulate languages by using regular operations:

of a word from the language A and a word from the language B1;

1. concatenation: AB is the language of all words made from the concatenation
2. power: Ai is recursively deﬁned as Ai−1 A with A0 = {ε};
3. union: A ∪ B is simply the union of A and B2;
4. closure: A

is the union of all Ai for i ≥ 0.

∗

We notice that languages over the alphabet  are subsets of ∗
is the set of all
words over the alphabet . A regular language is a language obtained with the above
operations from elementary languages:

: ∗

1. the empty language ∅,
2. the null language {ε},
3. all single-character languages {a} for a ∈ .

For simplicity, we omit braces in operations on languages so that ε denotes {ε}, and a
denotes {a}. For instance, the set of odd unary numbers is a regular language over the
unary alphabet since it is 1(11)

∗

.

8.1.2

(cid:1)Finite Automata

A ﬁnite automaton consists of

1. a ﬁnite set Q of states,
2. a particular state q0 ∈ Q called the initial state,
3. a particular subset F ⊆ Q of ﬁnal states,
4. a ﬁnite set  called the input alphabet,
5. a function δ from Q ×  to Q called the transition function.

We recursively extend the δ function on Q × ∗
δ(q, ε) = q

by

and

δ(q, u||a) = δ(δ(q, u), a).

If a word u is such that δ(q0, u) ∈ F, we say that u is accepted by the
automaton. The set of all accepted words is the language accepted by the
automaton.

1 This operation is often called multiplication.
2 This operation is often called addition.

(cid:1)Elements of Complexity Theory

217

- q0

-1

?
q1

1

-1

q2

Figure 8.1. Finite automaton which recognizes odd unary numbers.
∗
As an example, we deﬁne an automaton which accepts 1(11)

alphabet  = {1}. We take Q = {q0, q1, q2} and F = {q1}. We deﬁne δ by

with the unary

δ(q0, 1) = q1
δ(q1, 1) = q2
δ(q2, 1) = q1.

Obviously, we reach state q1 after an odd number of 1 characters. This automaton
is depicted in Fig. 8.1. Circles represent states. Double circles represent the terminal
states. Transitions are represented by arrows. The arrow with no starting state shows
the initial state.

These automata are called deterministic, but we also consider nondeterminis-
tic automata for which δ is simply a subset of Q ×  × Q (and not a function
of Q ×  to Q). A word u = (a1, . . . , a(cid:9)) is accepted if there exists a sequence
((q0, a1, q1), . . . , (q(cid:9)−1, a(cid:9), q(cid:9))) of (cid:9) consecutive transitions (qi−1, ai , qi ) starting from
the initial state q0, all along the characters ai of u, and ending on a ﬁnal state q(cid:9) ∈ F. As
a matter of fact, languages accepted by nondeterministic automata can also be accepted
by deterministic ones as the following theorem shows.
Theorem 8.1. For any nondeterministic ﬁnite automatonA, there exists a deterministic
ﬁnite automaton B which accepts the very same language.
Proof. Let A = (Q, q0, F, δ). We let Q
set of all subsets of Q which contain at least one element of F. We let q
deﬁne the deterministic automaton B = (Q
of Q and any character a, we let δ(cid:8)
(q
exists at least one state q ∈ q
accepts the very same language as A.

be the
= {q0}. We
(cid:8)
) as follows. For any subset q
(cid:8), a) be the set of all states r in Q such that there
such that (q, a, r) ∈ δ. We now need to prove that B

be the set of all subsets of Q and F

(cid:8), q

(cid:8), δ(cid:8)

, F

(cid:8)
0

(cid:8)
0

(cid:8)

(cid:8)

(cid:8)

For this we prove by induction (on (cid:9)) that δ(cid:8)

, a1||···||a(cid:9)) is exactly the set
of all states q(cid:9) ∈ Q such that there exists a sequence (q0, a1, q1), . . . , (q(cid:9)−1, a(cid:9), q(cid:9))
, a1 ··· a(cid:9)−1), we know that
(cid:8)
of elements of δ: assuming this is true for q
δ(cid:8)
(cid:8), a(cid:9)), which is the set of all r such that there exists
0
(q
, a1||···||a(cid:9))
at least one q ∈ q
(cid:8), a(cid:9)).
(cid:1)
as well by deﬁnition of q

, a1||···||a(cid:9)) is equal to δ(cid:8)
(q
and δ(cid:8)

such that (q, a(cid:9), r) ∈ δ. Therefore, this holds for δ(cid:8)

(q
(cid:8) = δ(cid:8)

(q

(q

(q

(cid:8)
0

(cid:8)
0

(cid:8)
0

(cid:8)

(cid:8)

The following theorem characterizes the languages accepted by some automata.3

3 The proof is available in Ref. [92, p. 33] or [19, p. 321].

218

Chapter 8

Theorem 8.2 (Kleene 1956 [83]). A language is regular if and only if it is the language
accepted by a ﬁnite automaton.

8.1.3

(cid:1)Beyond Finite Automata Capabilities

Finite automata do some computations in very simple ways. The output of the compu-
tation is the information whether the input word is accepted or not. Regular languages
are thus a nice notion of simple computation. It is however not complete because many
simple languages are not regular, thus not accepted by ﬁnite automata, as shown by the
following result.
Theorem 8.3. The set of all 0i 1i words for i ≥ 0 is not a regular language.
Proof. Let A = (Q, q0, F, δ) be a ﬁnite automaton which accepts this language, and
let us look for a contradiction. For any i, we let qi be δ(q0, 0i ). Obviously, δ(qi , 1i ) is
a ﬁnal state. But since 0i 1 j is not accepted for i (cid:4)= j, all qi must be pairwise different.
Hence we have an inﬁnite sequence of states q0, q1, . . . , which is not possible for ﬁnite
(cid:1)
automata.

Finite automata could have been thought of as a good model for computation since
all computers are indeed ﬁnite automata. What the above result shows is that 0i 1i
cannot be recognized when i is very long, especially when it is much longer than what
the computer can store. Therefore this notion does not scale. Furthermore, the ﬁnite
automaton model does not capture the intuitive notion of complexity since the transition
function can be very expensive to implement. This is why we need another model.

8.1.4

(cid:1)Turing Machines

A Turing machine is a simple computer model which formalizes a strong notion of
computability. It is deﬁned by

1. a special blank character B,
2. an alphabet  which does not include B,
3. a ﬁnite automaton (Q, q0, F, δ) on the alphabet  ∪ {B},
4. an extension of δ which outputs an element of  ∪ {B} and an element of

{left, right} in addition to the new state.

A Turing machine has a tape which is an inﬁnite sequence of cells. Each cell contains
an element of  ∪ {B}. The Turing machine also has a tape head which points on a
given cell. Originally, the state of the machine is the initial state q0, the tape head points
on the leftmost cell, the leftmost cells are ﬁlled with characters of the input word, and
all other cells are ﬁlled with blank characters. For every move of the machine,

1. the machine reads the pointed cell and applies δ on it,
2. change the state according to the output of δ,

(cid:1)Elements of Complexity Theory

219

3. print the output character of δ on the pointed cell,
4. move the tape head by one position according to the output of δ (if it points on

the leftmost cell, it cannot move to the left).

We say that the Turing machine halts if it happens to enter into a ﬁnal state. We can
deﬁne the language accepted by a Turing machine as for ﬁnite automata: accepted
words are inputs for which the Turing machine halts. Languages which are accepted by
one Turing machine are called recursively enumerable languages. Note that for words
that are not accepted, the Turing machine is not assumed to halt. We can only guarantee
that for input words which are accepted, the machine will eventually halt.

As an example we show how to accept the set A of all 0i 1i , which is not a regular
language. We ﬁrst describe how the Turing machine works at a high level. Here are the
different states of the Turing machine.

q0: if the pointed cell is blank, terminate (i.e. accept) if the pointed cell is 1, enter
into an endless loop (i.e. reject) otherwise replace the pointed cell by a blank
character, move right, and change the state to q1
(we have erased the ﬁrst 0, looking for a 1)

q1: repeatedly move right until the pointed cell is not 0 if the pointed cell is blank,
enter into an endless loop (i.e. reject) otherwise move right and change the
state to q2
(we have found the ﬁrst 1, looking for the last one)

q2: repeatedly move right until the pointed cell is not 1 if the pointed cell is 0, enter
into an endless loop (i.e. reject) otherwise move left and change the state to q3
(we have gone beyond the last 1, we move back)

q3: replace the pointed 1 by a blank, move left, and change the state to q4

(we have erased the last 1, we move back)

q4: repeatedly move left until the pointed cell is blank

(we are moving toward the ﬁrst nonblank character) move right and change
the state to q0

is then easy to deﬁne the Turing machine. We have Q = {q0, q1, q2, q3, q4,
It
qF , qL} and F = {qF}, where qL is a special state which enters into an endless loop. We
have to deﬁne δ(q, a) for each q ∈ Q and each a ∈ {0, 1, B}. This consists of a triplet
in Q × {0, 1, B} × {left, right}. Here is how δ is deﬁned. (Places with the – character
are not used, so we do not deﬁne them.)

q

q0
q1
q2
q3
q4
qF
qL

δ(q, B)

δ(q, 0)

δ(q, 1)

qF
qL
q3
–
q0
qF
qL

–
–
B
–
B
–
–

–
–
left
–

right

–
–

q1
q1
qL
–
q4
qF
qL

B
0
–
–
0
–
–

right
right

–
–
left
–
–

qL
q2
q2
q4
q4
qF
qL

–
1
1
B
1
–
–

–

right
right
left
left
–
–

220

Chapter 8

Although real computers have a ﬁnite memory and not an inﬁnite tape, Turing
machines are better models than ﬁnite automata since the microprocessor is very close
to a ﬁnite automaton and the memory is virtually inﬁnite from the microprocessor point
of view! However, the access model of the memory is not so relevant since we do not
need to scan all the memory in order to reach the end.

8.2 (cid:1)Ability Frontiers

8.2.1

(cid:1)Standard Computational Models

The Church hypothesis consists of considering that all computable languages over 
are precisely those that are accepted by some Turing machine. We can even show that
adding extra characters in the alphabet does not extend the computational power of
Turing machines. We can thus consider that all computable languages over  are those
that are accepted by some Turing machine with the same alphabet. Languages over a
larger alphabet can always be encoded with this alphabet.

When the underlying ﬁnite automaton is not deterministic, we obtain a nondeter-
ministic Turing machine. We say that a language L is accepted by a nondeterministic
Turing machine M if for any word x, we have x ∈ L if and only if there exists a running
of M on x which eventually halts. As we transformed nondeterministic ﬁnite automata
into deterministic ones, we can represent nondeterministic Turing machines as deter-
ministic Turing machines with two tapes: one regular tape and one read-only tape on
which we only move to the right. We use this tape as an additional input which is used
in order to decide what is the next state. In simpler words it is interesting to consider
a nondeterministic algorithm on input x as a deterministic algorithm on inputs x and
r. The input r is sometimes called a witness for x being in L. Hence we say that L is
accepted by a nondeterministic Turing machine M if for any word x, we have x ∈ L if
and only if there exists a witness r such that M(x, r) eventually halts.

One special case of nondeterministic Turing machines is probabilistic Turing ma-
chines. Here we consider that the new state of the Turing machine is decided at random,
and we consider the probability that the Turing machine halts. Equivalently, we consider
r as being a random input, following a given distribution.

Obviously, nondeterministic Turing machines do not extend the notion of com-
putability. Indeed, we can simulate acceptance by a nondeterministic Turing machine
by doing an exhaustive search on the random input r with a deterministic Turing ma-
chine. The crucial difference, as will be noticed later, is about the complexity.

8.2.2

(cid:1)Beyond Computability

It is fairly easy to show the limits of computability by proving that there exist languages
which are not computable. This can be made by a standard set theory argument.

(cid:1)Elements of Complexity Theory

221

Since Turing machines over an alphabet  are totally deﬁned by a ﬁnite set of states
and a function δ over ﬁnite sets, we deﬁne an encoding technique in order to represent
pairs (M, x) of Turing machines M and input word x as words over the alphabet {0, 1}.
Let (cid:30)M, x(cid:31) denote the encoding of (M, x). Let us now consider the language L which
consists of all words representing a pair (cid:30)M, x(cid:31) for which M accepts the word x. We
call L the universal language. We can easily see that L is computable: we can build a
Turing machine A such that for any Turing machine M, M accepts x if and only if A
accepts (cid:30)M, x(cid:31).

Since the set of words is enumerable, the set of all Turing machines is enumerable.
Thus, the set of all computable languages is enumerable. But, the set of all languages is
not enumerable, which can be proven by a standard diagonal argument: let (w1, w2, . . .)
be an inﬁnite sequence which contains all words, and let (M1, M2, . . .) be an inﬁnite
sequence which contains all Turing machines. Putting these two sequences together
makes arbitrary pairs (cid:30)Mi , wi(cid:31) of Turing machines and words. Now let L d be the set of
all wi for which (cid:30)Mi , wi(cid:31) (cid:4)∈ L. If L d were a computable language, it would have been
accepted by some Turing machine. Let M j be this machine, and let us consider the
corresponding word w j . By deﬁnition of L d, w j is in L d if and only if (cid:30)M j , w j(cid:31) (cid:4)∈ L,
namely if, and only if M j does not accept w j . But since the acceptance language of M j
is L d, M j accepts w j if and only if w j ∈ L d, namely if and only if M j does not accept
w j ! This leads us to a contradiction. Hence, no Turing machine can accept L d.

8.2.3

(cid:1)Decisional Problems and Decidability

Limits of computability are even more puzzling when we realize that deciding whether
a word is accepted or not cannot be algorithmically decided.

More precisely, we have seen that the above universal language L is computable.
Let ¯L denote the set of all (cid:30)M, x(cid:31) pairs for which M does not accept x. We notice that
if ¯L is computable, there exists a Turing machine M
which can tell us when x is not
accepted by M. By using the notations of Section 8.2.2, we can build a new Turing
machine which accepts L d as follows.

(cid:8)

1. Enumerate all (cid:30)Mi , wi(cid:31) pairs until wi is the input x
2. Simulate M

on (cid:30)Mi , x(cid:31)

(cid:8)

This Turing machine accepts the language L d (from Section 8.2.2). This contradicts
the fact that L d is not computable. Therefore, ¯L is not computable.

This means that the acceptance problem is essentially asymmetric: we have seen
that we can make a universal Turing machine which can tell when x is accepted by M,
but we cannot make a Turing machine which tells when x is not accepted by M. Telling
whether or not a word is accepted is called a decisional problem. We have seen that de-
ciding whether a given M accepts a given x is undecidable. In particular, it cannot decide
whether or not M eventually halts on x. This problem is called the halting problem.

222

8.3 (cid:1)Complexity Reduction

Chapter 8

The most important idea of complexity theory for cryptography is the notion of prob-
lem reduction with oracles. The question of problem reduction aims at proving that a
problem A is at most as hard as a problem B by showing that an “efﬁcient” algorithm
can solve B when using an imaginary machine (an “oracle”) which can solve A. For
this we must properly deﬁne the notion of efﬁciency and oracle.4

8.3.1

(cid:1)Asymptotic Time Complexity

Whenever a decision problem is decidable, we can still wonder about the cost of the deci-
sion (or how efﬁciently we can solve it). A decision problem is deﬁned by a language L.
An instance of the problem is a word x. The problem is to decide whether x ∈ L or not.

For a Turing machine M which accepts L, and for a word x in L, we consider the
number TM,x of steps of M with input x before halting. For all integers n we consider
the maximum T (n) of TM,x over all words x ∈ L with length at most n. We say that M
has a time complexity of T .

We are usually interested in asymptotic complexities, which means about the order
of magnitude of the growth of T (n) when n grows. For this we must introduce some
standard notations. Given two functions f (x) and g(x) deﬁned over a set of values x
which is not upper bounded (i.e. in which x can grow up to inﬁnity), we let

r f (x) = O(g(x)) if there exist two constants c > 0 and x0 such that for any
x ≥ x0, we have | f (x)| ≤ c|g(x)|
r f (x) = (g(x)) if there exist two constants c > 0 and x0 such that for any
x ≥ x0, we have | f (x)| ≥ c|g(x)|, i.e. if g(x) = O( f (x))
r f (x) = (g(x)) if we have both f (x) = O(g(x)) and f (x) = (g(x)), i.e. if
there exist three constants c1 > 0, c2 > 0, and x0 such that for any x ≥ x0, we
have c1|g(x)| ≤ | f (x)| ≤ c2|g(x)|
r f (x) = o(g(x)) if for any ε > 0 there exists a constant x0 such that for any
x ≥ x0, we have | f (x)| ≤ ε|g(x)|.

We say that M is linear if T (n) = O(n). Similarly, we say that M is quadratic (resp.
cubic) if T (n) = O(n2) (resp. T (n) = O(n3)). We say that M is polynomial if there
exists an integer d such that T (n) = O(nd).

Making a difference between a language which can be accepted in linear time
or in quadratic time is somehow arbitrary because it depends on the intrinsic power
of the Turing machine: we could have deﬁned another computational model with—
say—Turing machines with two tapes instead of one. As we already said, our Turing
machine model wastes time in order to move the head from both ends of the tape.

4 For further reading about this section we recommend Ref. [20].

(cid:1)Elements of Complexity Theory

223

However, by having instant access to the memory, we “only” gain a factor limited to
the size of the used memory, which is limited to the size of the input and the size
of the intermediate results, which is itself less than the time complexity. This would
have deﬁned the same notion of computable languages, but one language which is
quadratically computable with a single tape could have become linearly computable
with the other. Therefore we only try to distinguish languages which can be accepted
within a complexity which grows “smoothly” from others, with a notion of smooth
growth which is robust with our notion of Turing machine. As Section 8.3.2 shows,
“smooth” means “polynomial” here.

8.3.2

(cid:1)Complexity Classes P, NP, co-NP

Polynomial growth is quite robust. Actually, if we can make a polynomial-time al-
gorithm (in an intuitive sense) which computes the language, then it can also be
implemented on a Turing machine in polynomial time. Therefore the simple Turing
machine model is enough in order to distinguish polynomial time algorithms from
others. We say that a problem deﬁned by L is polynomial if there exists a polyno-
mial Turing machine which accepts L. We let P denote the set of all polynomial lan-
guages.

Interestingly, once we know an upper bound on the complexity of a Turing machine,
we can transform it into a Turing machine which always halts within this complexity
on an acceptance or rejection state. Hence, decision problems become symmetric in
deterministic time.

As already seen, nondeterministic Turing machines deﬁne the same notion of
computability. However, the notion of polynomial languages is very different. We let
NP denote the set of all languages which are accepted by a nondeterministic Turing
machine in polynomial time. Saying that a language L is accepted by a nondeterministic
Turing machine M in polynomial time means that

1. for any word x, we have x ∈ L if and only if there exists a running of M on x
2. there exists an integer d such that for any word x ∈ L of length n there exists a

which halts

running of M on x which halts within a time at most nd

Obviously, the class P is included in the NP class since deterministic Turing machines
are special nondeterministic Turing machines. The question whether P is equal to NP or
not is still open. It is considered as one of the most fundamental problems in theoretical
computer science.

It should be emphasized that decision problems are no longer known to be symmet-
ric. (Of course, if P = NP, it is symmetric.) We let co-NP denote the set of all languages
L such that the complement of L is in NP. The decision problem is obviously symmetric
in NP ∩ co-NP: for any language L in this class, there exists a nondeterministic Turing

224

Chapter 8

machine M which eventually halts within a polynomial time either on an acceptance
or on a rejection or on a failure state such that

1. there exists an integer d such that for any word x of length n and any running

of M on x, the machine halts within a time of at most nd

2. for any word x, the set of all possible ﬁnal states when running M on x includes
3. for any word x, we have x ∈ L if and only if there exists a running of M on x

either the acceptance or the rejection state, but not both

which halts on an acceptance state

The question of whether co-NP is equal to NP or not, i.e. whether the decision problem
is symmetric in NP, is still open. It is also considered as one of the most important
problems in theoretical computer sciences.

8.3.3

(cid:1)Intractability

We recall that our goal is to formalize what a hard problem is in order to make secu-
rity rely on a hard problem. In complexity theory, this is the notion of intractability.
Although this is a quite intuitive notion, it should be emphasized that negative notions,
e.g. inability to solve a problem, is hard to deﬁne. We rather use reference problems
which are believed to be hard and we reduce one problem to the other.

Let  be an alphabet and let L 1 and L 2 be two languages over . We say that
accepting L 1 reduces (in a Karp sense5) in polynomial time to accepting L 2 if there
exists a function f over ∗
which is algorithmically computable in polynomial time
and such that for any word x we have x ∈ L 1 if and only if f (x) ∈ L 2. Two languages
are polynomially equivalent if they reduce to each other.

The case of the NP class is quite interesting. We say that a language L is NP-hard
if any L 0 ∈ NP reduces to L in polynomial time. We further say that L is NP-complete
if L ∈ NP. Actually, NP-complete languages are such that any language in NP reduces
to them. Therefore they have the greatest complexity in NP.

As an example we mention the language SAT of all satisﬁable Boolean formulas.
Informally, a Boolean formula is deﬁned as a tree in which every inner node is labeled
by a Boolean gate AND, OR, or NOT (the latter gate being on nodes with degree one
only) and every leaf is labeled by a variable vi . The formula is satisﬁable if there exists
an assignment of all variables to true or false such that the root node is evaluated
to true. We assume that we have deﬁned an efﬁcient encoding rule over an alphabet
in order to represent a Boolean formula. As an example, we can use the preﬁx encoding

5 From the name of Richard Karp who was, e.g. with Stephen Cook, one of the pioneers in complexity theory

in the early seventies.

(cid:1)Elements of Complexity Theory
225
rule over  = {∪,∩,¬, v, 0, 1}. For instance, x = ∪ ∩ ¬v0 ∪ v1v10 ∪ ¬v10v1
represents

((NOT v0) AND (v1 OR v10)) OR ((NOT v10) OR v1).

This can be satisﬁed by the assignment v0 = v1 = v10 = true. Therefore the word x
is in SAT.

Obviously, SAT is in NP since we can easily make a polynomial time Turing
machine which, for every input x and a witness r which encodes an assignment of
variable, checks that x and r are syntactically correct and that the assignment r satisﬁes
x. The following result tells us how acceptance of SAT is hard.

Theorem 8.4 (Cook 1971 [47]). SAT is NP-complete.

The idea of the proof consists of taking any language in NP and the corresponding
nondeterministic Turing machine. We consider the possible running step histories of
the Turing machine as variables, and we deﬁne a Boolean formula which checks that
the running steps are feasible with the Turing machine. This way we transform any
word x into a Boolean formula f (x) in polynomial time, and x is in the language if
and only if f (x) ∈ SAT.

8.3.4

(cid:1)Oracles and Turing Reduction

Let us deﬁne oracle Turing machines. An oracle is a Boolean function deﬁned by
a language L which corresponds to the decision membership problem. An oracle is
“connected” to an input tape which contains a ﬁnite set of nonblank words. The input
tape contains a query and the oracle answers whether the query is in the language or
not. An oracle Turing machine is a Turing machine with an additional tape which is
the input of an oracle, and special states query, yes, and no. Each time the Turing
machine enters into the query state, it instantly moves to a yes or no state depending
on the answer of the oracle. (Thus the transition function of the ﬁnite automaton does
not need to be deﬁned on the query state.) In other words, we use the oracle as a
subroutine which answers whether or not the query xi is in the language L.

Usually, we write L as a superscript. For instance, we can consider the class PL of
languages accepted by an oracle Turing machine, where the latter works in polynomial
time and has access to an oracle for the decision problem of membership in L.

We say that accepting a language L 1 reduces (in a Turing sense) in polynomial time
to accepting a language L 2 if L 1 ∈ PL2, i.e. if there exists an oracle Turing machine
with an oracle for membership in L 2 which accepts L 1 in polynomial time (see Fig. 2).
As an example, we have seen in Section 7.3.2 some Turing reductions related to the
problem of computing orders of elements in a group. For example, we have seen that we

226

Chapter 8

∈ L2?

Yes/No

	

x1, . . . ,xq

w

- ∈ L1?

- Yes/No

Figure 8.2. Oracle turing machine.

can compute the factorization of n by using an oracle that implements the Carmichael
function λ(n).

Two languages are Turing-equivalent if they reduce to each other in polynomial

time.

We have to make two important remarks. First of all, the decision problems are
symmetric: if L 1 reduces to L 2, then the complement of L 1 also reduces to L 2 (since the
oracle Turing machine is deterministic and the running time is bounded, we just need
to wait until the time bound in order to decide whether or not a given word is in L 1).
Second, the oracle solves the decision problem of membership in L 2 which is believed
to be harder than accepting L 2 in asymmetric cases such as NP-complete problems. For
instance, the SAT decision problem is at least as hard as accepting co-SAT. Therefore
we can solve both NP (due to the Karp reduction) and co-NP with PSAT. For this reason
people believe that NP is not closed under the Turing reduction. Hence we should limit
ourselves to NP ∩ co-NP as a reference class for intractable problems, which is the
case of cryptography.

8.4 Exercises
Exercise 8.1. Let L be the subset of {0, 1}∗
of all bitstrings with an even number of bits
equal to 1. Prove that it is a regular language by producing a ﬁnite automaton which
recognizes L.

Do the same with L equal to the set of all numbers which are congruent to 1 modulo
3 represented in binary notations, i.e. L is the set of all bitstrings (a0, a1, . . . , an)
such that

n(cid:1)

i=0

ai 2i ≡ 1 (mod 3)

Let x and y be two integers. Inspiring yourself from the square-and-multiply algorithm
from right to left, do the same with L equal to the set of all bitstrings which represent
a number which is congruent to x modulo y.

(cid:1)Elements of Complexity Theory

227

Exercise 8.2. We represent triplets of integers (a, b, c) by bitstrings (a0, b0, c0, a1,
b1, c1, . . . , an, bn, cn) such that

i=0

a = n(cid:1)
b = n(cid:1)
c = n(cid:1)

i=0

i=0

ai 2i

bi 2i

ci 2i

Prove that the language of all (a, b, c) triplets such that c = a + b is regular.

Exercise 8.3. We represent pairs of integers (a, b) by bitstrings (a0, b0, a1, b1, . . . , an,
bn) such that

a = n(cid:1)
b = n(cid:1)

i=0

i=0

ai 2i

bi 2i

Derive a Turing machine which takes a pair of integers (a, b) as an input and outputs
a bitstring which represents a + b.

Do the same with the multiplication a × b.

Exercise 8.4. We represent a graph G whose vertices set is {1, 2, . . . , n} by its adja-
cency matrix M. Here M is of size n × n where Mi, j is a bit set to one if and only if
the i-th vertex and the j-th vertex are connected by an edge in G. The matrix M is
itself represented by a bitstring of length n2 which is obtained by reading the matrix
row by row, column by column. We say that a graph G is c-colorable if there exists a
mapping C from {1, 2, . . . , n} to {1, 2, . . . , c} such that there exists no edge (i, j) such
that C(i) = C( j): two connected vertices always have different colors.

We let L be the language of all 2-colorable graphs. Prove that L is in P by deriving

an algorithm which yields whether a given graph is 2-colorable or not.

We let L be the language of all 3-colorable graphs. Prove that L is NP-complete.

9
Public-Key Cryptography

Content

Difﬁe–Hellman: asymmetric cryptography, the DH key agreement protocol
(cid:1)Knapsack problems: NP-completeness, the Merkle–Hellman cryptosystem
RSA: the cryptosystem, attacks against particular implementations
ElGamal Encryption

Interestingly, cryptography was the ﬁrst application of computer science: after
Alan Turing created the notion of Turing machine, the very ﬁrst computer was built
in order to break real-life cryptosystems for military purposes. Similarly, the in-
vention of complexity theory—in particular the notion of intractability through NP-
completeness—has been directly applied to cryptography in order to invent asymmetric
cryptography, also called public-key cryptography.

This chapter relates to the early foundations of asymmetric cryptography, and

modern applications (and attacks).

9.1 Difﬁe–Hellman

Invention of public-key cryptography is often attributed to Whitﬁeld Difﬁe and Martin
Hellman: in a famous paper (Ref. [59]) which was published in the IEEE Transactions
on Information Theory journal in 1976, they gave “new directions in cryptography,”
describing how we can use one-way functions, and notions of trapdoor permutations in
cryptography. A public-key cryptosystem is nothing but a kind of one-way permutation
(anybody can encrypt, but cannot decrypt) with a hidden trapdoor which enables the
decryption to the legitimate party.

Although this seminal paper played an outstanding role in the area of cryptography,
the role of Ralph Merkle in the foundations should not be neglected. Merkle submitted
his scheme in 19751 (discussed in p. 115) which enables the secure communication
over an insecure channel. The Merkle Signature Scheme based on hash trees was also
made in the late seventies2. The Lamport scheme which is given on p. 111 (published
in 1979; see Ref. [113]) also illustrates how to make an asymmetric access control
protocol from a one-way function.

1 But it was published in 1978 only! See Ref. [129].
2 But it was rejected for publication, and then published 10 years later. See Ref. [131].

230

Chapter 9

9.1.1 Public-Key Cryptosystems

Formally, a public-key cryptosystem is deﬁned by

puts a key pair (K p, Ks) where K p is a public key and Ks is a secret key;

r a pseudorandom key generator Gen: this is a probabilistic algorithm which out-
r an encryption algorithm Enc: this is an algorithm (which can be probabilistic)
r a decryption algorithm Dec: this is an algorithm (which necessarily implements

which outputs a ciphertext Y from an input plaintext X and a public key K p;

a deterministic function) which outputs the plaintext X from a ciphertext Y and
a secret key Ks.

As it will be seen later, the encryption is not necessarily deterministic: we can have
several ciphertexts which correspond to the same plaintext. Of course, the cryptosystem
should complete the following requirements:

r For any generated key pair (K p, Ks), any plaintext x, and any possible output y
of EncK p (x), we have DecKs (y) = x.
r Given the access to speciﬁcations, K p, and a DecKs oracle, it is intractable to

decrypt a generated ciphertext y without sending the query y to the decryption
oracle.

Note that the latter requirement is stated in a very informal way. Indeed, it is very hard
to agree on a precise deﬁnition of security for public-key cryptosystems. Therefore
we ﬁrst commit on an intuitive notion of security. As we will see several examples of
attacks, deﬁnitions will become more and more precise (and counterintuitive).

Fig. 9.1 is the Shannon model of encryption revisited for public-key cryptosystems.
We can transform an insecure channel into a conﬁdential one with the help of public-
key cryptography and an extra channel in order to transmit the public key (usually, the
key generator is run by the receiver). It is important to notice that the property required

Adversary



Plaintext

-

X

Encryption

Ciphertext

-

Y

-

Decryption

Y

-

X

6

Kp

AUTHENTICATED

Public key
Kp

6
Secret key
Ks

Generator

Figure 9.1. Asymmetric Encryption.

Public-Key Cryptography

231

Adversary



Plaintext

-

X

Encryption

Ciphertext

-

Y

-

Decryption

Y

-

X

6

Key
K

6

Key
K

ExchangeA



AUTHENTICATED

-

ExchangeB

Figure 9.2. Secure Channel Setup by a Key Exchange Protocol.

for this extra channel is not conﬁdentiality, it is indeed authentication; the sender of
the encrypted message must be ensured that the public key he uses is the appropriate
one.

We clearly see here that encryption and decryption are essentially asymmetric:
only the recipient of the ciphertext needs to have access to the secret key in order to
decrypt. In conventional cryptography, the secret key had to be used for both encryption
and decryption, and decryption was essentially the same operation as encryption in an
opposite order. Asymmetry is better (here) since we should not have access to a secret
in order to be able to encrypt. In addition, the secret key is the secret of a person instead
of being the secret of a pair of users.

Of course, this beneﬁt has a cost: public-key cryptosystems are much more involved
(both in terms of human being understanding and in terms of computer operations); they
are quite rare (Difﬁe and Hellman did not provide any example of such a cryptosystem:
they gave the notion of such a thing without proving it actually existed); as conventional
cryptosystems, their security is not guaranteed, and is often more risky to claim.

9.1.2 The Difﬁe–Hellman Key Agreement Protocol

Although Difﬁe and Hellman did not provide any example of a public-key cryptosystem,
they proposed a concrete key agreement protocol. The aim of this protocol is to generate
a common secret key between two parties over an insecure (but authenticated) channel
which can later be used with conventional cryptography (see Fig. 9.2). This kind of
protocol is also often called key exchange protocol, but there is no real exchange here
since the key is randomly generated.3

3 For this some authors distinguish “key exchange,” “key transfer,” and “key agreement.”

232

A

pick x at random, X ← gx
K ← Y x

Chapter 9

−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−− pick y at random, Y ← gy

X

B

Y

(K = gxy)

K ← X y

Figure 9.3. The Difﬁe–Hellman Key Agreement Protocol.

We assume that we have a communication channel between A and B which provides
authentication and integrity protection. If A and B want to communicate conﬁdentially
over an insecure channel, they must agree on a secret key K in order to use conventional
cryptography. For this, we assume that we are given a (multiplicatively denoted) group
G equipped with an efﬁciently computable group law, but some particular other problem
which will be deﬁned later is hard. In particular, computing discrete logarithms must be
hard. We also assume that we are given an element g ∈ G. Let (cid:9) be an approximation
of log2 #G (for instance, the integer part). Although #G (or more precisely the order of
the cyclic subgroup of G spanned by g) is not necessarily known, (cid:9) should be public.
The protocol works as follows (see Fig. 9.3).

square-and-multiply algorithm), and sends X to B.

1. A picks a random integer x of (cid:9) bits, computes X = gx in G (by using the
2. B picks a random integer y of (cid:9) bits, computes Y = g y in G (by using the
3. A computes K = Y x , while B computes K = X y in G.

square-and-multiply algorithm), and sends Y to A.

Note that when a multiple q of the order of g in G is provided, then we can pick x and y
in {0, . . . , q − 1}. Obviously, if A and B follow the protocol correctly, they end up with
the same key K which is K = gx y. This protocol must also protect the conﬁdentiality
of K , and so it should be hard, given X and Y , to compute K = X logg Y . This notably
implies that computing the discrete logarithm must be hard.

As an example, we can take prime p = 21489151 (although p is not large enough
∗
to make the computation of the discrete logarithm hard in Z
p). We can pick an arbitrary
g like g = 1609879. Then A picks x = 3916708 at random and computes X = gx mod
p = 13164781 and sends it to A. Then B picks y = 16766518 at random and computes
Y = g y mod p = 4109137 and sends it to B. In parallel, B computes K = X y mod p =
13275737. A can also compute K = Y x mod p = 13275737.

We notice that it is important to generate a g with a large order. It is quite hard,
in general, to compute the order of an element. (In our example, we can see that
p − 1 = 2 · 3 · 52 · 143261 and that g has the order 3 · 143261 “only”.) We can however
generate an appropriate (g, q, p) triplet with a large prime q such that g has an order
∗
of q in Z
p by

1. generating a prime q,
2. picking k at random and p = 1 + kq until it is prime,
3. picking g0 at random and g = g

p−1
q
0 mod p until it is not 1.

Public-Key Cryptography

A

pick x, X ← gx

−−−−−−−−−→

X

E

(cid:5)
, X

(cid:5) ← gx

(cid:5)

pick x

K1 ← (Y

(cid:5))x

Y

(cid:5)←−−−−−−−−−
(cid:5))
(K1 = gxy

(cid:5) ← gy
(cid:5)
(cid:5)
, Y
pick y
K1 ← X y
, K2 ← Y x
(cid:5)

(cid:5)

233

B

(cid:5)

X

−−−−−−−−−→
Y←−−−−−−−−− pick y, Y ← gy

(K2 = gx

(cid:5)
y)

K2 ← (X

(cid:5))y

Figure 9.4. First Man-in-the-Middle Attack in the Difﬁe–Hellman Key Agreement Protocol.

This ensures that g has an order of q (its order must be a factor of q which is not 1)
without having to completely factorize p − 1. Note that once we are ensured that g
spans a group of prime order q, then we can pick x and y in {0, . . . , q − 1} in the
Difﬁe–Hellman protocol.

We also notice that A and B must communicate over a channel which really provides
authentication. Otherwise the Difﬁe–Hellman protocol is vulnerable to the man-in-the-
middle attack. Assuming that messages are not authenticated, an adversary E can sit
in the middle and run concurrent protocols with A and B as depicted in Fig. 9.4. Then
E will share a key with A and B separately although A and B think that they share
a key with each other. Here A and B obtain different keys and E continues with an
active attack: she decrypts messages coming from one participant, re-encrypts them,
and sends them to the other participant. She can also make a more subtle attack in which
she no longer has to be active after the key agreement, and A and B obtain the same
key. For this we assume that the order of the group G can be written bw with b-smooth
(e.g. b = 1). In this attack E simply raises X and Y to the power w and get X
(cid:8)
and Y
so that A and B obtain the same key K which is a w-th power, i.e. in a subgroup of
(cid:8) = 1.) E can thus compute
smooth order. (In the case where b = 1, we obtain X
K by using the Pohlig-Hellman algorithm. This attack is depicted in Fig. 9.5.

(cid:8) = Y

(cid:8)

Another important property is the notion of forward secrecy. This property means
that if any long-term secret key is compromised, then the secrecy of the Difﬁe–Hellman
key will be preserved. Indeed, this key is meant to be used during a session and to be
discarded afterward. In the case which is described above, both x and y are ephemeral
keys which are discarded after the protocol. This means that they cannot be compro-
mised. We can also use a static version of the Difﬁe–Hellman protocol in which x
and y are long-term secret keys. This version does not provide forward secrecy since
disclosure of x or y eventually compromises K .

A

pick x, X ← gx

−−−−−−−−−→

X

E

(cid:5) ← X w

X

(cid:5)←−−−−−−−−−

Y

K ← (Y

(cid:5))x

solve X

(cid:5) ← Y w

Y
(cid:5) = gx
(K = gxyw)

(cid:5)

w, K ← yx

B

(cid:5)

X

−−−−−−−−−→
Y←−−−−−−−−− pick y, Y ← gy

(cid:5)

w

K ← (X

(cid:5))y

Figure 9.5. Second Man-in-the-Middle Attack in the Difﬁe–Hellman Key Agreement Protocol.

234

Chapter 9

DLP

x
	
X

X,Y

-

K = Y x

- K

Figure 9.6. Reduction of DHP to DLP.

The Difﬁe–Hellman problem is stated as follows:

DHP (Difﬁe–Hellman Problem)

Parameters: a group G, an element g ∈ G.
Input: two random elements X and Y in the subgroup of G spanned by g.
Problem: compute K = gx y where x = logg X and y = logg Y .

(We assume that generating an (cid:9)-bit integer x and computing X = gx correctly gener-
ates X, and the same for Y .) If this problem is hard, K is conﬁdential.

We would like to compare this problem with other ones. We ﬁrst focus on the
discrete logarithm problem (DLP) that is deﬁned on p. 160. Obviously, if we know
how to solve DLP, then we can solve DHP by computing x as the DLP of X and then
computing K = Y x . Thus DHP reduces to DLP (see Fig. 9.6). So, if the Difﬁe–Hellman
problem is hard, then the discrete logarithm problem in the subgroup spanned by g is
hard as well. DHP is believed to be hard for G = Z
∗
p with a large prime p.

For completeness we also mention the Decisional Difﬁe–Hellman problem, which

can be formulated as follows.

DDHP (Decisional Difﬁe–Hellman Problem)
Parameters: a group G, an element g ∈ G.
Input: a triplet (X, Y, K ) of elements in the subgroup spanned by g.
Problem: decide whether K = X logg Y or not.

Obviously DDHP reduces to DHP. This problem is also believed to be hard when the
subgroup of G = Z
∗
p (for a large prime p) which is spanned by g is large.

9.2 (cid:1)Experiment with NP-Completeness

Although Difﬁe and Hellman clearly deﬁned the notion of public-key cryptosystem
(by the notion of trapdoor permutation), they did not suggest any example. They did
not even prove that such a primitive existed.

Public-Key Cryptography

235

In the quest for trapdoor permutations, it is tempting to try to rely on some well-
known hard problems. Intuitively, the notion of trapdoor which makes computation easy
suggests that the decryption problem must be in the NP class. The hardest problems
in the NP class are NP-complete problems. One famous NP-complete problem is the
knapsack problem. So it is tempting to try to build a cryptosystem based on a knapsack
problem.

9.2.1

(cid:1)Knapsack Problem

Here is the description of the subset sum decision problem.4

DSSP (Decisional Subset-Sum Problem)

Input: a set of integers {a1, . . . , an, s} (represented in binary).
Problem: does a subset of {a1, . . . , an} whose sum is s exist?

Intuitively, the ai represent the size of packets that we want to put in a knapsack of
size s. The question is whether a subset of packets which exactly ﬁts into the knapsack
exists or not. This is why we often call this problem the knapsack problem.

Theorem 9.1 (Karp 1972). The language of subset sum problems which are solvable
is NP-complete.

Since deciding whether or not a subset sum problem has a solution is hard, ﬁnding
a solution to a solvable problem must be hard as well. Hence we can try to build a
cryptosystem based on the hardness to ﬁnd a solution. The problem consists of hiding
a trapdoor for the legitimate decryptor.

9.2.2

(cid:1)The Merkle–Hellman Cryptosystem

The Merkle–Hellman cryptosystem is one of the ﬁrst examples (or candidates) of a
public-key cryptosystem. It was published in 1978 as Ref. [132]. The main idea consists
in giving to the knapsack problem a particular form so that the problem becomes trivial,
and in hiding this particular form with a secret transformation.

Here, the particular form is super-increasing knapsacks. We say that (b1, . . . ,
j=1 b j for i = 1, . . . , n + 1. We notice
i−1
bn, bn+1) is super-increasing if we have bi >
that if (b1, . . . , bn, bn+1) is super-increasing, and if we let M = bn+1, then any knapsack
problem (b1, . . . , bn, s) modulo M is equivalent to the knapsack problem (b1, . . . , bn, s)
over the integers, which is itself quite trivial: we notice that bn is in the solution sum if
(cid:8) = s
and only if bn ≤ s, and we then reduce to a subproblem (b1, . . . , bn−1, s
or s

(cid:8) = s − bn.

(cid:26)

(cid:8)

) with s

4 For more information, see Ref. [72].

236

Chapter 9

The secret transformation which is used in order to hide the super-increasing

property is simply a multiplication by a secret key modulo M.

Here is the cryptosystem.

Public parameter: an integer n.
Key generation: choose a super-increasing sequence (b1, . . . , bn, bn+1 = M), an
M , and a permutation π of {1, . . . , n}.
integer W ∈ Z
∗
Compute ai = W bπ(i) mod M for i = 1, . . . , n.
Public key: K p = (a1, . . . , an).
Secret key: Ks = (b1, . . . , bn, M, W, π).
Message: a binary sequence m = m1 ··· mn of length n.
Encryption: c = m1a1 + ··· + mnan.
Decryption: compute cW
lem x1b1 + ··· + xnbn = cW

−1 mod M, solve the super-increasing knapsack prob-

−1 mod M and let mi = xπ(i).

Unfortunately, this cryptosystem was broken a few years later by Adi Shamir (see
Ref. [164]). As it was explained later, the security of the Merkle–Hellman cryptosystem
does not rely on the full genericity of the NP-complete problem, but on some particular
instances which are not necessarily hard. Indeed, the problem can be solved if we can
ﬁnd a small vector in a related lattice.

Mathematically, a lattice is a discrete subgroup of Rn. More intuitively, it is the
set of all linear combinations of some given constant vectors, but with only integral
coefﬁcients. Finding a small vector (in the sense of Euclidean norm) in a lattice is
known to be a hard problem, but which might be easy in many cases. Namely, the
famous LLL algorithm can be used to reduce lattices and break cryptosystems like the
Merkle–Hellman one.5

9.3 Rivest–Shamir–Adleman (RSA)

The ﬁrst public-key cryptosystem which is still secure and used was invented by Ronald
Rivest, Adi Shamir, and Leonard Adleman, the initials of whom led to the name of
the cryptosystem: RSA. It was published in 1978 as Ref. [158] in the journal Commu-
nications of the ACM. Since then, this algorithm has been adapted, generalized, and
transformed into several standards. Surprisingly, as it will be seen, although the plain
RSA cryptosystem was not broken, many adaptations and standards based on RSA are
weak. This raises all the ambiguity of public-key cryptosystems.

9.3.1 Plain RSA Cryptosystem

As depicted in Fig. 9.7, here is how the plain RSA algorithm works. (By plain RSA we
mean a theoretical algorithm. As it will be seen, this algorithm is not directly usable

5 A survey by Nguyen and Stern on lattices in cryptography is available as Ref. [139].

Public-Key Cryptography

237

Adversary



Plaintext

x

-

Encryption

Ciphertext
xe mod N

-

-

Decryption

y

-

yd mod N

6

e,N

AUTHENTICATED

Public key
e,N

6
Secret key
d, N

N = pq

ϕ(N) = (p− 1)(q− 1)
1 = gcd(e,ϕ(N))
−1 mod ϕ(N)
d = e



-

Generator

Figure 9.7. Plain RSA Cryptosystem.

since it is described in a mathematical format. We will see that there are many bad ways
to use it in practice.)

−1 mod (( p − 1)(q − 1)).

Public parameter: an even integer s.
2 bits. Set N =
Setup: ﬁnd two random different prime numbers p and q of size s
pq. Pick a random e until gcd(e, ( p − 1)(q − 1)) = 1. (Sometimes we pick a
special e like e = 17 or e = 216 + 1 and we select p and q accordingly.) Set
d = e
Message: an element x ∈ {1, . . . , N − 1}.
Public key: K p = (e, N ).
Secret key: Ks = (d, N ).
Encryption: y = x e mod N .
Decryption: x = yd mod N .

We notice that the modulus N is of s bits. The security will highly depend on this
s parameter. In particular, if factoring integers of this size is easy, then RSA is not
secure because N is public and factoring N yields p and q, which are enough to enable
decryption. Typically, we take s = 1024.

It is easy to check that the encryption is deterministic and that if we decrypt
y = x e mod N , we raise it to the power d modulo N and obtain x ed mod N . Since
ed ≡ 1 (mod ϕ(N )), it can be written that ed = 1 + kϕ(N ). We have

yd mod N = x ed mod N = x 1+kϕ (N ) mod N .

For gcd(x, N ) = 1, this is x. For gcd(x, N ) = p, we can check that it is congruent to
x modulo p and modulo q separately, so it is x as well, due to the Chinese Remainder
Theorem. For gcd(x, N ) = q, we similarly obtain x. Hence y decrypts well into x.

The complexity of the RSA algorithm is quite simple to analyze.

238

Chapter 9

Setup. We have to generate two random primes of s
2 bits. We know that this works
within a complexity of O(s4) elementary operations by using the Miller-Rabin
test over random integers. Then, multiplying p and q costs O(s2), ﬁnding an
invertible e costs one gcd computation, hence O(s2), and computing d requires
an extended Euclid algorithm, hence O(s2) as well. In total the complexity is
O(s4).
Encryption. The encryption costs one full modular exponential, hence O(s3). We
however often choose e in order to decrease this complexity. For instance, with
e = 17 or e = 216 + 1, the complexity is O(s2).
Decryption. The decryption costs one modular exponential, hence O(s3).

The security analysis is more tricky. Obviously, the knowledge of p and q is enough
to recover the secret key and thus to break the system: we just need to invert e modulo
( p − 1)(q − 1). Therefore the factorization of N enables breaking the system. This
does not mean that RSA is as strong as factoring is hard, although it is believed to be
so. Here are a few problems.

RSADP (RSA Decryption Problem)

Input: an RSA public key (e, N ), an encrypted message y.
Problem: compute x such that y = x e mod N .

RSAKRP (RSA Key Recovery Problem)

Input: an RSA public key (e, N ).
Problem: compute d such that x ed mod N = x for any x ∈ Z
∗
N .

RSAEMP (RSA Exponent Multiple Problem)

Input: an RSA modulus N .
Problem: ﬁnd an integer k such that x k mod N = 1 for any x ∈ Z
∗
N (i.e. λ(N )
divides k).

RSAOP (RSA Order Problem)
Input: an RSA modulus N .
Problem: compute the order of Z

RSAFP (RSA Factorization Problem)

∗
N (i.e. ϕ(N )).

Input: an RSA modulus N .
Problem: compute the factorization of N .

Obviously, RSADP reduces to RSAKRP. Indeed, the secret key enables the decryption!
Reducing the other way is still an open problem: we cannot prove at this time that the
secret key is necessary in order to decrypt, nor that decrypting enables the computation
of the secret key. What we can prove is that RSAKRP is equivalent to RSAFP (i.e., the
knowledge of the secret key is equivalent to the ability to factorize N ), which does not
mean that decrypting is equivalent to factorizing. For this let us ﬁrst do the following
reductions.

r RSAKRP reduces to RSAOP. Clearly, if we can compute the order ϕ(N ) which
is equal to ( p − 1)(q − 1), we can invert e modulo this order and get a d.
r RSAOP is equivalent to RSAFP. Clearly, if we can factorize N , we get p and q
and we can compute ϕ(N ) = ( p − 1)(q − 1). Conversely, if we have ϕ(N ), we

Public-Key Cryptography

239
notice that ϕ(N ) = N − ( p + q) + 1 so p and q are the two roots of X 2 − (N −
ϕ(N ) − 1)X + N = 0. Solving this equation leads to the factorization of N .
r RSAEMP reduces to RSAKRP. If we can compute d, then we can get a multiple
of λ(N ), namely ed − 1.

So it is sufﬁcient to show that RSAFP reduces to RSAEMP in order to prove that
RSAKRP, RSAEMP, RSAOP, and RSAFP are all equivalent. Let us assume that we
know an integer k which is a multiple of λ(N ). The following algorithm factorizes N
in a surprisingly similar way to the Miller-Rabin primality test.

1. Let us thus write k = 2tr with r odd. (We isolate the powers of two.)
N and we compute y = xr mod N . We iterate until
2. We pick a random x ∈ Z
∗
y (cid:4)= 1. (Note that for at least three fourths of the x’s which are not quadratic
residues modulo p or q, the corresponding y cannot be equal to 1, so we usually
do not have to iterate.)
3. If any of y, y2 mod N , . . . , y2t−1 mod N is equal to N − 1, go back to the
previous step and try again. Otherwise, since y2t mod N must be 1, we have
found a nontrivial square root z of one.
4. Compute gcd(z + 1, N ) which must be a nontrivial factor of N : either p or q.
q ) = −1, then we
(See Fig. 9.8.) Note that if we have, for instance, ( x
2 mod q = q − 1. Thus, if 2ir = lcm( p − 1, q − 1),
have x p−1
then x 2i−1r mod N is equal to 1 modulo p and to q − 1 modulo q, hence it is a nontriv-
ial square root of one and the previous algorithm yields p and q. The same holds for
q ) = 1. Thus the previous algorithm works for at least half of the x’s,
p ) = −1 and ( x
( x
and eventually halts with the factorization of N .

2 mod p = 1 and x q−1

p ) = 1 and ( x

Here is the list of reductions that we have proven.

RSADP ⇐ RSAKRP ⇐ RSAOP
RSAEMP ⇒ RSAFP

#

⇓

The problem of whether breaking RSA is equivalent to the factorization of N or not
is a famous open problem. Paradoxically, it is a good thing to have no reduction, because
one would have been able to use it as a chosen ciphertext attack: using the decryptor as
an oracle in a chosen ciphertext attack would have led to the factorization of N , thus

(cid:7)
-(cid:10)= 1

xr mod n

-(cid:10)= 1

SQ

-(cid:10)= 1

SQ

at most t

(cid:8)(cid:9)
···

(cid:10)

SQ

-

1

-(cid:10)= 1

SQ

-(cid:10)= 1
6
?

is it ≡ −1?

Figure 9.8. Reduction of RSAFP to RSAEMP.

240

Chapter 9

to a secret key recovery. This means that the lack of equivalence between RSADP and
RSAFP guarantees that we cannot extract a secret key from a sealed decryption device.
Obviously, breaking RSA means computing e-th roots. Even with e = 3, we do not
know if computing a cubic root is equivalent to factorizing N in the RSA case, i.e. when
gcd(e, ( p − 1)(q − 1)) = 1. We can prove equivalence in some other cases, i.e. when e
is not invertible modulo ( p − 1)(q − 1). For e = 2, we can prove that computing square
roots is equivalent to factorizing N : we just pick a random x, compute y = x 2 mod N ,
and compute one square root z of y. If x ≡ ±z (mod N ), we just try again. Otherwise
we use the Fermat method in order to factorize N . This eventually halts because the
square root algorithm has no means to know which are the two square roots which are
not interesting (namely x and N − x). For e = 3, we can similarly prove that computing
cubic roots is equivalent to factorizing N when 3 divides ( p − 1)(q − 1).

9.3.2 RSA Standards

PKCS (Public-Key Cryptography Standards) is a set of algorithms published by the
RSA Data Security company. Of course, they are based on the RSA algorithm. Here
we give details about the encryption scheme of the PKCS#1v1.5 standard. Note that it
is now replaced by PKCS#1v2.1 (Ref. [14]) as we will discuss in Section 9.3.8.

We are given a modulus N of k bytes. In order to encrypt a message M of length

at most k − 11 bytes, we proceed as follows.

1. We generate a string PS of pseudorandom nonzero bytes so that the message

concatenated with PS is of length k − 3 bytes.
another zero byte, and the message. We write this 00||02||PS||00||M.

2. We construct the byte string by concatenating a zero byte, a byte equal to 2, PS,

3. This byte string is converted into an integer.
4. We compute the plain RSA encryption.
5. We convert the result into a string of k bytes.

The decryption is then straightforward.

1. We convert the ciphertext into an integer. We reject it if it is greater than the

modulus.

2. We perform the plain RSA decryption and obtain another integer.
3. We convert the integer back into a byte string.
4. We check that the string has the 00||02||PS||00||M format for some byte strings

PS and M where PS has no zero bytes.

5. We output M.

The 02 byte is used in order to specify that the message is an encrypted message in the
RSA format. Some other parts of the PKCS standard use other values for this byte.

Public-Key Cryptography

241

9.3.3 Attacks on Broadcast Encryption with Low Exponent

Although the RSA cryptosystem is believed to be quite strong, there are many ways to
use it insecurely. One of it is the broadcast encryption with low encryption exponent.
Let us assume that one sender wants to broadcast the same message x to n different
parties who have public keys (e, N1), . . . , (e, Nn) with the same low exponent e. (In
many applications, e = 3 is suggested, so this hypothesis seems reasonable.) The sender
has to send yi = x e mod Ni to the i-th receiver for i = 1, . . . , n.

Assuming that we have n > e, it becomes fairly easy to decrypt all the correspond-
ing ciphertexts y1, . . . , yn: Let us assume that all Ni are pairwise relatively primes,
which means that no prime factor is used in two different moduli Ni and N j . (If this
does not hold, then the system has indeed other weaknesses.) Let N be the product of
all Ni . Due to the Chinese Remainder Theorem, we can compute y such that y < N
and y ≡ yi (mod Ni ) for all i. This way we have y ≡ x e (mod Ni ) for all i, thus y ≡ x e
(mod N ). However, the natural integer x e is less than N because x is less than all Ni
and N is the product of more than e integers Ni . Since y < N as well, we have indeed
y = x e. Then it becomes trivial to extract the e-th root over the natural integers in order
to compute x.6

9.3.4 Attacks on Low Exponent

Low exponents have several drawbacks. We distinguish low e’s from low d’s.

When e is low, there exist efﬁcient attacks due to Don Coppersmith against various
uses of RSA (see Ref. [49]). More precisely, there exists an efﬁcient algorithm in log N
which ﬁnds roots of a modulo N polynomial equation of degree e as long as one of it is
less than N 1
3 of the bits of the plaintext are known
and e = 3, then one can decrypt efﬁciently. As another application, if two plaintexts
differ only in a (known) windown of length one ninth of the full length and e = 3, one
can decrypt the two corresponding ciphertexts.

e . As an application, if a fraction of 2

When d is low, there exists an efﬁcient algorithm due to Michael Wiener which
√
enables the recovery of the secret key (see Ref. [185]). It works in typical cases (with
p and q of same size) with d < 4
N . This bound can further be improved by using the
LLL lattice reduction algorithm.

9.3.5 Side Channel Attacks

There are several ways to get “side information” in order to be able to break RSA.
As a ﬁrst example we mention the differential fault analysis. We assume that we
can play with the decryptor device as with a black box and we want to retrieve the

6 This simple attack is due to Johan H˚astad. See Ref. [87].

242

Chapter 9

secret key. We also assume that this device uses the Chinese remainder accelera-
tion: we know that computing yd mod N can be done within O((log N )3). We can
also compute yd mod p and yd mod q and combine them by the Chinese Remain-
der Theorem. Since p and q are half size numbers, doing computation modulo p
(or q) requires one eighth of this complexity. The exponent d can further be reduced
modulo p − 1, and modulo q − 1, and so we get an acceleration by a factor of 8 by
using this trick. Therefore, assuming that the decryptor uses this acceleration trick is
reasonable.

Now we assume that we can physically stress the device (by heat, beams, corrupted
input power or clock signals, etc.) so that the device will make errors. Let us just pick
x and compute y = x e mod N and send y to the decryptor. We assume that we stress
the decryptor so that an error occurs in the modulo p computation, but not in the
(mod q) and
modulo q computation. The decryptor will return x
(cid:8), N ) = q and we can factorize N and compute
x (cid:4)≡ x
the secret key of the decryptor!7

(mod p). Therefore, gcd (x − x

(cid:8)

(cid:8)

such that x ≡ x

(cid:8)

This attack is rather an implementation attack in which the adversary gets side

information by quite active attacks.

A more peaceful way to retrieve side information is to measure the computation
time: some microprocessors have a multiplication instruction whose computation time
depends on the input operands. As was shown by Paul Kocher, this information can
be used in order to recover some information about internal computations, and then to
recover the secret keys (see Ref. [103]).

A more sophisticated attack consists in measuring the evolution of the power con-
sumption with time: if we know which program is run by a microprocessor (but we
do not know the key), we can measure with an oscilloscope the power consumed by
the microprocessor for every instruction. The RSA decryption consists of computing a
modular exponentiation to a secret power d. The square-and-multiply method sequen-
tially computes a square and a multiplication for a bit set to 1, or just a square for a bit
set to 0. This is done for every bit of d, sequentially. Presumably, the square operation
and the multiplication will be implemented quite differently, and the power consump-
tion analysis will be able to say when the decryption device makes a multiplication and
when it computes a square. Therefore we will be able to “see” the 0 bits and the 1 bits
of d sequentially! (see Ref. [104]).

This technique can be extended to block ciphers. Assuming that the power depends
on the values of the registers (a memory bit consumes a different power for a bit 0 and
a bit 1), the power analysis can tell what is the Hamming weight (namely the number of
bits set to 1) of all registers. If the computation starts by computing x ⊕ K for a secret
K , since we know x and we can get the Hamming weight of x ⊕ K , by trying several

7 This attack was published in Ref. [35].

Public-Key Cryptography

243

x values, we can retrieve K . (Note that this is not that simple in practice since many
technical problems must be solved.)

There is yet another example of side channel attack which is due to Daniel
Bleichenbacher, from Bell Labs (see Ref. [34]). It is an attack against PKCS#1v1.5. In
this scenario, the cleartext is ﬁrst transformed into a plaintext according to a speciﬁc
format prior to encryption.8 After decryption, the format is checked before the clear-
text is recovered. The problem is: what about invalid formats? In the ﬁrst version of
PKCS#1v1.5, an invalid format was indicated as an error to the sender of the cipher-
text, and this information was useful for adversaries. Actually, adversaries could use the
receiver as an oracle which checked the correctness of the format, and the decryption
problem would become easy with the help of this oracle by successive approximations.

9.3.6

(cid:1)Bit Security of RSA

So far we wondered how hard recovering the whole plaintext was. Assuming that it is
hard, one can however wonder if recovering a part of it is also hard. As an example,
we can ask ourselves what is the security of the least signiﬁcant bit: how hard is it to
recover the least signiﬁcant bit of the plaintext?

We can prove that recovering this bit is as hard as the decryption problem, by using
the notion of Turing reduction. Let us assume that we have an oracle lsbdec which,
given a ciphertext y, returns the least signiﬁcant bit of x = yd mod N . First of all, we
notice that we can use it to implement an oracle which returns the most signiﬁcant bit
msbdec(y) = msb(x) by observing the following identity.
msb(x) = lsb(2x mod N ).

Hence

msbdec(y) = lsbdec(2e y mod N ).

We can decrypt a given ciphertext y by using the following algorithm.

1: a ← 0, b ← N
2: for i = 0 to (cid:28)log2 N(cid:29) do
if msbdec(2ie y mod N ) = 0 then
3:
a ← (a + b)/2
4:
else
5:
b ← (a + b)/2
6:
end if
7:
8: end for
9: yield (cid:28) a(cid:29)

8 We recall that the cleartext is the message in clear and the plaintext is the input of the encryption algorithm.

244
Chapter 9
We can indeed show that msb(2i x mod N ) = 0 if and only if there exists an integer k
such that

2k
2i+1

≤ x
N

<

2k + 1
2i+1

.

So this algorithm simply gets the binary expansion of x/N .

The least (and most) signiﬁcant bits of the plaintext are thus called hard core
bits since recovering them can be used to break the cryptosystem. Further studies can
additionally show that even approximating those bits, i.e. being able to predict them
with a nontrivial advantage, can also be used to break the scheme.

There are bits which are not hard-core, e.g. the Jacobi symbol. Let

(−1)bit(x) =

(cid:18)

(cid:19)

x
N

(cid:18)

(cid:19)

(−1)bitdec(y) =
(cid:17)
(cid:17)

(cid:16)

=

x e
N

=

y
N

(cid:18)

x
N

(cid:16)

x e mod N

N

(cid:18)

(cid:19)e =

(cid:19)

.

x
N

and bitdec be deﬁned as above. We can easily compute bitdec by noticing that

The plain RSA encryption indeed preserves the Jacobi symbol since e is odd and

9.3.7

(cid:1)Back to the Encryption Security Assumptions

After having seen all these attacks on public-key encryption schemes, a question re-
mains: what minimal security requirements must such a scheme satisfy? There are so
many attack scenarii that we have seen so far that the intuitive notion of encryption
security, i.e. that the decryption problem is computationally hard, is not sufﬁcient. For
instance the decryption problem could in theory still be hard even though recovering
half of the plaintext would be easy, but this would not match our intuition about security.
Currently, there are two popular notions of security against three notions of adversaries,
leading us to six possible deﬁnitions.

First of all, Shaﬁ Goldwasser and Silvio Micali proposed the notion of semantic
security (see Ref. [77]). This notion intuitively means that the ciphertext leaks no
interesting bit of information about the plaintext. This notion is described as a game
between a challenger and an adversary. The game runs as follows (see Fig. 9.9).

1. First of all, the challenger and the adversary are given the cryptosystem.
2. The challenger generates a matching pair of public and secret keys and discloses

the public one.

3. The adversary selects two plaintexts x0 and x1 and sends them to the challenger.

Public-Key Cryptography

Challenger

generate Kp, Ks
pick b ∈R {0,1}
compute y = Enc(xb)

245

Adversary

Kp

x0, x1

−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−− select x0,x1
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−− guess b

y

b

Figure 9.9. Semantic Security Game.

4. The challenger picks uniformly at random a bit b. He encrypts xb and sends the

ciphertext y to the adversary.

5. The adversary tries to guess b.

When the cryptosystem is secure, the adversary cannot guess b with a signiﬁcant
advantage, so we often say that the two messages are indistinguishable. The indistin-
guishability notion is denoted IND. As it will be explained below, semantic security is
denoted IND-CPA.

Note that our deﬁnition does not provide so much information to the adversary
who only has the public key and a ciphertext. So he cannot do anything but encrypt
messages himself. We call this a CPA adversary as for “chosen plaintext attack.” The
semantic security notion is thus denoted IND-CPA.

Semantic security generalizes the notion of “bit security” to all bits. Indeed, if
there is any function bit from the plaintext space to {0, 1} such that bitdec(y) is easily
computable without the secret key, where bitdec is deﬁned by bitdec(y) = bit(Dec(y))
then we can mount an attack that can win in the semantic security game: the adversary
selects two random plaintexts x0 and x1 such that bit(xi ) = i for i = 0, 1 and deduce b
from b = bitdec(y).

Note that all notions of security require that the encryption is nondeterministic.
Indeed, if the encryption were deterministic, the adversary would easily encrypt x0 and
x1 with the public key and compare both results to y. He would be able to guess b with
a 100% chance. We can extend this notion of semantic security to adversaries that are
given more resources. First of all, we can consider adversaries who can play during
“lunch time” with a decryptor. Indeed, prior to the selection of x0 and x1, we assume
that the adversary can play with a decryption oracle as he likes. After this phase (after
lunch), the adversary has no longer access to this oracle. He selects the two plaintexts and
plays like in the previous game. This notion of adversary is denoted CCA1 and called
“nonadaptive chosen ciphertext attack.” The security notion is denoted IND-CCA1.

We have yet another adversary who is more powerful and denoted CCA2 (or
simply CCA). Here the adversary keeps access to the decryption oracle even after
having received the challenge ciphertext y. The only restriction is that he is not allowed
to send y to the oracle. This leads to the stronger security notion IND-CCA.

246

Chapter 9

Another security notion was proposed: the nonmalleability, denoted NM. We have a
security notion for NM-CPA, NM-CCA1, and NM-CCA depending on which adversary
we consider. The nonmalleability is also described by a game which is a little more
complicated than the game for semantic security. What is worth keeping in mind is that
NM-CCA and IND-CCA are equivalent and that NM-CPA and NM-CCA1 are proven
to be stronger security notions than IND-CPA and IND-CCA1 respectively. This leads
us to the following matrix of security notions.

Adversary power

CPA

CCA1

CCA

Key recovery

Weaker

Message decryption

Bit retrieval

Indistinguishability

Nonmalleability

Stronger

Resistance to the strongest attack (key recovery with chosen plaintexts) is the weakest
security model. Conversely, resistance to the weakest attack (malleability with CCA)
is the strongest security model.

9.3.8 RSA–OAEP

Based on the previous notes and the bad experience of PKCS#1v1.5, the PKCS was
updated into PKCS#1v2 with the OAEP preformatting technique. OAEP stands for
“Optimal Asymmetric Encryption Padding” and is due to Mihir Bellare and Phillip
Rogaway . We describe here the encryption scheme of PKCS#1v2.1 (Ref. [14]).

We are given a modulus N of k bytes and a hash function H which hashes to hLen
bytes. We also use a “Mask Generation Function” MGF which is indeed a family of
one-way functions such that MGF(cid:9) maps a bitstring into a string of (cid:9) bytes. In order to
encrypt a message M, we proceed as depicted in Fig. 9.10.

1. Set an optional label L associated with the message and compute H(L). (This
feature is to be used for speciﬁc applications. The default value for L, if L is
not provided, is the empty string.)
2. Set DB (as for “Data Byte”) to the concatenated string H(L)||00||···
||00||01||M where we put enough zero bytes (possibly none) in order to get
a length of k − 1 − hLen.

Public-Key Cryptography

247

seed

?
⊕

?

00

maskedSeed

H(L)

00···01

?

M

-

MGFk−hLen−1

?
⊕-

MGFhLen



?

maskedDB

Figure 9.10. OAEP Preformatting for RSA.

3. Pick a random byte string of length hLen denoted seed.
4. Set dbMask to MGFk−hLen−1(seed).
5. Set maskedDB to DB ⊕ dbMask.
6. Set seedMask to MGFhLen(maskedDB).
7. Set maskedSeed to seed ⊕ seedMask.
8. Set EM (as for “Encoded Message”) to the concatenated string 00||maskedSeed

||maskedDB. Note that this string is of length k.

9. This byte string is converted into an integer.
10. We compute the plain RSA encryption.
11. We convert the result into a string of k bytes.

Note that the OAEP padding is indeed a two-round Feistel scheme. The decryption is
then straightforward.

1. We convert the ciphertext into an integer. We reject it if it is greater than the

modulus.

2. We perform the plain RSA decryption and obtain another integer.
3. We convert back the integer into a byte string.
4. We parse the string to 00||maskedSeed||maskedDB where maskedSeed is

of length hLen.

5. Set seedMask to MGFhLen(maskedDB).
6. Set seed to maskedSeed ⊕ seedMask.
7. Set dbMask to MGFk−hLen−1(seed).
8. Set DB to maskedDB ⊕ dbMask.
9. We check that DB has the H(L)||00||···||00||01||M.
10. We output M.

248

Chapter 9

The PKCS speciﬁcations further suggest a mask generation function MGF1 which
is based on a hash function. The MGF1(cid:9)(x) string simply consists of the (cid:9) leading
bytes of

H(x||00000000)||H(x||00000001)||H(x||00000002)||···

in which x is concatenated to a four-byte counter.

The security proof of RSA-OAEP is far beyond the scope of these lecture notes.
We simply mention that it exists with the strongest security notion that we have seen,
i.e. IND-CCA, and that its signiﬁcance was subject to a highly technical controversy.9

9.4 ElGamal Encryption

Taher ElGamal initiated a famous family of digital signature schemes which will be dis-
cussed in Chapter 10 (see Refs. [63–65]). He also deﬁned a nondeterministic encryption
scheme based on the discrete logarithm problem. Here is how it works.

∗
p.

Public parameter: a large prime p, a generator g of Z
Setup: generate a random x ∈ Z p−1, and compute y = gx mod p.
Message: an element m ∈ Z
∗
p.
Public key: K p = y.
Secret key: Ks = x.
Encryption: pick a random r ∈ Z p−1, compute u = gr mod p and v = myr mod
p. The ciphertext is (u, v).
Decryption: Extract the u and v parts of the ciphertext and compute m =
vu

−x mod p.

(See Fig. 9.11.) Obviously, the correct encryption followed by a correct decryption
recovers the correct message.

One beneﬁt of this scheme is that a single prime number can be used for all users.
Hence key setup, encryption, and decryption take O(s3) time where s = log p. The
drawback is that the ciphertext size is twice the plaintext size.

For security analysis we distinguish the decryption problem from the key recovery

problem, as for the RSA cryptosystem.

EGDP (ElGamal Decryption Problem)

∗
Parameters: a prime number p and a generator g of Z
p
Input: a ciphertext (u, v) and a public key y

9 See Refs. [25, 26, 71, 169]. A nice survey is available in Ref. [120].

Public-Key Cryptography

249

Adversary



Plaintext

m

-

Encryption

Ciphertext
(gr, myr)

-

-

Decryption

(u,v)

-

−x

vu

6

y

AUTHENTICATED

Public key
y

6
Secret key
x

y = gx



-

Generator

Figure 9.11. ElGamal Cryptosystem.

Problem: compute m such that there exists r such that u = gr mod p and v =
myr mod p

EGKRP (ElGamal Key Recovery Problem)

∗
Parameters: a prime number p and a generator g of Z
p
Input: a public key y
Problem: compute x such that y = gx mod p

We notice that being able to decrypt (i.e. solving EGDP) means being able to compute
−x from u and v. This implies being able to compute ux as v/m. Obviously, being
vu
able to compute ux from u and gx is equivalent to the Difﬁe–Hellman problem as
deﬁned on p. 234 DHP with G = Z
∗
p.

Let us formally prove that given the public parameters p and g, ElGamal decryption
EGDP is equivalent to the Difﬁe–Hellman DHP problem. First of all, we assume that
we have an oracle which solves the Difﬁe–Hellman problem: given A = ga mod p and
B = gb mod p, the oracle computes gab mod p. We can use this oracle in order to
decrypt (u, v) with the public key y. For this we simply give A = y and B = u to the
oracle. We obtain gxr mod p from the oracle which is ux mod p, so we can compute
m = vu

−x mod p.

Second, we assume that we have a decryptor oracle which given (u, v) and y =
−x mod p. We can use this oracle in order to solve the Difﬁe–
∗
p at
− log B mod p from which we deduce Alog B mod p.

gx mod p computes vu
Hellman problem with A and B: we simply give u = A, y = B, and take v ∈ Z
random, and we get v A

Similarly, we can prove that the ElGamal key recovery EGKRP problem is
equivalent to the discrete logarithm with known order DLP problem as deﬁned
p. (For G = Z
on p. 160 with G = Z
∗
∗
p with a prime p, DLP and DLKOP are
equivalent.)

250

Chapter 9

Here is the list of reductions that we have proven.
EGDP ⇐ EGKRP
#
#
DHP ⇐ DLP

Note that we have variants of the ElGamal encryption with the exclusive or ⊕
instead of the multiplication for v: we can deﬁne v = m ⊕ (yr mod p). This is typically
used when adopting the ElGamal encryption over elliptic curves.

9.5 Exercises
Exercise 9.1. Let N = pq be the product of two odd primes p and q such that 3 divides
ϕ(N ).

1. Under which condition on p mod 3 and q mod 3 does 3 divide ϕ(N )?
∗
2. We call cubic residue modulo N an element x of Z
N such that there exists y
such that x ≡ y3 (mod N ). Let CRN denote the set of all cubic residues modulo
N .
Given a cubic residue x ∈ CRN , how many cubic roots of x do we have
when p ≡ 1 (mod 3) and q ≡ 2 (mod 3)? How many cubic roots do we have
when p ≡ q ≡ 1 (mod 3)?
Let y be one cubic root of x. We pick z uniformly at random in the set of all
cubic roots of x. Show how y − z may lead to the factorization of N ? What is
the probability?
3. Let us assume that we have an oracle which for any x ∈ CRN outputs one cubic
root. Show that we can use it in order to factorize N in polynomial time in log N .
Deduce that computing cubic roots modulo N is equivalent to factorizing N
when 3 divides ϕ(N ).

Exercise 9.2 (Common modulus). Let us assume that A and B use RSA public keys
with the same modulus N but different exponents e1 and e2.

Prove that A can decrypt messages sent to B.
Prove that C can decrypt a message sent to A and B provided that gcd(e1, e2) = 1.10

Exercise 9.3. We want to set up the RSA cryptosystem in a network of n users. How
many prime numbers do we have to generate?

We want to reduce this number by generating a smaller pool of prime numbers and
making combinations of two of these primes: for each user, we pick a new pair of two
of these primes in order to set up his key. Show how one user can factorize the modulus
of some other user.

10 This exercise was inspired by Ref. [172].

Public-Key Cryptography

251

Show how anyone can factorize all moduli for which at least one prime factor has

been used in at least one other modulus.
Exercise 9.4 (Small exponent and known variations). Let us assume that e = 3 and
that we obtain the encryption of two unknown messages x1 and x2 for which we know
that x2 − x1 = δ. Show that we can decrypt both messages by solving polynomial
equations.

Extend this attack for x2 = P(x1) where P(X) is a polynomial of low degree.11

Exercise 9.5 (Rabin cryptosystem). Let us consider the following cryptosystem.

Setup: ﬁnd two prime numbers p and q, set N = pq, and pick a random B ∈ ZN
Messages: x ∈ ZN
Public key: B, N
Secret key: B, p, q
Encryption: E(x) = x(x + B) mod N
Decryption: D(y) is one of the four square roots of B2
4

+ y minus B

2

Explain how we can compute the square roots in ZN .

Show that we can make decryption deterministic by adding redundancy in the
plaintexts.

Show that the cryptosystem can be broken by a chosen ciphertext attack.

Show that the ability to decrypt is equivalent to the ability to factorize N .12

Exercise 9.6. Let p be a prime number such that p ≡ 3 (mod 4). Let g be a generator
p. Given y = gx mod p (with x unknown), how can we efﬁciently compute the
∗
of Z
parity bit x mod 2 of x?

Show that if y is a quadratic residue modulo p, then y p+1

4 mod p is a square root
of y. What is the link between the discrete logarithm of y and the discrete logarithm of
y p+1

4 mod p?
We assume that we are given an oracle O which for any y = gx mod p outputs
(cid:29) mod 2 of the logarithm of y. Show how to efﬁciently compute

the second parity bit (cid:28) x
discrete logarithms by using O.

2

Deduce that for p ≡ 3 (mod 4), computing the second parity bit of the discrete
logarithm is polynomially equivalent to computing the discrete logarithm in terms of
complexity.

11 This exercise was inspired by Ref. [50].
12 This exercise was inspired by Ref. [153].

10
Digital Signature

Content

RSA signature: PKCS, ISO/IEC 9796
ElGamal signature family: ElGamal, Schnorr, DSS, ECDSA
(cid:1)Attacks on ElGamal signatures: existential forgery, Bleichenbacher attack
(cid:1)Provable security: interactive proofs, random oracle model

As public-key cryptosystems are the asymmetric alternative to conventional en-
cryption, there is an alternative to MAC algorithms which is the notion of digital
signature. With it, a secret key is given to each person, and a corresponding public key
is released. The person can sign any document, and anybody else can verify the cor-
rectness of the signature. We have already seen in Chapter 5 several signature schemes
based on conventional cryptography: the Lamport scheme and the Merkle scheme based
on hash trees. We study other schemes based on asymmetric cryptography techniques
in this chapter.

10.1 Digital Signature Schemes

Formally, a public-key signature scheme consists of

Ks;

r a pseudorandom key generator which generates a public key K p and a secret key
r a signature algorithm which from each message X and the secret key Ks computes
r a veriﬁcation algorithm which from each message X, signature σ , and public

(in a deterministic or a probabilistic way) a signature σ ;

key K p veriﬁes (in a deterministic way) the correctness of the signature

(see Fig. 10.1).

Digital signatures face several security issues.

1. They must provide authenticity and integrity. For this, it must be impossible for
anyone who does not have access to the secret key to forge a (X, σ ) pair which
is valid for the public key. This is called a signature forgery.

Of course this assumption must remain valid even when the adversary gets
several valid (Xi , σi ) pairs, or even when the adversary can choose the Xi and
play with the signer as with a black box.

254

Chapter 10

Adversary

-



Message

X

6σ

X,σ

X

?

Signature

6

Secret key
Ks

Public key
Kp

AUTHENTICATED

Generator

Figure 10.1. Digital signature.

-

X

-

X
?

σ
?

Verification

6

Kp

We distinguish several classes of attacks which are listed below from the

strongest to the weakest.

key which can be used to forge signatures.

r Total break: From a public key, the adversary manages to recover the secret
r Universal forgery: From a public key, the adversary manages to derive an
r Selective forgery: The adversary can generate messages X such that she can,
r Existential forgery: From a public key, the adversary is able to create a pair

from a public key, forge a signature for the selected message. (Note that the
target message selection is made prior to the knowledge of the public key.)

algorithm which makes it feasible to forge the signature of any message (or
random ones).

made by a message X and a forged signature, but has no control on which X
is output from the attack.

The strongest security requirement corresponds to security against the weakest
attack, i.e. the existential forgery attack. This security model was ﬁrst proposed
by Shaﬁ Goldwasser, Silvio Micali, and Ronald Rivest (see Refs. [79, 80]).
Conversely, total break is the strongest attack model. So security against this is
the weakest security model.

Note that in all attack models we must also discuss the power and access
capabilities of the adversary: Is the adversary bounded in space complexity or
time complexity? Can the adversary obtain samples of signatures, or query a
signing oracle with selected messages? This leads us to similar discussions as
for public-key cryptosystems (see Section 9.3.7).

2. They must provide nonrepudiation. It must be impossible for the legitimate
signer to repudiate his signature. When a signed message (X, σ ) is valid, the
signer cannot claim that the signature was forged. Indeed, if the signature scheme
is secure according to the previous criterion, it is impossible for an adversary
to have forged this message, so the signature cannot have been created by

Digital Signature

255

someone else but the secret key holder. Note that this implies that the key
holder is restricted to a single person, which is a critical legal issue in the case
of signatures. (Encryption does not have the same legal problem.)

Here nonrepudiation relies on the security assumption, and on the hypoth-
esis that no attack is feasible. In most cases we cannot prove that no attack
is feasible but only reduce the forgery problem to some other problem we do
not know how to solve (yet). In some special signature schemes which can be
used in paranoid cases where we do not want to rely on this unstable assump-
tion, an additional protocol makes it feasible for a signer to formally prove
that a signature was forged when it happens to be the case. The denial proto-
col must be such that it is infeasible for the signer to repudiate a signature he
created. Most of the time we cannot prove that it is infeasible. We can only
argue that it must be hard. But here the debatable assumption is in favor of the
signer and not on the veriﬁer, which may be more suitable depending on legal
constraints.

10.2 RSA Signature

The original RSA paper (Ref. [158]) proposed a way to use the RSA algorithm as
a digital signature scheme. The relationship between public-key cryptosystems and
digital signatures is however quite general and not speciﬁc to RSA.

10.2.1 From Public-Key Cryptosystem to Digital Signature

Assuming that we have a public-key cryptosystem with a key generator, an encryption
algorithm which is deterministic, and a decryption algorithm, we can sign a message
by decrypting it. Veriﬁcation simply consists of encrypting it. Note that here we have
a signature with message recovery: we do not need to send X together with σ since X
can be extracted from σ . So we can replace the veriﬁcation algorithm by an extraction
algorithm. This case is depicted in Fig. 10.2.

Message

-

X

Sign

6

Signature

--

σ

Adversary



-

σ

Extraction

-

X

6

Kp

Secret key
Ks

Public key
Kp

AUTHENTICATED

Generator

Figure 10.2. Digital signature with message recovery.

256

Message

X

X

?

Hashing

-d

-

Adversary



-

6σ X,σ

σ
?

Chapter 10

-

X

X
?

Decryption

Encryption

Hashing

Secret key
Ks

Public key
Kp

AUTHENTICATED

Kp

6

d

d

U



Compare

-

Generator

Figure 10.3. Encryption to signature.

In order to limit the size of the signature, we can use a cryptographic hash function
before signing. This hash function must however be collision-resistant (otherwise we
can have the same signature for two different messages, so a chosen message attack
with the ﬁrst message makes a signature forgery on the second message).

Fig. 10.3 represents the generic transformation of public-key encryption into a
signature scheme. Note that it does not provide message recovery. In the following, we
however strongly discourage to talk about encryption instead of signature, since the
threat model is totally different.

10.2.2 On the Plain RSA Signature
Let K p = (N , e) and Ks = (N , d) be an RSA key pair for signature. The signature of
a message m by using the plain RSA algorithm is simply σ = md mod N , as depicted
in Fig. 10.4. This suffers from several problems which are mainly due to the RSA
properties.

First of all, it is easy for anyone to pick a random σ and to construct m = σ e mod N
which makes (m, σ ) a valid pair. This is an existential forgery: we can forge a valid
pair, but we have no control on the meaning of the forged message m.
(cid:8), σ (cid:8)

Second, we can easily modify two valid pairs (m, σ ) and (m

(cid:8)(cid:8), σ (cid:8)(cid:8)

), for instance
) is then a new valid pair.

(cid:8)(cid:8) = mm

(cid:8)

mod N and σ (cid:8)(cid:8) = σ σ (cid:8)

by taking m
This comes from the property of malleability of the RSA algorithm.

mod N . (m

These security issues can be ﬁxed by cryptographic hash functions. We must at

least adapt the plain RSA system in order to prevent these attacks.

Digital Signature

257

Adversary

Message

m

-

Signature

-



md mod N

-

σ

Extraction

-

σe mod N

6

Secret key
d,N

Public key
e,N

AUTHENTICATED

6

e, N

Generator

10.2.3

ISO/IEC 9796

Figure 10.4. Plain RSA signature scheme.

The ISO/IEC 9796 (Ref. [10]) was the ﬁrst international norm which speciﬁed a mes-
sage formatting scheme to be used in order to feed a given signature scheme.1 The
original norm does not specify the signature scheme, but it is RSA in most applications
(it can also be the Rabin scheme).

The signature of a d-bit message m into a k-bit signature proceeds as follows. (Let

us consider that d ≤ 512 and k = 1024 for instance.)

1. We pad m with leading zero bits (at most seven) so that the total length can be

cut into a sequence of z bytes mz, mz−1, . . . , m1.

2. We extend the message by taking the t rightmost bytes of the inﬁnite sequence

. . . , mz, mz−1, . . . , m1, mz, mz−1, . . . , m1, . . . , mz, mz−1, . . . , m1
where t is the smallest size such that 16t ≥ k − 1. (Hence t = 64 bytes.)

3. We add redundancy by inserting a byte S(x) to the left of each of the t bytes x of
the string, and XOR r onto the z-th rightmost redundancy byte S(mz). We thus
obtain a string of 2t bytes, which consists of at least k − 1 bits. Here r is one plus
the number of zero bits which have been padded at the beginning (hence between
1 and 8), and S is the shadow function deﬁned by S(x H xL) = π(x H )π(xL) where
(cid:17)
x H xL represents the two hexadecimal digits of x, and π is a permutation deﬁned
by
π =

(cid:16)

4 5
9 4 2 F 0 D B 6

A B C D E F
A C 1

4. We take the k − 1 rightmost bits and we pad a one bit to the left and replace the
rightmost byte x = x H xL by xL6 in hexadecimal. We thus obtain a formatted
string of k bits.

1

0
E 3 5

2 3
8

7

.

6

7

8

9

5. We sign the formatted string, for instance by using the plain RSA.

1 Note that it was originally published in 1991.

258

Chapter 10

The extraction scheme is thus performed as follows.

1. We open the signature. (In case of RSA, we raise the signature to the public

exponent.)

2. We ﬁrst check that the signature is of length k and that the rightmost hexadecimal

digit is 6.
3. We perform a message recovery: we remove the leading bit 1, replace the right-
most two bytes yH yR x H x R by yH yRπ−1(yH )x H , obtain . . . , x2, x1, take z as
the smallest index such that x2z ⊕ S(x2z−1) (cid:4)= 0 (reject if it does not exist) and
set r equal to this value (and check that r ≤ 8), extract x2z, x2z−2, . . . , x2, and
remove the r − 1 leftmost bits (reject if they are not equal to zero). We must
obtain a message m.

4. Check that the formatting scheme on m leads to the value obtained after opening

the signature. (Check the redundancy.)

One important property of this scheme is the message recovery: as long as the message
m is of length at most d, we do not need to send it with the signature σ : the veriﬁcation
process enables the recovery of m.

As an example, let us consider the message “PAY 1’000’000.-CHF” with

k = 512. The message (of 18 characters) turns in hexadecimal into

A Y

P
H
50 40 59 20 31 27 30 30 30 27 30 30 30 2e 2d 43 48

0 0

0

1 ’

0

0

.

’

0

–

C

F
46

5040 5920312730303027 3030302e2d434846. We

hence
and we need t = 32 bytes. So we take
3127303030273030 302e2d434846||5040 5920312730303027 3030302e2d434846
i.e. the message plus an extra 14 bytes. We then add the S redundancy and get

have

z = 18

83315f278e308e30 8e305f278e308e30 8e305c2era2d9843 904892464e509e40
4d595e2083315f27 8e308e308e305f27 8e308e308e305c2e 5a2d984390489246

(note that the z-th redundancy byte is 4e as S(50)) and XORing the z-th redundancy
byte to r = 1 we obtain

83315f278e308e30 8e305f278e308e30 8e305c2era2d9843 904892464f509e40
4d595e2083315f27 8e308e308e305f27 8e308e308e305c2e 5a2d984390489246

and the ﬁnal operation leads to

83315f278e308e30 8e305f278e308e30 8e305c2era2d9843 904892464f509e40
4d595e2083315f27 8e308e308e305f27 8e308e308e305c2e 5a2d984390489266

which can now feed the plain RSA signature scheme.

Digital Signature

259

10.2.4

(cid:1)Attack on the ISO/IEC 9796 Signature Scheme

A ﬁrst surprising attack has been found by Jean-S´ebastien Coron, David Naccache,
and Julien Stern in Ref. [51] on a slightly modiﬁed variant of the ISO/IEC 9796 norm:
let us assume that we remove the XOR to r in the third step of the signature. Let us
further assume that k is a multiple of 64. For an original message of d = k
2 bits, we
16 . We write m = mz ··· m2m1. If m1 = m1H m1L is the least signiﬁcant
have t = z = k
byte, we write u = m1L||6 in hexadecimal. The formatted string is

((S(mz) ⊕ 1) OR 80)||mz||S(mz−1)||mz−1||···||S(m2)||m2||S(m1)||u.

With the XOR to r removed, following our assumption, the formatted string is

(S(mz) OR 80)||mz||S(mz−1)||mz−1||···||S(m2)||m2||S(m1)||u.

Now if mz is chosen so that the leftmost bit of S(mz) is set to 1, and if we choose
m1 = 66, the formatted string is

S(mz)||mz||S(mz−1)||mz−1||···||S(m2)||m2||S(m1)||m1.

We notice that z is a multiple of 4. So we can choose a special message m of the form

m = m4m3m2m1||m4m3m2m1||···||m4m3m2m1.

We still assume that m4 is such that the leftmost bit of S(m4) is set to 1, and that
m1 = 66. The formatted string is now
S(m4)||m4||S(m3)||m3||S(m2)||m2||2266||···||S(m4)||m4||S(m3)||m3||S(m2)||m2||2266.

Let us write

x = S(m4)||m4||S(m3)||m3||S(m2)||m2||2266

and  = 1 + 264 + 2128 + ··· + 2k−64. The formatted string is nothing but x × ,
where x is a 64-bit integer. We have 223 messages mi of this type for which the
formatted string is xi × . We can estimate the probability that all prime factors of
−7.7. Therefore, we may have more than 215 messages for
xi are less than 216 to be 2
which the xi factorizes into primes which are less than 216. We can prove that with less
than 200 such messages, we are likely to ﬁnd four messages mg, mh, mi , m j such that
xg × xh = xi × x j . In this case, the signature of message m j is simply the multiplica-
tion of the signatures of mg and mh divided by the signature of mi . We can thus forge
a new signature with the signature of mg, mh, mi !

This attack was later transformed and improved into an attack against the real

ISO/IEC 9796 norm (see Refs. [51, 81]).

260

10.2.5 PKCS#1

Chapter 10

The PKCS (Public-Key Cryptography Standards) that we saw in Chapter 9 also includes
a signature scheme based on RSA. Here we give details about the signature scheme of
the PKCS#1v1.5 standard (Ref. [13]).

We are given a modulus n of k bytes. In order to sign a message, we proceed as

follows.

1. We hash the message, for instance with MD5, and get a message digest.
2. We encode the message digest and the identiﬁer of the hash algorithm.
3. We pad it with a zero byte to the left, then with many (at least 8) FF bytes in
order to reach a length of k − 2 bytes, then with a 01 byte. We obtain k − 1
bytes.

4. This byte string is converted into an integer.
5. We compute the plain RSA signature.
6. We convert the result into a string of k bytes.

The veriﬁcation is then straightforward.

1. We convert the signature into an integer. We reject it if it is greater than the

modulus.

2. We perform the plain RSA veriﬁcation and obtain another integer.
3. We convert back the integer into a byte string.
4. We check that the string has the 00||01||FF··· FF||00||D format for a byte

string D.

5. We decode the data D and obtain the message digest and the hash algorithm.

We check that the hash algorithm is acceptable.

6. We hash the message and check the message digest.

The PKCS#1v2.1 (Ref. [14]) includes another signature scheme which uses the
padding scheme called PSS (as for Probabilistic Signature Scheme), which is similar
to the OAEP (Optimal Asymmetric Encryption Padding) (see Section 9.3.8).

10.3 ElGamal Signature Family

In his PhD, Taher ElGamal studied the application of the discrete logarithm in cryp-
tography (see Refs. [63–65]). The ElGamal signature started a dynasty of signature
schemes based on the discrete logarithm problem.

10.3.1 ElGamal Signature

The original ElGamal signature scheme is deﬁned as follows. We use a cryptographic
hash function H.

Digital Signature

Adversary

-



Message

M

6
r,s

M,r,s

M

?

k ∈ Z∗
p−1
r = gk mod p
s = H(M)−xr

k

mod p− 1

Signature

6

Secret key
x

Public key
y

AUTHENTICATED

y = gx mod p

Generator

Figure 10.5. ElGamal signature scheme.

261

-

M

M
?

r,s
?

Verification

6

y

0 ≤ r < p
yrrs ≡ gH(M)

-

(mod p)

∗
Public parameters: a large prime number p, a generator g of Z
p .
Setup: generate a random x ∈ Z p−1 and compute y = gx mod p.
Secret key: Ks = x.
Public key: K p = y.
Message digest: h = H(M) ∈ Z p−1.
Signature generation: pick a random k ∈ Z
Veriﬁcation: check that yrr s ≡ gh (mod p) and 0 ≤ r < p.

k mod p − 1, the signature is σ = (r, s).
h−xr

p−1, compute r = gk mod p and s =
∗

(See Fig. 10.5.) We ﬁrst prove that the signatures are correct: if σ was correctly com-
puted, we have 0 ≤ r < p and

yrr s ≡ gxr+ks ≡ gh

(mod p).

Now we can study the security of this scheme: the difﬁculty to forge valid signatures.
A signature forgery consists of ﬁnding r and s such that yrr s ≡ gh (mod p).
This is quite easy if we know how to compute the discrete logarithm of y (which is
actually the secret key). It is quite easy as well, in general, if we forget the requirement
0 ≤ r < p. Indeed, one can just pick α ∈ Z p−1 and β ∈ Z
p−1 at random, take r p =
∗
yαgβ mod p, s = h/β mod ( p − 1), and r p−1 = −sα mod ( p − 1), then reconstruct
r such that r mod p = r p and r mod ( p − 1) = r p−1 by using the Chinese Remainder
Theorem.

One problem with the plain ElGamal scheme concerns the signature length. As-
suming that p is of length 1024 bits, it means that σ is of length 2048 bits (256 bytes),
which is quite long.

262

Chapter 10

10.3.2

(cid:1)The Bleichenbacher Attack against the ElGamal Signature

We describe here an attack against the ElGamal signature which works for special
public parameters. The attack is due to Daniel Bleichenbacher (see Ref. [33]).

Let p and g be the parameters in the ElGamal signature. Let us assume that
p − 1 = bw with an integer b which is smooth (namely such that all its prime factors
are small). As an example, we can just have b = 2, which works for every odd prime
p. Let us further assume that we know some relation g1/t mod p = cw. This is a
reasonable assumption if we have g = b (note that the complexity of the exponentiation
is decreased if g is small) and p ≡ 1 (mod 4) since we can check that the relation holds
for t = p−3

2 and c = 1: we have
p − 1
g

(cw)t ≡

(cid:16)

(cid:17) p−1

2

−1 ≡ −g

(−1) p−1
g p−1

2

2

≡ g

(mod p)

since g p−1
and (−1) p−1

2

is a square root of 1 which is not 1 (otherwise g would not be a generator)
2 = 1 due to the assumption on p mod 4.

p = cw, anyone can forge a signature for any message digest h.

We will show now that with the assumption that p−1
r First take r = cw.
r Find z such that ywc ≡ gwcz (mod p). This is nothing but the discrete loga-

= b is smooth and g1/t mod

w

rithm of ywc mod p, in basis gwc mod p, which spans a group of order fac-
tor of b. Thus the Pohlig–Hellman algorithm works, thanks to the assumption
on b.

r Take s = t(h − cw z) mod ( p − 1).
r Yield the signature (r, s).

We only have to prove that the signature is valid. First we check that 0 ≤ r < p. Next
we have

yrr s ≡ ycw (cw)t(h−cw z) ≡ ycw gh−cw z ≡ gh

(mod p).

Therefore the signature is valid.

The main idea here is to simplify the discrete logarithm problem by rais-
the underlying subgroup becomes

ing everything to the power w, so that
smooth.

We can get rid of this kind of attack by using subgroups of prime order.

Adversary

-



Digital Signature

Message

M

6
e,s

M,e,s

M

?

q

k ∈ Z∗
r = gk mod p
e = H(M||r)
s = ex + k mod q

Signature

6

263

-

M

M
?

e,s
?

Verification

6

-
−e mod p )

compare e and
H (M||gsy

Secret key
x

Public key
y

AUTHENTICATED

y

y = gx mod p

Generator

Figure 10.6. Schnorr signature scheme.

10.3.3 Schnorr Signature

In 1989, Claus Schnorr presented a new variant of the ElGamal signature which over-
comes the drawbacks that we have seen: a long signature size and the Bleichenbacher
attack (but this attack was unknown at that time). (See Ref. [160, 161]). Here is a short
description of the Schnorr signature.

Public parameters: pick a (not-too-large, but large enough) prime number q, a large
∗
p whose a-th power is denoted g

prime number p = aq + 1, a generator of Z
(an element of order q).
Setup: pick x ∈ Zq and compute y = gx mod p.
Secret key: Ks = x.
Public key: K p = y.
Signature generation: pick a random k ∈ Z
Veriﬁcation: check that e = H(M||gs y

−e mod p).

H(M||r), and s = ex + k mod q, the signature is σ = (e, s).

q, compute r = gk mod p, e =
∗

(See Fig. 10.6.) Here H is a hash function (whose output is smaller than the size of q)
and M||r denotes the concatenation of M and r.

We can see here that the signature complexity is very cheap when (k, r) are pre-
computed. In addition, the size of the signature is a message digest size plus a modulo
q number which can be quite short. We can thus have signatures of size less than
300 bits.

264

Chapter 10

-

M

-

compare r and
g

H(M)

s

y r
s mod p mod q

Adversary

-



Message

M

6
r,s

M, r,s

M

?

q

k ∈ Z∗
r = gk mod p mod q
s = H(M)+xr
mod q

k

Signature

6

Secret key
x

Public key
y

AUTHENTICATED

y = gx mod p

Generator

10.3.4 The Digital Signature Standard (DSS)

Figure 10.7. DSA.

M
?

r, s
?

Verification

6

y

DSS (as for Digital Signature Standard) was published as Ref. [7] by NIST, a branch
of the American department of commerce, in 1994. It includes a Digital Signature
Algorithm (DSA) which is very similar to the Schnorr signature, which led to a juridical
controversy.

Public parameters: pick a 160-bit prime number q, a large prime number p =
∗
p whose a-th power is denoted g (an element of

aq + 1, a generator of Z
order q).

Setup: pick x ∈ Zq and compute y = gx mod p.
Secret key: Ks = x.
Public key: K p = y.
Signature generation: pick a random k ∈ Z
and s = H(M)+xr

(cid:18)

(cid:19)
mod q, the signature is σ = (r, s).
s mod q mod p

s mod q y r

g H(M)

Veriﬁcation: check that r =

k

mod q.

q, compute r = (gk mod p) mod q,
∗

(See Fig. 10.7.) Here H is the standardized hash function SHA-1 which hashes onto
160 bits.

The signature is still quite short: 320 bits in total. The main difference with the
Schnorr signature is the removal of the r in the message to be hashed. This leads to
such that q = H(M) −
a security problem when we know two messages M and M
H(M

). (See Ref. [180]).

(cid:8)

(cid:8)

10.3.5

(cid:1)ECDSA

ECDSA (as for Elliptic Curve Digital Signature Algorithm) is yet another variant
of the ElGamal signature. It is dedicated to elliptic curves and is directly adapted

Digital Signature

265

from DSA. It is a standard from several organizations including NIST and ANSI (see
Refs. [3, 8]).

Public parameters: we use ﬁnite ﬁelds of two possible types: either a ﬁeld of
characteristic two or a large prime ﬁeld. The public parameters consist of the
ﬁeld cardinality q, the selected ﬁeld representation (in the characteristic two
case, i.e. an irreducible polynomial over Z2), an elliptic curve deﬁned by two
ﬁeld elements a and b, a prime number n larger than 2160, and an element
G of the elliptic curve of order n. The elliptic curve equation over GF(q) is
y2 + x y = x 3 + ax 2 + b in the characteristic two case and y2 = x 3 + ax + b
in the prime ﬁeld case. Public parameters are subject to many security criteria.
Setup: pick an integer d in [1, n − 1], compute Q = dG. Output (K p, Ks) =
Signature generation: pick k in [1, n − 1] at random and compute

(Q, d).

(x1, y1) = kG

r = x1 mod n
s = H(M) + dr

k

mod n

Here x1 is simply a standard way to convert a ﬁeld element x1 into an integer.
If r = 0 or s = 0, try again. Output the signature σ = (r, s)
Veriﬁcation: check that Q (cid:4)= O, Q ∈ C, and n Q = O. Check that r and s are in
[1, n − 1] and that r = x1 mod n for (x1, y1) = u1G + u2 Q, u1 = H(M)
s mod
n, and u2 = r

s mod n.

(See Fig. 10.8). The H hash function is the same standard hash function as usual, i.e.
SHA-1.

Adversary

Message

M

M

?

-

6
r,s

M,r,s

n

k ∈ Z∗
r = (k.G)1 mod n
s = H(M)+dr
mod n

k

Signature

6

Secret key
d

Public key
Q

AUTHENTICATED

Q = d.G mod p

Generator

Figure 10.8. ECDSA.

-

M

-

mod n

(cid:12)

1

(cid:11)

compare r and
s Q

H(M)
s G + r

M
?

r,s
?

Verification

6

Q

266

Chapter 10

Adversary

-



Message

M

6
r,s

M,r,s

M

?

q

k ∈ Z∗
r = gk mod p mod q
s = H(r||M)+xr
mod q

k

Signature

6

Secret key
x

Public key
y

AUTHENTICATED

y = gx mod p

Generator

M
?

r,s
?

Verification

6

y

-

M

-

compare r and
H(r||M)
g

s

y r
s mod p mod q

Figure 10.9. Pointcheval–Vaudenay signature scheme.

The difﬁculty with ECDSA is to deal with objects of many different types: elliptic
curve points, ﬁeld elements, integers, and of course bitstrings. The standard provides
extensive details about how to represent and manipulate them.

10.3.6 Pointcheval–Vaudenay Signature

The Pointcheval–Vaudenay signature is yet another variant of DSS with a slight mod-
iﬁcation: we just have to compute H(r||M) instead of H(M) (see Refs. [38, 147]).
(See Fig. 10.9). This may look as a minor modiﬁcation. It is actually not: with this
modiﬁcation, we can prove that the signature scheme is secure provided that

1. H behaves like a random function,
2. the discrete logarithm is hard,
3. k (cid:11)→ (gk mod p) mod q is a one-way function.

(The last two conditions are actually necessary. The ﬁrst one is still a reasonable assump-
tion, but may be too strong.) None of the previous signature schemes had this strong
security result. As we will see below, the security of the Schnorr signature is close to
being formally proven under the hypothesis of the hardness of the discrete logarithm.

This variant was added in the ISO/IEC 14888 norm (Ref. [12]).

10.4 (cid:1)Toward Provable Security for Digital Signatures

10.4.1

(cid:1)From Interactive Proofs to Signatures

We have seen in Chapter 5 access control protocols with challenge and response. Similar
protocols exist with asymmetric keys. They can be transformed into signature schemes.

Digital Signature

A

pick k, r = gk mod p

s = ex + k mod q

267

B

r||certificate for y

−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−− pick e, 1 ≤ e ≤ 2t
−−−−−−−−−−−−−−−−−−−→

e

s

check rye ≡ gs

(mod p)

Figure 10.10. The Schnorr identiﬁcation protocol.

For instance, the Schnorr signature comes from the Schnorr identiﬁcation protocol.
This protocol is actually performed by the owner of the secret key in order to convince
someone that he knows the secret key related to some public key. When we have
a certiﬁcate from a trusted authority, stating that the public key is linked to some
identity, we have an identiﬁcation protocol. It works as follows.

Public parameters: p, q, g as in the Schnorr signature.
Setup: compute Ks = x and K p = gx mod p as in the Schnorr signature.
Interactive proof : in order to prove that A knows Ks to B, they proceed as follows
(see Fig. 10.10).
1. A picks k at random, computes a commitment r = gk mod p and sends it
2. B picks a challenge e at random such that 1 ≤ e ≤ 2t ,
3. A computes the response s = ex + k mod q and sends it to B,
4. B checks that r ye ≡ gs (mod p).

to B,

The connection with the Schnorr signature is straightforward: the challenge e is simu-
lated by a hash function with the commitment r and the message.

Let us ﬁrst consider a simple case where a honest B always accepts when interacting
with A. This means that A can compute the response to all potential challenges e, in
particular to any two different e1 and e2. We can thus transform A into an extractor for
the discrete logarithm x of y as follows. Let A generate r, and let us send her e1. We
obtain a successful response s1 in return. Let us now restart A from the same internal
state in order to generate r again, and let us now submit the challenge e2. We obtain a
successful response s2 in return. Obviously we have

r ≡ gs1 y

−e1 ≡ gs2 y

−e2

(mod p).

So s1−s2
mod q is a discrete logarithm of y in basis g. This means that if A successfully
e1−e2
passes all identiﬁcation challenges then she must be able to compute the discrete
logarithm of y. Therefore there is no way for a malicious cheater to impersonate A
to B with 100% chances without the ability to solve the discrete logarithm problem.
We can prove a much stronger result, namely that there is no way to succeed with
probability P without the ability to compute x.

Here is a more precise statement.

268
Chapter 10
Theorem 10.1. For any P > 21−t , we can transform a prover which succeeds the
Schnorr Identiﬁcation Protocol with a honest veriﬁer with probability P and time com-
plexity T into an algorithm which ﬁnds the secret key x in time complexity O(T 2t /P).

This is the standard way to prove that A must know x, otherwise fail the identiﬁcation
protocol with probability at least 1 − 21−t . (Obviously, this statement makes sense
when t is small because of the 2t factor in the complexity reduction.)

The proof works as follows: let us assume that B follows the protocol correctly
and that A succeeds with probability P to pass the ﬁnal check. Clearly A must output
a random r which only depends on the initial (random) state ρ of A (so let us denote
it r(ρ)), and a value s which depends on some e and on ρ (so let us denote it s(ρ, e)).
The probability is over the distribution of ρ and the distribution of e. Let Pr[ρ] be the
probability of having ρ as an initial state. Let 1S(ρ,e) be 1 if the ﬁnal check succeeds
with r(ρ), e, and s(ρ, e), and 0 if it fails. We have

(cid:1)

ρ,e

(cid:1)

P =

−t 1S(ρ,e) Pr[ρ].
2

P =

−t N (ρ) Pr[ρ].
2

Let N (ρ) denote the number of possible e’s for which the protocol succeeds. We have

Clearly, by splitting the sum for N (ρ) ≤ 1, and 2t ≥ N (ρ) ≥ 2, we have

ρ

Therefore we have

hence Pr[N (ρ) ≥ 2] ≥ P−2

−t

P ≤ 2

−t Pr[N (ρ) ≤ 1] + Pr[N (ρ) ≥ 2].
−t +(cid:12)
P ≤ 2
1−2−t ≥ P

2 . We transform A as follows.

Pr[N (ρ) ≥ 2]

1 − 2

−t

(cid:13)

1. We generate r from a random ρ.
2. For all e’s we compute s(ρ, e) and we check if this succeeds.
3. If we have less than two succeeding e’s, we try again. Otherwise, we have (e1, s1)

and (e2, s2) such that

We thus compute

r ≡ gs1 y

−e1 ≡ gs2 y
x = s1 − s2
e1 − e2

−e2

(mod p).

mod q.

Since with probability Pr[N (ρ) ≥ 2] we obtain two succeeding e’s, and that this proba-
2 , this works within a complexity ofO(2t /P) times the complexity
bility is greater than P
of A and B.

Digital Signature

269

We emphasize that the security proof of the Schnorr identiﬁcation protocol suffers

fromr being meaningful for really small values of t,
r being valid when the veriﬁer is honest only.

The ﬁrst drawback makes the security result on the signature scheme impossible, be-
cause hash functions usually output t ≥ 128 bits. The second drawback is reasonable
for a digital signature, because the message (and therefore how the challenge will be
generated out of the commitment) is decided before the commitment is made. However
in real identiﬁcation protocols, we must prove that no veriﬁer can extract any useful
information out of this protocol. This is still possible for really small t’s, and the reason
why it is possible is because we can actually cheat with probability 2

−t .

Indeed, we can make a prover A which succeeds with probability 2

−t without

knowing the secret key:

1. A picks a random e0 with 1 ≤ e0 ≤ 2t and a random s ∈ Zq,
2. A computes r = y
3. for any received challenge, A outputs s.

−e0 gs mod p and sends it to B,

Clearly this succeeds when e = e0. Therefore, if a cheater B can extract some informa-
tion in order to break the system, we can run it with this cheater and retry every time it
fails. Within a complexity factor of 2t , this simulation with the cheater will break the
scheme as well, but without knowing the secret key. Therefore, if a malicious veriﬁer
can extract some useful information from the protocol within a complexity O(T ),
then there exists an algorithm that produces the same information within a complexity
O(2t T ).

The above argument is valid, for two reasons.

r The commitment is statistically indistinguishable from a right commitment: the
−e0 gs mod p is clearly uniform in the subgroup spanned by g.
distribution of y
Therefore, making a commitment like this will not change the behavior of the
malicious veriﬁer.
−e0 gs mod p is also
clearly independent from e0 thanks to s, which is not revealed at this point.
Therefore, the success probability is independent from (r, e), and actually equal
to 2

r The challenge e cannot depend on e0: the distribution of y

−t .

We summarize all these properties by saying that the Schnorr identiﬁcation scheme is
a zero-knowledge interactive proof for small values of t. Security for higher values (in
particular for the Schnorr signature) is an open problem.

In Chapter 11, we will see other identiﬁcation protocols (like the Fiat–Shamir or
the Guillou–Quisquater protocols). Transformation into signature schemes is straight-
forward: each time the veriﬁer has to produce something, we generate it from a

270

Chapter 10

pseudorandom generator fed with the previous communications and the message to
be signed.

10.4.2

(cid:1)Security in the Random Oracle Model

A further step forward to formal proofs of security was made by Jacques Stern and David
Pointcheval. They formally proved the security of the ElGamal signature scheme in what
is called the “random oracle model”, demonstrating that it is hard for an adversary to
existentially forge signatures, even with an access to the signing oracle. Note that this
proof holds on average over the public parameter choice (i.e. the value of p and g)
and not for any choice. As a matter of fact, the result by Pointcheval and Stern was
presented at the same conference where the attack of Bleichenbacher was presented.
(The latter indeed demonstrates that for some choices of p and g it is indeed very easy
to forge a signature.) (See Refs. [33, 146]).

This proof technique was later adapted to other signature schemes. It also motivated
the Pointcheval–Vaudenay one for which the proof argument works better and for any
choice of public parameters. We provide here this demonstration.

The security proof relies on the assumption that the underlying hash function H
is replaced by a truly random oracle. This is not a practical model but an idealized
approximation of what happens in practice.

Let us formalize the Pointcheval–Vaudenay scheme with a random oracle by the

following three algorithms.

Setup → ( p, q, g, y, x): generate two prime numbers p and g such that q divides
p − 1, an integer g which spans a subgroup of Z
∗
p of order q. Pick a secret
key x ∈ Zq. Compute the public key y = gx mod p.
q, compute r = (gk mod p) mod q,
Sig(M) → (h, r, s): pick a random k ∈ Z
∗
query the oracle with r||M and obtain h = H(r||M), and compute s =
k mod q, the signature is σ = (h, r, s).
h+xr
(cid:18)
Ver(M, h, r, s): query the oracle with r||M and check that h = H(r||M) and r =
s mod q y r
g h

s mod q mod p

(cid:19)

mod q.

We will prove the following theorem.

Theorem 10.2. We consider the three algorithms Setup, Sig, and Ver as deﬁned above.
We assume that the oracle H in the above algorithms is a random oracle which outputs
elements of Zq. We let (cid:9) − 1 denote the largest preimage set size for the k (cid:11)→ (gk mod
∗
p) mod q mapping from Z
q to Zq. Let ε, T , and Q be arbitrary positive real numbers
such that ε (cid:24) Q2(cid:9)/q. We say that A is a (ε, Q, T )-adversary if it is an algorithm
which, given the output ( p, q, g, y) from Setup, can query Sig or H at most Q times
and output within a time less than T a (M, h, r, s) quadruplet such that M was never
queried to Sig, and that the probability that Ver accepts the quadruplet is greater

Digital Signature

271

(cid:13)

QT
ε

(cid:9) log (cid:9)

, and a probability of success close to 1.

O(cid:12)
than ε. We can transform any (ε, Q, T )-adversary into an algorithm which, given the
output ( p, q, g, y) from Setup, outputs x such that y = gx mod p, with a complexity
Note that if k (cid:11)→ (gk mod p) mod q looks like a random function, then we have (cid:9) =
O(log q). Indeed, for a random function over Zq, there are less than qc log q
(c log q)! possible
sets of c log q pairwise different elements and every single set is mapped onto a single
value with probability q1−c log q. So the probability that such a set exists is less than
q/(c log q)! This becomes very small for large q.
Proof. Let (M, h, r, s) be the output of A. First of all, we notice that if r||M has never
been queried to H (neither directly nor by the Sig oracle), then the probability that
h = H(r||M) is 1/q. If we replace A by an algorithm which decides to fail if r||M
was not queried, the new probability of success is greater than ε − 1/q, which can be
approximated to ε. So from now on we assume that r||M was always queried to H.

We also notice that there is no use for A to query H with some values which are
already known, so we assume that every time A sends some query to H then it was
never queried before to H and it cannot be written r||M where M was queried to Sig
before and r was a part of the output.

The ﬁrst step of the proof consists of simulating the two oracles H and Sig by some
probabilistic algorithms which produce the same statistic behavior. The simulator works
by managing a list of declared (u, H(u)) pairs. Initially, the list is empty. Every time
the adversary queries an oracle, the simulator uses the deﬁned list, may insert a new
pair, and may decide to abort the simulation. We will show that the probability to abort
is negligible. Here is how the simulator works.

let u be the query
pick a random h
insert (u, h) in the list
output h

1: if oracle H is queried then
2:
3:
4:
5:
6: else
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end if

let M be the query
pick α, β ∈ Z
∗
q at random
r ← (gα yβ mod p) mod q
s ← r/β mod q
h ← sα mod q
if there is some (r||M, h) pair in the list then
abort the simulation
else
insert (r||M, h) in the list
output (h, r, s)

end if

Chapter 10
272
Let L = ((u1, h1), . . . , (un, hn)) be the list of all known hi = H(ui ) values. Let L
∗
be the event that hi = H(ui ) for i = 1, . . . , n for the random oracle H. Obviously
Pr[output h|L] = Pr[H(u) = h|L
] when we query H with u in the simulator. It is not
more difﬁcult to realize that

∗

Pr[output (h, r, s)|L] = Pr[Sig(M) = (r, s) and H(r||M) = h|L

∗

]

(assuming that no ui is equal to r||M) when we query Sig with M in the simulator.
Therefore we deduce that except when the simulator aborts, it perfectly simulates the
two oracles. Since r is generated with probability up toO((cid:9)/q), the r||M is equal to some
ui with probability up to O(n(log q)/q). The cases which are not simulated thus happen
with probability less than O(Q2(log q)/q), which can be neglected. We conclude this
ﬁrst step of our proof by saying that the adversary A running with the simulator will
output a valid (M, h, r, s) quadruplet with probability at least ε − O(Q2(log q)/q). By
assuming that ε (cid:24) Q2(log q)/q we can simply say that it succeeds with probability at
least ε.

The second step is known as the “forking lemma.” We assume that every time the
simulator for H wishes to pick a random value, it sequentially reads it from a tape ρ
which is set up at the beginning. What we will do is run A with the simulator several
times with some modiﬁed tape. More precisely, we re-run A with a tape which starts
by the same numbers, but whose content has been overwritten starting from the value
which is read at some crucial point. Indeed, for any initial random tape ρ which leads to
a valid forgery, there is one crucial moment in the simulator when r||M was a query to
the H oracle. (Note that it cannot come from the query to the signing oracle, otherwise
it would mean that M was queried to this oracle, which is forbidden.) We number the
oracle calls from 1 to Q. Given a random tape ρ, for any i we let ρi be the preﬁx of
ρ which consists of all values which have been used before the i-th query. We let c(ρ)
be the number of the oracle call when r||M is queried to H. We let dist(ρ) be ρc(ρ).
We intend to re-run A with a new tape dist(ρ)||ρ(cid:8)
is a new random sequence.
Given a tape ρ we let succ(ρ) be the event that A succeeds when initialized with ρ,
and succc(ρ) be the event that both succ(ρ) and c(ρ) = c occur. Note that cutting all
possible ρ sequences at the oracle query times leads to a tree structure of depth Q.
Given a truncated random tape ν, we can consider the probability f (ν) that a random
tape ρ = ν||ρ(cid:8)
leads to a successful run such that dist(ρ) = ν. By using Lemma 10.3
we show that

where ρ(cid:8)

(cid:22)

Pr
ρ

f (dist(ρ)) >

(cid:28)(cid:28)(cid:28)(cid:28) succ(ρ)
(cid:23)

ε
2Q

≥ 1
2

.

In other words,

(cid:22)

Pr
ρ

Pr
ρ(cid:8)

(cid:14)

succc(ρ)(dist(ρ)||ρ(cid:8)

(cid:28)(cid:28)(cid:28)(cid:28) succ(ρ)
(cid:23)

(cid:15)

)

>

ε
2Q

≥ 1
2

.

Digital Signature
273
This means that if we pick a successful ρ, there is a very high chance that dist(ρ)||ρ(cid:8)
succeeds on the same crucial oracle call with probability at least ε
2Q . Starting from such
, and the probability to get at least one dist(ρ)||ρ(cid:8)
a ρ, we can try 2Q
tape such that succc(ρ)(dist(ρ)||ρ(cid:8)
(cid:9) . So if we iterate (cid:9) times, we obtain
(cid:9) tapes with the same crucial oracle call and the same preﬁx tape with probability at
least e

ε log (cid:9) values of ρ(cid:8)

) is at least 1 − 1

−1. To recapitulate, the algorithm works as follows.
1: Pick a random tape ρ until A succeeds
2: for i = 1 to (cid:9) do
until dist (ρ)||ρ(cid:8)
3:

try 2Q
(if this does not work the algorithm fails)

ε log (cid:9) values ρ(cid:8)

succeeds and c (dist(ρ)||ρ(cid:8)

) = c (ρ)

4: end for
5: yield the (M, h, r, s) quadruplets corresponding to the (cid:9) found values ρ(cid:8)

This algorithm succeeds with a constant probability and yields (cid:9) quadruplets. We
notice that the crucial oracle call is the one where r||M is queried to H, so all
quadruplets have the same r and M. So we have (cid:9) quadruplets (M, hi , r, si ) such
that r =

mod q mod p

mod q y

mod q.

(cid:19)

(cid:18)

hi
si

g

r
si

The third step uses the fact that k (cid:11)→ (gk mod p) mod q has no preimage set of

size (cid:9). This implies that we must have

hi + xr

si

≡ h j + xr

s j

(mod q)

for some 1 ≤ i < j ≤ (cid:9). This leads to

x = hi s j − h j si
r(si − s j )

mod q.

We can try all combinations of (i, j) until this formula gives x.

(cid:1)

Lemma 10.3. We consider a ﬁnite tree and a mapping dist which maps any leaf λ to
one of its ancestors dist(λ). We call it a distinguished ancestor. We assume we are given
a distribution which deﬁnes a random descent from the root of the tree to a random
leaf λ. We let visit(ν) be the event that the descent goes through ν, i.e. that ν is an
ancestor of λ. We let succ(λ) be an event related to a leaf λ of the tree. When it occurs
we say that λ is successful. We let p = Pr[succ(λ)] and ¯d = E(depth(λ)) for a random
leaf λ. Finally, we let f (ν) = Pr[succ(λ) and dist(λ) = ν|visit(ν)]. For any real number
θ > 0, we have

(cid:22)

Pr
λ

f (dist(λ)) > (1 − θ)

p
¯d

(cid:28)(cid:28)(cid:28)(cid:28) succ(λ)
(cid:23)

≥ θ.

274

This lemma means that within ( p

Chapter 10
−1θ−1) trials we can ﬁnd with high probability
a successful leaf λ such that f (ν) > (1 − θ) p/ ¯d for ν = dist(λ), i.e. a subtree in which
random leaves are successful with a ﬁxed distinguished ancestor ν with probability at
least (1 − θ) p/ ¯d.
Proof. We let G be the set of all good nodes ν such that f (ν) > (1 − θ) p/ ¯d. We have

(cid:1)

ν(cid:4)∈G

Pr[dist(λ) (cid:4)∈ G|succ(λ)] =

Pr[dist(λ) = ν|succ(λ)].

Since visit(ν) is included in dist(λ) = ν, we have

Pr[dist(λ) = ν|succ(λ)] = Pr[visit(ν)]
= Pr[visit(ν)]

Pr[succ(λ) and dist(λ) = ν|visit(ν)]

Pr[succ(λ)]

f (ν)
p

.

Hence

Pr[dist(λ) (cid:4)∈ G|succ(λ)] ≤ 1 − θ
¯d
≤ 1 − θ
¯d

(cid:1)
(cid:1)

ν(cid:4)∈G

ν

Pr[visit(ν)]

Pr[visit(ν)].

The last sum is equal
Pr[dist(λ) ∈ G|succ(λ)] ≥ θ.

to ¯d,

thus Pr[dist(λ) (cid:4)∈ G|succ(λ)] ≤ 1 − θ. We deduce
(cid:1)

10.5 Exercises

Exercise 10.1. Show that if we can compute the discrete logarithm of y in the subgroup
∗
of Z
p spanned by g, then we can break the ElGamal signature, the Schnorr signature,
and the DSA signature.

(cid:8)

, q = H(M) −
) is a valid prime which can lead to some ( p, q, g) DSA public parameters. In
(cid:8)

Exercise 10.2. Prove that for some pair of two messages M and M
H(M
this case, prove that any signature of M is also a valid signature of M
Exercise 10.3. In the DSA signature scheme, show that if we can invert the k (cid:11)→
(gk mod p) mod q function, then we can break the scheme.

.2

(cid:8)

2 This exercise was inspired by Ref. [180].

Digital Signature

275

Exercise 10.4. Deﬁne a signature scheme which is similar to the ElGamal signature
∗
scheme, but which uses an elliptic curve modulo p instead of the Z
p group. Precisely
describe the setup, signature, and veriﬁcation algorithms.
(Hint: Assume that we can easily compute the order of an elliptic curve.)

If we now want to design an algorithm similar to the Schnorr signature, we have

to face a new problem. What is this problem?

Exercise 10.5. Let us assume that a lazy signer has precomputed one (k, r) pair for
the DSS signature scheme and that he always uses the same one. Show how to attack
him and recover his secret key.

Exercise 10.6. Let us assume that a lazy signer has one precomputed (k, r) pair for
the Schnorr signature. Each time he needs to sign, he uses the precomputed signature
and replaces (k, r) by (2k mod q, r 2 mod p). What is the complexity gain? Show how
to attack him and recover his secret key.

Exercise 10.7. The DSA scheme is nondeterministic. Let us assume that the signer now
chooses k by using a pseudorandom generator fed with a secret key and the message
to be signed. Show that a veriﬁer which has the secret key can efﬁciently verify the
signature.

Exercise 10.8 (Ong–Schnorr–Shamir). Precisely deﬁne a public-key signature
scheme such that there is a common large composite modulus n (of unknown factoriza-
tion), a public key K p = k, a secret key Ks = s such that s2 ≡ −k
(mod n), and where
the veriﬁcation of a signature σ = (x, y) consists of checking that x 2 + ky2 ≡ H(m)
(mod n).3

(Note: This is insecure. It has been broken by Pollard and Schnorr.)4

Exercise 10.9. Let us assume that we have n ElGamal signatures to verify. We need to
check n triplets (Mi , ri , si ), where Mi is the message and (ri , si ) is the signature. We
assume all signatures come from the same signer and correspond to the same public
key y and the same p, g parameters.

What is the complexity of verifying all the signatures sequentially in terms of the

size (cid:9) of p in bits?

3 This exercise was inspired by Ref. [144].
4 See Ref. [151].

276

Chapter 10

Let A be a set of N pairwise relatively prime numbers from Z

than B. We pick n pairwise different numbers a1, . . . , an in A. We deﬁne

∗
p−1 which are smaller

1

R = r a1
G = a1
Y = a1

r1
s1

n mod p

. . . r an
H(M1)

+ ··· + an

s1
+ ··· + an

rn
sn

H(Mn)

mod ( p − 1)

sn

mod ( p − 1).

Show that yY R ≡ gG (mod p) if all signatures are correct. What is the complexity of
this computation in terms of n, (cid:9), and B? Show that if one signature is incorrect, then
this property holds only with a probability at most N

−2

11
(cid:1)Cryptographic Protocols

Content

(cid:1)Zero-knowledge: Fiat–Shamir, Feige–Fiat–Shamir
(cid:1)Secret sharing: threshold scheme, perfect schemes
(cid:1)Special purpose signatures: undeniable signatures

In this chapter we review other particular cryptographic protocols. Although they
have a minor practical relevance when compared to encryption or signature, they beau-
tifully illustrate how cryptography can be a fun and a highly technical science.

11.1 (cid:1)Zero-Knowledge

All access control protocols that we have seen so far leak some information (which is
not necessarily useful). For instance, password access controls require to disclose the
password. Challenge–response protocols aim at proving the knowledge of a password,
but require to disclose responses for some given challenges. Complexity theory can
however show that it is possible to make access control without disclosing any informa-
tion through the puzzling notion of zero-knowledge proof of knowledge. This is made
possible by the power of interaction.

This puzzling concept was ﬁrst introduced by Shaﬁ Goldwasser, Silvio Micali, and
Charles Rackoff in the eighties (see Ref. [78]). The concept of power of interaction was
further extended by Adi Shamir who proved that all languages which can be accepted
by an interactive proof are actually all languages which can be accepted by a Turing
machine limited by a polynomially bounded memory space. This result was stated
by the equation IP=PSPACE (for “Interactive Proof” and “Polynomial space”; see
Ref. [166]).

11.1.1

(cid:1)Notion of Zero-Knowledge

In an interactive proof of knowledge, a prover aims at convincing a veriﬁer that he
knows some secret key, by his ability to solve some equations. For instance, he proves
that he knows p and q such that N = pq by his ability to compute square roots modulo
N . This can be used, for instance, for identiﬁcation schemes: assuming that the knowl-
edge of p and q such that N = pq is associated to some identity IN , the prover can
prove that he is IN . The interactive proof of knowledge is deﬁned as follows.

278

Chapter 11

The setup algorithm. This generates a secret key Ks to be given to the prover and

a public key Kp to be broadcasted.

The prover and the veriﬁer speciﬁcations. It deﬁnes two probabilistic algorithms
and the interaction between them. At the end, the veriﬁer must accept or reject
the proof.

The interactive proof must fulﬁll the following requirements.
Completeness: If the prover and the veriﬁer behave as speciﬁed, the veriﬁer always

accepts the proof.

Soundness: There exists a threshold probability τ such that for any probability
p > τ , there exists an extractor which transforms any possible prover which
is accepted by the speciﬁed veriﬁer with probability p into an algorithm which
outputs the secret key. (The transformation must however be “efﬁcient.”)

This intuitively means that any successful cheater must be able to recover the
secret. The extractor is actually a machine which can play with the cheater
and reset its internal state.

Zero-knowledge: There exists a simulator who transforms any veriﬁer (who
tries to extract some information from the honest prover) into an algorithm
which outputs a possible history of the protocol with the same probability
distribution as the history generated by the interaction between the honest
prover and the veriﬁer. (The transformation must however be “efﬁcient.”)

This intuitively means that if a veriﬁer can use the protocol in order to extract
some useful information out of the prover, then he must be able to do the
same with the simulator. Therefore he gains no advantage in interacting with
the prover: the prover reveals zero-knowledge.

11.1.2

(cid:1)The Basic Fiat–Shamir Protocol

We ﬁrst give a simpliﬁed description of the zero-knowledge protocol which was
invented by Amos Fiat and Adi Shamir (see Ref. [68]). This basic version is actu-
ally formally equivalent to the original protocol by Shaﬁ Goldwasser, Silvio Micali,
and Charles Rackoff (see Ref. [78]).

Setup of public parameters: We generate two prime numbers p and q. We let n =
pq and we then erase p and q. We also let t be a security parameter (typically,
t = 20).
key consists of v = 1/s2 mod n.
Protocol: Perform t rounds in which
1. the prover picks a random r and sends a commitment x = r 2 mod n to the

Setup of the keys: The secret key consists of a random element s ∈ Z
∗
n. The public

veriﬁer,

2. the veriﬁer sends a random bit e as a challenge to the prover,
3. the prover sends the response y = rse mod n to the veriﬁer,
4. the veriﬁer aborts the protocol and rejects unless we have y2ve ≡ x

(mod n).

After t successful rounds, the veriﬁer accepts.

(cid:1)Cryptographic Protocols

Prover

pick r, x = r2 mod n

y = rse mod n

v, x

Verifier
−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−− pick e = 0 or 1
−−−−−−−−−−−−−−−−−−−→
check y2ve ≡ x

e

y

279

(mod n)

Figure 11.1. One round of the basic Fiat–Shamir protocol.

See Fig. 11.1.) This interactive proof is obviously complete: if the prover and the veriﬁer
behave as speciﬁed, then the ﬁnal check of each round leads to

y2ve ≡ r 2(s2v)e ≡ r 2 ≡ x

(mod n).

The difference between the Goldwasser–Micali–Rackoff protocol and this version
of the Fiat–Shamir protocol is that the former was stated as a “zero-knowledge proof
of membership,” i.e. a proof that v is a quadratic residue, and the latter was stated as a
“zero-knowledge proof of knowledge,” i.e. a proof that the prover owns a square root
of v. The latter version is more practical from a cryptographic perspective since one
frequently has to identify itself by some kind of proof of ownership. People traditionally
say that the protocol is not completely zero-knowledge and does reveal one bit of
information: that v is a square. If v was not a square, the protocol would eventually fail.
∗
Since distinguishing squares from non-squares is hard in Z
n, this is actually one bit
of information. This is however not a weakness, because people who want to retrieve
information are dishonest veriﬁers, and people who choose v not to be square are
dishonest provers. So we do not really care about what kind of interaction comes out
between a dishonest prover and a dishonest veriﬁer.

Soundness is easy: First of all, we notice that if the prover is able to answer the two
possible challenges e = 0 and e = 1 in a single round with commitment x, it means
that he can compute y0 and y1 such that

≡ x ≡ y2

1

v

y2
0

(mod n).

Therefore, he can produce z = y0/y1 mod n such that z2 ≡ v (mod n). Let us now
consider a dishonest prover who is able to pass the protocol with probability p =
−t + ε and let us construct an extractor. Let p0 be the probability that the prover
2
is not able to answer both challenges in any round of a protocol. Clearly, we have
p ≤ p02

−t + (1 − p0), hence

p0 ≤ 1 −

ε

1 − 2−t

≤ 1 − ε.

ε rounds, we have a probability greater than (1 − ε) 1

Therefore, if we iterate 1
ε which is
approximately 1 − e
−1 that the prover can answer two challenges in a single round. The
extractor works as follows: we iterate many times the protocol with a honest veriﬁer. In
each round, we reset the prover and we ask the other challenge. If the prover happens to

280

Chapter 11

answer both challenges, then we extract z as above, otherwise we keep on running the
protocol. Our analysis shows that we have a fair chance of extracting the secret within
an order of magnitude of 1

ε iterations.

It is easy to cheat with probability 2

−t : in each round, we start by predicting the
challenge e0. Then we generate x = y2ve0 mod n for a random y and take it as the
commitment. If the challenge e chosen by the veriﬁer is equal to e0, then we can answer
it by y. Otherwise it fails.

This inspires the simulator: we play with the malicious veriﬁer by trying to predict
e0 and picking x = y2ve0 mod n for a random y. The simulator sends x to the veriﬁer
and receives e. If e = e0, the simulator can produce y. Otherwise the simulator resets
the veriﬁer and tries again. It can be easily checked that both x is uniformly generated
among all quadratic residues modulo n. In addition, we can easily check that the
generated x in the simulation is independent from e0. Therefore no malicious veriﬁer
can pick e and have it equal to e0 with probability different from 1
2 . This means that
the simulation works with probability 1
2 . If it fails, we can just reset the veriﬁer to
the state before the commitment, and try again with other choices for (e0, y). The
generated (x, e, y) protocol history will obviously get the same distribution as for
the right interaction between the honest prover and the malicious veriﬁer. Therefore
the veriﬁer has no advantage in playing with the real prover (but a complexity factor
due to the probability of failures in the simulation of Step 2) and the protocol discloses
zero-knowledge.

An additional interesting property of the Fiat–Shamir protocol is that keys can
be identity-based: in the Schnorr identiﬁcation protocol, the key was y = gx mod p,
and we had to produce a certiﬁcate that y actually corresponds to some given identity.
Here, the certiﬁcate can be included in the secret key. Actually, if the certiﬁcate issuing
authority keeps p and q, it can choose v as the formatted identity string and compute one
square root for s to give it to the prover. Here, v is bound to correspond to some identity.

11.1.3

(cid:1)The Feige–Fiat–Shamir Protocol

The basic Fiat–Shamir protocol has been improved (see Ref. [66]). Here are the speci-
ﬁcations of the Feige–Fiat–Shamir protocol.

Setup of public parameters: We generate two prime numbers p and q such that p ≡
q ≡ 3 (mod 4). We let n = pq and we then erase p and q. (Note that−1 is not
a quadratic residue modulo both p and q. Therefore the Jacobi symbol (−1/n)
is equal to+1.) We also let k and t be two security parameters (typically, k = 5,
t = 4).
Setup of the keys: The secret key consists of k random elements si ∈ Z
n for i =
∗
1, . . . , k and k random bits b1, . . . , bk. The public key consists of all vi =
(−1)bi /s2
i mod n. (All vi are random residues with Jacobi symbol equal to
+1.)

(cid:1)Cryptographic Protocols

Prover

pick r, x = ±r2 mod n

y = rse1
1

sek
k mod n

v1,
e1

−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−− pick e1,
−−−−−−−−−−−−−−→

,vk,x
,ek

y

check y2ve1

Verifier

,ek = 0 or 1
≡ ±x

vek
k

281

(mod n)

Figure 11.2. One round of the Feige–Fiat–Shamir protocol.

Protocol: Perform t rounds in which

1. the prover picks a random r and sends a commitment x = ±r 2 mod n to

the veriﬁer,

2. the veriﬁer sends random bits e1, . . . , ek as a challenge to the prover,
3. the prover sends the response y = rse1
4. the veriﬁer aborts the protocol and rejects unless y2ve1
1

k mod n to the veriﬁer,
. . . vek
k

. . . sek

1

≡ ±x

(mod n).

(See Fig. 11.2.) After t successful rounds, the veriﬁer accepts.

This interactive proof is obviously complete: if the prover and the veriﬁer behave as
speciﬁed, then the ﬁnal check of each round leads to

y2ve1
1

··· vek

k

≡ r 2(s2

1

v1)e1 . . . (s2
k

vk)ek ≡ r 2(−1)b1+···+bk ≡ ±x

(mod n).

−k in each iteration (thus with prob-
It is also easy to cheat with probability 2
−kt ): if we can predict the challenge e0
, . . . , e0
k (which we can with prob-
−k), we can just pick a random response y and compute the commitment
k mod n at random. This property is actually used for proving the

ability 2
ability 2
x = ±y2ve0
zero-knowledge property.

. . . ve0

1

1

1

k

Let G be the subgroup of Z

∗
n of all z such that the Jacobi symbol (z/n) is
+1. We notice that the x generated by the right prover is a random element of
G with uniform distribution. If we predict to have a challenge e0 and we gener-
ate x as above, we also obtain a random x ∈ G with uniform distribution, which is
further independent from the guessed challenge e0: let ve0 denote ve0
k mod n. We
1
have

. . . ve0

1

k

[x, e] = 1
2

[±y2 = x|ve, e] = 1
2

Pr
y,e

Pr
x,e

Pr
y

[±y2 = x|ve] Pr

e

[e] = 1
#G

[e].

Pr
e

Therefore the generated x is independent from the guessed e0. One problem is that
the challenge e may be chosen by the malicious veriﬁer with an unusual distribution,
depending on x. But since x is independent from e0, which is moreover chosen with
the uniform distribution, the probability Pr[e = e0|x] must be 2
−k. Therefore if we
play with the veriﬁer and make a reset after each failure, we can simulate one round
after about 2k trials. We thus generate (x, e, y) with the same distribution as the real

282

Chapter 11

interaction between the honest prover and the same veriﬁer. Therefore the veriﬁer gets
no important advantage in playing with the prover: he can just play with the simulator.
(The complexity factor is about 2k because of the potential failures.)

In order to prove the soundness, we ﬁrst notice that if the dishonest prover can
(cid:8)
k) for the same

answer to two different questions e = (e1, . . . , ek) and e
such that
commitment x, then he can produce y and y

, . . . , e

(cid:8) = (e

(cid:8)
1

(cid:8)

x ≡ ±y2ve1

1

. . . vek
k

≡ ±(y

(cid:8)

(cid:8)
1

)2ve
1

(cid:8)
k

. . . ve
k

(mod n)

thus he can solve

±(y/y

(cid:8)

)2 ≡ ve

1

−e1

(cid:8)
1

. . . ve
k

−ek

(cid:8)
k

(mod n).

Since this does not necessarily require the full knowledge of the secret key, we just say
that we consider the scheme as broken if one can express a product of the vi ’s with
powers 0, +1, or −1 (with at least one nonzero power) in a ±z2 mod n way. We have
thus proven that provided that this problem is hard, no dishonest prover can answer to
two different challenges.

Let us now assume that one prover can pass the protocol with probability 2

−kt + ε.
We can prove that after 1
ε iterations of the protocol there is a fair amount of chance
that the prover is able to answer to at least two different challenges in a single round.
Therefore an extractor can break the above problem.

11.2 (cid:1)Secret Sharing

Sometimes, it is necessary to be really paranoid for access control. A typical example
is nuclear weapon access. We must not provide access to this dangerous power to
anyone who may be the victim of a human failure such as death, insanity, bribery,
blackmail, etc. Fiction literature is quite inventive on this issue. For this we thus need
to have independent control and backup solutions. Let us say, for example, that access
is provided only if one of the following conditions are met

r the president and the head of the parliament agree
r the president and the chief of the army agree
r the vice president, the head of the parliament, and the chief of the army agree
r and others.

This list of conditions actually deﬁnes an access structure.

Cryptography formalizes this problem as follows: there is an access control secret
key S which is shared among several participants (each participant has a share), and

(cid:1)Cryptographic Protocols

283

only a few combinations of participants enable the reconstruction of S by putting their
shares together. These cryptographic schemes are called secret sharing schemes.

We can illustrate it by an easy case: when the key is shared among n participants who
all need to cooperate in order to reconstruct S. In this case we can give a random Si = Xi
as a share to the i-th participant for i = 1, . . . , n − 1, and give S ⊕ X1 ⊕ ··· ⊕ Xn−1
to the n-th participant. In this case no proper subset of the n participants can reconstruct
S. In fact, no proper subset of the n participants can learn any information about S.

11.2.1

(cid:1)The Shamir Threshold Scheme

Adi Shamir proposed the ﬁrst secret sharing scheme with an elaborate access structure
(see Ref. [163]). This secret sharing scheme allows to share S among n participants
so that any subset of t participants can reconstruct S, but no subset of at most t − 1
participants learn any information about S. Here t is called the threshold.

We have already seen the t = n case.
Another easy case is t = 1: we just give Si = S as a share to the i-th participant.
Let us start with the t = 2 case. We now assume that S is encoded as an element of
a ﬁeld K. We also assume that each participant has a non-zero identity string xi which
is also a ﬁeld element. Let A be a random element of K with uniform distribution. We
deﬁne Si = Axi + S. For any user, the share distribution is uniform, and independent
of S. Therefore, no single user can reconstruct S. Furthermore, any two users xi and
x j can reconstruct S by

S = Si − Si − S j
xi − x j

xi .

The system is depicted in Fig. 11.3. For a user who corresponds to x3, the system is
depicted by a straight line which goes through the (x3, S3) point, but the user has no
clue which line it is. If another user joins (say x6) they can together reconstruct the
straight line and deduce S.

Generalization is now straightforward. We let A1, . . . , At−1 be independent ran-
domly distributed ﬁeld elements. We deﬁne Si = At−1x t−1
+ ··· + A1x + S. We can
deﬁne P(x) = At−1x t−1 + ··· + A1x + S and we have Si = P(xi ). For any t partici-
pants, we have t points (xi , Si ) on the graph of the polynomial mapping P(x), which
is of degree less than t, and therefore we can reconstruct it by interpolation. Indeed,
putting the xi1

i

, . . . , xit shares together, the polynomial is
x − xk
− xk
xi j

P(x) = t(cid:1)

(cid:27)

Si j

j=1

k(cid:4)=i j

.

284

Chapter 11

6

cS6



cS3

cS

c

c

c

x1

c

x2

c

x3

c

x4

c

x5

c

x6

c

x7

-

Figure 11.3. Shamir threshold secret sharing scheme with t = 2.

(It is easy to check that this polynomial maps xi onto Si , therefore P(x) minus this
, . . . , xit . Since it is of degree at most t − 1, the
polynomial has at least t roots xi1
difference must be zero, so this must be the P(x) polynomial.) The interpolation now
yields S = P(0). Finally, the reconstruction formula is simply

S = t(cid:1)

j=1

(cid:27)

Si j

1≤k≤t
k(cid:4)=i j

xk

xk − xi j

.

It is by far more technical to prove that no set of t − 1 participants have any

knowledge about S. Before doing this, we need more theory.

11.2.2

(cid:1)Perfect Secret Sharing Schemes

Here we treat the secret S and the shares Si as random variables. We use again the
Shannon entropy.
Deﬁnition 11.1. Let n be an integer and let  be a set of subsets of {1, . . . , n}. A secret
sharing scheme among n participants P1, . . . , Pn and based on an access structure 
is said to be perfect when

∀A (H(S|Si ; i ∈ A) = 0 ⇐⇒ A ∈ )
∀A (H(S|Si ; i ∈ A) = H(S) ⇐⇒ A (cid:4)∈ )

where S is the secret and Si is the share of Pi .

(cid:1)Cryptographic Protocols

285

Intuitively, the access structure is the set of all subsets of participants who can recon-
struct S. If a subset A of participants can reconstruct S, putting their shares together
provides a zero entropy to S, which is expressed by H(S|Si ; i ∈ A) = 0. The secret
sharing scheme is said to be perfect when no subset A of participants which is not in
 can gain any information on S by putting their shares together, which is expressed
by H(S|Si ; i ∈ A) = H(S). This last condition is very similar to the notion of perfect
secrecy which was used for encryption.

We can now formalize the security result of the Shamir secret sharing scheme.

Theorem 11.2. For any threshold t, the Shamir threshold secret sharing scheme is
perfect, with the access structure which consists of all subsets of at least t participants.

We have already seen how to reconstruct S as long as we have at least t shares. Therefore
the entropy of S is zero given these shares.

We should now prove that for any i1, . . . , it−1, the distribution of S restricted to val-
, . . . , Sit−1 is still uniform. We know (thanks to interpolation) that the function
, . . . , Sit−1) is one to one. This means that for
, . . . , sit−1), the probability that we have S = s and Si j
= si j simultaneously
−t . Now we have

ues of Si1
which maps (S, A1, . . . , At−1) to (S, Si1
any (s, si1
is (#K)

Pr[S = s, Si j

= si j ; j = 1, . . . , t − 1]

(cid:1)

s

Pr[Si j

= si j ; j = 1, . . . , t − 1] =
= (#K)

−t+1

and

Pr[S = s|Si j

= si j ; j = 1, . . . , t − 1] = (#K)

−1.

Therefore the distribution of S is still uniform when we ﬁxed Si1
no information by having these shares only.

, . . . , Sit−1: we gained

11.2.3

(cid:1)Access Structure of Perfect Secret Sharing Schemes

An interesting question is for which access structure does a perfect secret sharing
scheme exist? We have already seen that it exists for a threshold access structure,
thanks to the Shamir scheme. Obviously, if  is an access structure for which there
exists a perfect secret sharing scheme,  must fulﬁll the following requirement.

For any A ∈  and any B such that A ⊂ B, we have B ∈ . (If we can reconstruct
S from the shares of A, we can still do it with more shares in B.)

We call this the monotonicity property.

286

Chapter 11

A natural question now is: Is any set of subsets with the monotonicity property
an access structure for a perfect secret sharing scheme? This question is addressed in
Section 11.2.4.

Before continuing we deﬁne the notion of access structure spanned by a given set
of subsets. Given a set of subsets 0, we deﬁne  = (cid:30)0(cid:31) as the set of all supersets of
any set in 0. This clearly has the monotonicity property. It is also clearly the smallest
set of subsets with the monotonicity property which includes 0. For this reason we
say that  is spanned by 0.

11.2.4

(cid:1)The Benaloh–Leichter Secret Sharing Scheme

The Benaloh–Leichter secret sharing scheme enables the construction of a perfect secret
sharing scheme for any access structure with a monotonicity property (see Ref. [27]).
It works as follows.

1. Given an access structure  with the monotonicity property, we ﬁrst express
 as an algebraic expression with only ∪ and ∩ operations from all i access
structures deﬁned as follows: i is the set of all participant subsets which include
the i-th participant. In other words, i = (cid:30){i}(cid:31).
r if X is attached to some subexpression t ∪ t
r if X is attached to some subexpression t ∩ t
,
t
Y to t and X − Y to t

2. To each subexpression we recursively attach variables in a top–down way:

, we attach a new random variable

, we attach X to both t and

(cid:8)

(cid:8)

(cid:8)

(cid:8)

.

3. For each participant i we collect all variables attached to occurrences of i , and

we deﬁne them as the share of the participant.

We have the following result.

Theorem 11.3. The above construction builds a perfect secret sharing scheme of ac-
cess structure .

As an example, let us deﬁne a perfect secret sharing scheme among a set of four

participants P1, P2, P3, P4 such that

r P1 and P2 can reconstruct the secret,
r P1 and P3 can reconstruct the secret,
r P2, P3, and P4 can reconstruct the secret.

Clearly, the access structure expresses into

 = ((1 ∩ 2) ∪ (1 ∩ 3)) ∪ ((2 ∩ 3) ∩ 4)

(cid:1)Cryptographic Protocols

287

∪

 

 

@

@

 

 

 

 

 

 

@

@

@

@

@

@

@@

@

@

∩

   @@

  

 

 

∩

   @@

∩

@@

@

@

4

∪

  

 

 

∩

   @@

1

2

1

3

2

3

Figure 11.4. Example for an access structure term.

as drawn in Fig. 11.4. We start by attaching S to the whole expression . It is written as
a ∪ between (1 ∩ 2) ∪ (1 ∩ 3) and (2 ∩ 3) ∩ 4. So we attach S to both terms.
Similarly, in the former term, we attach S to 1 ∩ 2 and 1 ∩ 3. We have thus three
terms to which S is attached. For the ﬁrst one 1 ∩ 2, we attach W to 1 and S − W
to 2. For the second one 1 ∩ 3, we attach X to 1 and S − X to 3. For the third
one (2 ∩ 3) ∩ 4 we attach Y to 2 ∩ 3 and S − Y to 4. It thus remains to attach
Z to 2 and Y − Z to 3. To summarize, we have two occurrences of 1 to which
are attached W and X, we have two occurrences of 2 to which are attached S − W
and Z, we have two occurrences of 3 to which are attached S − X and Y − Z, and
we have one occurrence of 4 to which is attached S − Y . Therefore we deﬁne the
share

S1 = (W, X)
S2 = (S − W, Z)
S3 = (S − X, Y − Z)
S4 = S − Y

where W, X, Y, Z are independent uniformly distributed random variables. We can,
for instance, check that P1 and P2 can reconstruct S because they have W and S − W .
But P2 and P3, for instance, cannot reconstruct S because (S − W, Z , S − X, Y − Z)
is equivalent to (S − W, X − W, Y, Z) which gives no clue about S.

11.3 (cid:1)Special Purpose Digital Signatures

In this section we list a few important variants of digital signature schemes.

288

Chapter 11

Adversary

-



Message X

X

?

6σ

X,σ

X
?

σ
?

Signature

 -

Prover



ZK

-

X

-

-

X
?

σ
?

Verifier
6

Secret key
Ks

Public key
Kp

AUTHENTICATED

Kp

Generator

Figure 11.5. Undeniable digital signature.

11.3.1

(cid:1)Undeniable Signature

In classical digital signature schemes, anyone who has the public key of the signer is
able to verify whether a given signature is valid or not. This property is called universal
veriﬁability. Sometimes, signers want to keep private their signatures and do not want
to have a universally veriﬁable signature. Another kind of signature proposed by David
Chaum and Hans van Antwerpen, called invisible signature, makes it impossible for
anyone to verify whether the signature is valid or not, and even to associate it to the
signer, but a special interactive process which requires the cooperation of the signer
would make it possible (see Ref. [45]). In such a case, it could be easy for the signer
who wants to deny his signature to simply claim that a signature is invalid. But we
want to prevent dishonest signers to repudiate their own signatures. This is why we
usually call these signatures undeniable signatures. The name may be quite confusing
because all signatures are aimed to be undeniable. Indeed, a classical signature is
undeniable because of the universal veriﬁability. Some authors prefer to talk about
invisible signatures instead of undeniable signatures, but since the latter is quite widely
used we will stick to this name.

Undeniable signatures consist of several algorithms. As for the classical signatures,
there is a key pair generation algorithm and a signature algorithm. The veriﬁcation
algorithm is replaced by an interactive protocol between the signer and the veriﬁer
as depicted in Fig. 11.5. Actually, the veriﬁcation is replaced by two protocols: one
which is used for the signature conﬁrmation and one which is used for the signature
denial. Of course, it is assumed to be impossible to make existential forgeries, and to
distinguish valid signatures from invalid ones. Additionally, it must be impossible for
the prover to successfully run the conﬁrmation protocol when the signature is invalid,
or to successfully run the denial protocol when the signature is valid. Furthermore,

(cid:1)Cryptographic Protocols

289

Prover
pick x ∈ Z
∗
n
split x y = r 2ga1

1 ga2
2

(cid:30)x(cid:31)
y

−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−
−−−−−−−−−−−−−−→

x,r,a1,a2

Veriﬁer

pick y ∈ Z
∗
n
check (cid:30)x(cid:31), x y

Figure 11.6. MOVA key validation protocol.

malicious veriﬁers should learn no other information from the protocols but the validity
or invalidity of signatures.

As an example we describe here the MOVA scheme designed by Jean
Monnerat and Serge Vaudenay which is based on quadratic residuosity.1 It depends
on several security parameters m, t, k, (cid:9). In a typical setting we can propose m = t =
k = (cid:9) = 30.
Key setup. We take two different odd prime numbers p and q and compute n = pq.
We take two elements g1, g2 ∈ Z
n such that (g1/n) = (g2/n) = −1, (g1/ p) = −1,
∗
and (g2/ p) = +1. Note that it sufﬁces to use the Chinese Remainder Theorem with a
∗
nonquadratic residue (resp. a quadratic residue) in Z
p and a quadratic residue (resp. a
∗
nonquadratic residue) in Z
q to make g1 (resp. g2). The public key is (n, g1, g2). The
secret key is p. In order to prove that the generated key is valid, the scheme requires a
special protocol. As depicted in Fig. 11.6, this consists of m iterations of the following
protocol.

veriﬁer.

1. The prover picks x ∈ Z
∗
n at random and sends a commitment to x to the
2. The veriﬁer picks y ∈ Z
∗
n at random and sends it to the prover.
n and a1, a2 ∈ Z2 such that x y ≡ r 2ga1
3. The prover looks for r ∈ Z
∗

(mod n).
For this he takes a1 such that (x y/ p) = (−1)a1, a2 such that (x y/n) = (−1)a1+a2,
−a2
and computes a square root r of x yg
2 mod n by using the Tonelli algo-
rithm and the factorization of n. He then reveals r, a1, a2 and opens the com-
mitment to x.

4. The veriﬁer checks the commitment for x and that x y ≡ r 2ga1

−a1
1 g

1 ga2
2

1 ga2
2

(mod n).

We can prove that this protocol is complete, sound, and zero-knowledge. More precisely,
−m if there exists some z ∈ Z
∗
n which
it cannot succeed with probability greater than 2
cannot be written z = r 2ga1
n and a1, a2 ∈ Z2.
2 mod n for r ∈ Z
∗

1 ga2

∗
Using this process the key holders own some nontrivial character χ over Z
n, i.e.
n and {−1,+1} such that χ(g1) = −1 and
∗
2 mod n for r ∈ Z
∗
n and

some group homomorphism between Z
χ(g1) = +1, and he proves that all Z
∗
n can be written r 2ga1
a1, a2 ∈ Z2.

1 ga2

1 We actually describe a particular case based on characters of order 2. See Refs. [135, 136].

290

Chapter 11

Prover

v = χ(u)
check u

u
(cid:30)v(cid:31)

←−−−−−−−−−−−−−−
−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−
−−−−−−−−−−−−−−→

r,a1,...,as

v

Veriﬁer

∗
n

pick r ∈ Z
pick a1, . . . , as ∈ Z2
u = r 2ga1

··· gas

s

1

check (cid:30)v(cid:31)
v = ea1

1

··· eas

s

Figure 11.7. MOVA proof of interpolation protocol.

Proof of interpolation. The MOVA scheme requires to prove that given some subset

A = {(g1, e1), . . . , (gs , es)}

∗
n

× {−1,+1}, there exists a character χ, i.e. a group homomorphism χ form Z
∗
of Z
to {−1,+1} such that χ(gi ) = ei for i = 1, . . . , s. In all cases here this character is the
n
Legendre symbol χ = (·/ p), which is computable from the secret key only. We call
this a proof of interpolation for A. As depicted in Fig. 11.7, this consists of k iterations
of the following protocol.

1. The veriﬁer picks r ∈ Z
n, a1, . . . , as ∈ Z2, computes u = r 2ga1
∗
s mod n
and sends it to the prover.
2. The prover computes v = χ(u) and sends a commitment of v to the veriﬁer.
3. The veriﬁer discloses r, a1, . . . , as.
4. The prover veriﬁes that u = r 2ga1
1
5. The veriﬁer veriﬁes the commitment and checks that v = ea1

(These steps are used in order to make sure that the veriﬁer is honest.)

s mod n and opens the commitment.

··· gas

··· gas

1

··· eas
s .

1

We can prove that this protocol is complete, sound, and zero-knowledge. More precisely,
−k if there exists no character χ for
it cannot succeed with probability greater than 2
which χ(gi ) = ei for i = 1, . . . , s.

Signature algorithm. The signature algorithm is quite simple. Given a message X, we
generate x1, . . . , xt ∈ Z
∗
n by using a generator fed with the hashed value of X. We then
compute yi = (xi / p) for i = 1, . . . , t. The signature is σ = (y1, . . . , yt ). (Note that it
consists of t bits.) We can show that someone who can distinguish a valid signature
∗
from an invalid one can solve the quadratic residuosity problem in Z
n, i.e. can distin-
guish a quadratic residue from a nonquadratic residue with a Jacobi symbol equal to
+1.
Conﬁrmation algorithm. To conﬁrm a signature σ = (y1, . . . , yt ) for a message X, we
recalculate x1, . . . , xt and then run the proof of interpolation with
A = {(g1,−1), (g2,+1), (x1, y1), . . . , (xt , yt )}.

(cid:1)Cryptographic Protocols

Prover

compute vi = χ(ui)
get λ from wi/vi = ( i/yi)λ
check ui,wi

291

Verifier
pick λ ∈ Z2, ai, bi ∈ Z2, ri ∈ Z∗
i , wi = (−1)ai λ
2 xλ
ui = r2

1 gbi

i gai

n

i

ui,wi
(cid:13)λ(cid:14)

←−−−−−−−−−−−−−−
−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−
−−−−−−−−−−−−−−→ check (cid:13)λ(cid:14), λ

ri,ai,bi

λ

Figure 11.8. MOVA denial protocol.

Denial algorithm. To deny a signature σ = (z1, . . . , zt ) for a message X, we recalculate
x1, . . . , xt and then run the proof of noninterpolation below. As depicted in Fig. 11.8,
this consists of (cid:9) iterations of the following protocol.

2 x λ

i gai

1 gbi

i mod n and wi = (−1)ai zλ
ui = r 2
i
(u1, . . . , ut ) and (w1, . . . , wt ) to the prover.

n, a1, . . . , as , b1, . . . , bs , λ ∈ Z2, computes
1. The veriﬁer picks r1, . . . , rt ∈ Z
∗
for i = 1, . . . , t. He sends the
2. The prover computes yi = (xi / p) and vi = (ui / p) for i = 1, . . . , t. Since
wi /vi mod n should be equal to (xi /zi )λ for all i and that there must be some
(cid:4)= zi , the prover can recover λ. In case of inconsistency he picks
i for which xi
λ at random. He sends a commitment to λ.

3. The veriﬁer discloses (ri , a1, bi ) for i = 1, . . . , t.
4. The prover veriﬁes the consistency with (ui , wi ) and λ for i = 1, . . . , t. He then

opens the commitment.

5. The veriﬁer veriﬁes the commitment and checks that λ is correct.

We can prove that this protocol is complete, sound, and zero-knowledge. More precisely,
it cannot succeed with probability greater than 2

−(cid:9) if the signature is valid.

11.3.2

(cid:1)Other Special Purpose Digital Signatures

Group signature. We may need to have a signature scheme for a group of participants:
we want to certify that someone from a given group has signed a document with-
out disclosing who really did. Group signatures usually require a heavy process for
welcoming or revoking members, but have pretty efﬁcient signature and veriﬁcation
schemes.

Ring signature. Some special groups are ad hoc groups. They are not controlled by
any means of membership and there is no formal process. Anyone can indeed invent
a group by describing who is member and produce a proof that someone from this
list did sign a document. (The main drawback is that the proof is pretty long.) These
ad hoc groups are called rings.

292

Chapter 11

Blind signature. This signature enables someone to ask a signer to sign the message
so that the signer has no means to know what he signed. He only gets insurance that
he signed one document, and only one document. One may wonder what kind of
applications can have blind signatures. The most important one is digital cash. Privacy
in digital cash requires untraceability of the money. Here digital coins are signed by a
bank, one by one, in a blind way because customers do not want to have coins which
can be traced.

Invisible signature with designated conﬁrmer. We have seen the notion of invisible
signatures with undeniable signatures. This was in order to protect the privacy of the
signer. One problem remains: the signer can be asked to conﬁrm or deny under threats.
In order to have even stronger protection for the signer, another kind of signature
prevents him from being able to conﬁrm or deny, but he can designate a third party
(e.g. a lawyer) who will be able to do both. Of course, this third party should not be
able to forge signatures.

Fail-stop signature. In classical digital signatures, the signer is protected by complexity
theory arguments that nobody can forge a signature. Fail-stop signatures are signatures
with a stronger protection level: here the signer is protected by information theory
arguments. Indeed, even if forging a signature is hard, if it happens to occur, the signer
will be able to demonstrate that it is a forgery by solving a problem that he would not
have been able to solve in other circumstances.

11.4 (cid:1)Other Protocols

Cryptography contains some other beautiful dedicated protocols. We give a few exam-
ples here.

Escrowed digital cash. We have seen that we can make untraceable digital cash (for more
privacy) with blind signatures. The main drawback is indeed the lack of traceability.
In order to protect against criminal organizations and prevent money laundering, law
enforcement requires having some kind of hidden traceability based on legal request.
This makes practical digital cash systems quite complicated.

Electronic votes. Voting schemes are fundamental for democracy. In order to be im-
plemented in an electronic way, we must protect privacy, and prevent the temptation of
being corrupted. For this we must both authenticate the voter (in order to avoid double
votes) and treat his ballot in an anonymous way. The tricky part is that anonymity must
be enforced against a third party, but also against the voter himself: the voter must not
be able to prove that he has voted for someone. Interestingly, the protection must remain
valid even in the case of a revolution when a dictator could get access to all master
secret keys, etc. Adversaries are indeed quite hard to formalize in voting schemes!

Mental Poker. When trying to play a card game (e.g. poker) remotely, there is a big
security problem related to card shufﬂing and distribution: one should make sure that no

(cid:1)Cryptographic Protocols

293

participant can learn what card was distributed to another participant, one should make
sure that no card is distributed twice, and one should make sure that the distribution of
card shufﬂing is as fair as possible. Some protocols can be used in order to share the
card deck, to shufﬂe, to distribute, etc.

Multiparty computation. A more general problem is to make multiparty computation:
each participant has a secret value, and we want to compute a function in terms of
all secret values, without disclosing the values. In the electronic vote, we want to
count the number of occurrences of several values, and hopefully get the majority. We
can invent other odd problems: compute the maximum fortune of jealous millionaires
who do not want to disclose their fortune, but only to know how much owns the
richest; compute perfect matchings out of private requirements in blind date ceremonies,
etc.

Broadcast encryption. In pay television, we need to broadcast a signal which is en-
crypted, and to distribute different keys to the participants in order for them to be able
to decrypt.

11.5 Exercises
Exercise 11.1. An idiot dishonest veriﬁer thinks that giving the challenge e = 0 in the
basic Fiat–Shamir protocol is useless since the answer y = r does not depend on the
secret key. Therefore he decides to always ask e = 1.

Show how a malicious prover can impersonate anyone to this veriﬁer.

Exercise 11.2. Let us deﬁne the following identiﬁcation scheme. A user U has a secret
key which consists of two prime numbers p and q such that p ≡ q ≡ 3 (mod 4). We
have n = pq which is public and associated to the identity of U by a certiﬁcate. In
order to prove the knowledge of the secret key, we can challenge U with a random x.
U then computes one square root y of x modulo n, and we can check that it is indeed
a square root by raising it to the square.

Why is it insecure?

Exercise 11.3. Explain how to transform the Fiat–Shamir protocol into a signature
scheme.

Exercise 11.4 (Guillou–Quisquater zero-knowledge protocol). We consider the fol-
lowing scheme.

Setup of public parameters: take n = pq with p, q primes and an exponent v ∈
∗
Z
ϕ(n).
Setup of the keys: for a given prover, deﬁne his identity string J , and compute B =
1/J 1/v mod n. The public key is K p = (J, v, n). The secret key is Ks = B.

294

Chapter 11

Protocol: perform

1. the prover picks a random r and sends T = r v mod n,
2. the veriﬁer sends random d among 1, . . . , v − 1,
3. the prover sends D = r Bd mod n,
4. the veriﬁer checks that T ≡ Dv J d (mod n).

Prove that this is a zero-knowledge proof of knowledge of B.2

Exercise 11.5. Construct a perfect secret sharing scheme with the following access
structure

 = (cid:30){P1},{P2, P3},{P2, P4, P5},{P3, P4, P5}(cid:31).

2 This exercise was inspired by Ref. [82].

12
From Cryptography to
Communication Security

Content

Security setup: certiﬁcates
Remote access: SSH
Secure Internet transactions: SSL
Security for individuals: PGP

To conclude this book, this chapter shows how to put together materials from the
previous ones in order to build cryptographic applications that provide communication
security. We illustrate this with a few popular examples.

The main objective is to set up a notion of secure communication session.
One example is the (public-key-less) Bluetooth technology which was outlined in
Section 5.6.2. We will see some other examples here.

The session usually starts by peer authentication, exchange of public-key materi-
als, and goes on with an authenticated key agreement protocol. This ensures that both
peers share a common secret key. The secret key is derived into several symmetric secret
keys. Then message security, i.e. message integrity, message authentication, and mes-
sage conﬁdentiality, is ensured by means of MAC and symmetric encryption. Session
security additionally requires to ensure the sequentiality of messages, i.e. no adversary
can replay a message, swap messages, or erase a message. This is usually achieved by
means of a synchronized message counter. Some additional security properties may be
required, such as

will be delivered to the right receiver on time;

r Timeliness of message delivery: a message sender is ensured that his message
r Termination fairness: peers are ensured to terminate the session in the same state
r Anonymity: a peer is ensured that her identity does not leak;
r Untraceability: a peer is ensured that the other peer will no later be able to
r Unlinkability: peers are ensured that we cannot even realize that two different

(either termination success or premature abortion);

identify her in other sessions;

communication sessions share the same entity;

296

Chapter 12

Adversary



Key
K

-

Encryption

Ciphertext

-

Y

-

Decryption

Y

-

K

6

Kp

6
Secret key
Ks

Verification

Certificate

Kp,σ



 Kp,σ

Generator

6

KCA

p

AUTHENTICATED

Generator

-

Signature

Public key
Kp


AUTHENTICATED

Figure 12.1. Key exchange using certiﬁcates.

r Untransferability: a peer is ensured that the other peer is not a fake one who
r and so on.

hides the real one;

12.1 Certiﬁcates

We have seen in Chapter 9 that we can perform a secure key agreement, e.g. by trans-
mitting a key K by using a public-key cryptosystem. This is indeed quite convenient to
exchange a common authenticated secret in a client–server protocol because it can later
be used in order to set up a secure communication channel. Key agreement protocols
however assume that, e.g., we transmitted the public key K p of the server to the client
in an authenticated channel prior to the transmission. This prior authenticated channel
could be set up by a common trusted third party (see Fig. 12.1). Indeed, a “certiﬁcate
authority” (CA) could issue a signature σ for K p, and the public-key authentication is
then guaranteed. The only remaining problems consist in securing the communication
of the public key K p from the server to the certiﬁcate authority, and the communication
of the authority public key K CA

to the client (see Fig. 12.2).

p

Certiﬁcates can follow the X.509 standard format. It is also available as the Internet
standard RFC 2459 (Ref. [93]). A certiﬁcate looks like the one in Figs. 12.3 and 12.4.
Fig. 12.3 represents the overall structure, including some information about the issuer of
the certiﬁcate (here EPFL in Switzerland), the certiﬁcate validity period (here between
July 2002 and July 2003), and the signature for the certiﬁcate (here it is a signature
with md5WithRSAEncryption algorithm). Fig. 12.4 provides information about
the subject part. (It is put at the place of the . . . in Fig. 12.3.) We can see the subject entity
(here an IMAP server for electronic mail boxes in EPFL), its encryption algorithm type
(here RSA), and its public key (a modulus and an exponent).

From Cryptography to Communication Security




Client 1



QQk
Q




Q

K CA
Q
p
Q

K CA

p



K CA
p



Q




Q



Authority



+


QQk
Q





K 1

p


K 2
p

Q
Q
K 3
p

Q

Q




Client 2




Client 3



+






Server 1

297







Server 2





Q


Server 3




Figure 12.2. Critical secure channels when using certiﬁcates.

12.2 SSH: Secure Shell

SSH (as for “Secure SHell”) was originally made to enable remote access to a computer
in a secure way under UNIX-like operating systems. It was made to be used just like the
rlogin command (remote login). There is now a series of commercial applications

Certificate:

Data:

Version: 3 (0x2)
Serial Number: 212 (0xd4)
Signature Algorithm: md5WithRSAEncryption
Issuer:

C=CH, ST=Vaud, L=Lausanne, O=EPFL,
CN=EPFL Certification Authority/Email=cert-auth@epfl.ch

Validity

Not Before: Jul 11 09:42:05 2002 GMT
Not After : Jul 11 09:42:05 2003 GMT

...

Signature Algorithm: md5WithRSAEncryption

a2:ae:a1:b0:f0:24:47:ca:29:b8:78:a6:58:7d:62:3e:25:c9:
e6:c8:f7:58:99:18:ab:f5:ed:e7:74:7f:a9:4b:5f:07:e3:80:
a4:68:ea:0a:d2:8f:bb:b7:cc:cc:85:81:d0:15:4a:ee:7e:74:
f3:be:49:73:bc:4a:ab:22:4e:86:c6:9b:97:d7:4d:16:05:5c:
69:14:b6:10:36:da:70:64:50:23:4a:33:4c:fe:33:ca:3a:4e:
cb:c5:9b:28:be:df:b8:30:e7:07:13:d7:e2:88:b2:c2:af:19:
28:53:7d:39:37:d1:7c:c7:0b:10:3d:12:9d:15:8d:38:dd:6a:
06:55

Figure 12.3. Certiﬁcate for a secure IMAP server (overall structure).

298

Chapter 12

Subject:

C=CH, ST=Vaud, L=Lausanne, O=EPFL,
CN=imapwww.epfl.ch

Subject Public Key Info:

Public Key Algorithm: rsaEncryption
RSA Public Key: (1024 bit)

Modulus (1024 bit):

00:da:33:16:c5:8b:30:e5:f8:be:4d:43:68:02:e3:
e4:0e:09:35:72:f4:72:0a:fd:71:6c:79:08:e5:a8:
31:44:00:f8:e4:72:b1:23:83:6b:b4:f2:85:54:75:
c7:1e:a0:53:e1:10:b5:e6:85:8a:67:ec:8e:5e:5c:
6f:c6:b5:95:a0:55:3f:c0:45:8e:54:19:78:6e:40:
3d:ae:01:55:1c:31:fc:d4:e3:3a:9f:47:a8:6c:25:
47:f9:87:d5:ab:dc:0b:e3:71:a7:44:03:97:55:86:
46:d0:48:11:b5:bb:90:fd:d4:c7:25:3b:98:83:20:
9a:b5:ae:34:23:b8:43:12:71

Exponent: 65537 (0x10001)

Figure 12.4. Certiﬁcate for a secure IMAP server (subject part).

based on SSH and a popular open source variant based on the openSSH library.1
The Linux community is familiar with the ssh command (still for remote login) and
scp command (for remote ﬁle transfer) since system administrators tend to close all
communication ports but the one used by these commands.

12.2.1 Principles of SSH

The principle of SSH is to implement secure (i.e. conﬁdential and authenticated) com-
munication channels in a client–server session. The philosophy of SSH was originally
to be user-friendly (ssh had to be used exactly like rlogin), ready to use without
any complicated installation, and to be deployed easily. The counterpart was that the
security level was not so high, although higher than what was used before. The new
release of SSH (known as SSH2) uses public-key infrastructures in order to authenti-
cate servers. This is typically heavy stuff, but the user can easily bypass it: he just has
to click “OK” anytime there is a security warning.

When a client wishes to connect to a server, the server sends its public key together
with a certiﬁcate (if available). The ﬁrst connection is critical: either the client is able
to strongly authenticate the public key, e.g. by checking a certiﬁcate or having the
user to check the public-key ﬁngerprint, or the client has to trust that the public key is
correct. Then the client stores the public key in a ﬁle (typically, .ssh/known hosts).
Assuming that this ﬁrst connection is OK, all future connection to the same server
should be secure by comparing the received key with the correct public key from this
ﬁle. The underlying assumption is that this ﬁle has integrity protection. If the key does

1 See http://www.ssh.com and http://www.openssh.org.

From Cryptography to Communication Security

299

not match (for whatever reason), the user has a security warning saying that the public
key has changed and that some adversary may be trying to impersonate the server by
sending a wrong key. Typically, the user does not care and clicks “OK.” This is the
major problem of SSH, but remember that the purpose was just to increase the security,
not to have a perfect one.

The client and the server run a key agreement protocol such that the server is
authenticated, and devise a symmetric key to be used to set up a secure channel. Then,
the client is authenticated by a password which is sent through the secure channel.

12.2.2 SSH2 Key Exchange and Authentication

SSH2 uses DSS for server authentication and Difﬁe-Hellman key agreement for setting
up a symmetric session key (previous versions were entirely based on RSA). Both
∗
are based on some generator g which generates a subgroup of Z
p of prime order q.
Concretely, the clients and the server exchange some “Initial Message” IC and IS, and
the protocol version VC and VS that they support. Then, as illustrated in Fig. 12.5, the
key agreement runs as follows.

1. The client picks a random x ∈ {1, . . . , q − 1}, computes e = gx mod p, and
sends it to the server.
2. The server picks a random y ∈ {1, . . . , q − 1}, computes
f = g y mod
p and K = ey mod p. Then he computes
the hashed value H of
VC||VS||IC||IS||K S||e|| f ||K and signs it, where K S is his public key, and sends
K S, f , and the signature s to the client.
3. The client can verify K S at this time (e.g. using a certiﬁcate or his list of known
public keys). Then the client computes K = f x mod p, the hashed value H of
VC||VS||IC||IS||K S||e|| f ||K , and checks if s is a valid signature for H.

Then the client and the server can use K as a symmetric key for symmetric en-
cryption and MAC. The choice of the algorithms is negotiated between the client and
the server. Several encryption schemes are proposed, including triple DES, AES, RC4,

Client

version VC, initial message IC

pick x, e = gx mod p

K = f x mod p, check KS
H = hash(VC||VS||IC||IS||KS||e|| f||K)
(s,H)

VerKS

VC, IC

−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−− version VS, initial message IS
−−−−−−−−−−−−−−→

VS, IS

e

Server

pick y, f = gy mod p, K = ey mod p
H = hash(VC||VS||IC||IS||KS||e|| f||K)

←−−−−−−−−−−−−−− s = Sig(H)

, f, s

KS

Figure 12.5. Semi-authenticated key exchange in SSH.

300

Chapter 12

and IDEA. The MAC algorithm is typically HMAC based on SHA-1 or MD5. To set
up the secure channel, the client and the server derive six keys from K and H. Keys
are generated as Gen (K||H||string||session id) where session id is an identiﬁer for the
session and string is a constant which depends on which of the six keys is generated.

r Initial value IV from the client to the server: string = A.
r Initial value IV from the server to the client: string = B.
r Encryption key from the client to the server: string = C.
r Encryption key from the server to the client: string = D.
r Authentication key from the client to the server: string = E.
r Authentication key from the server to the client: string = F.

The generator is deﬁned by taking the ﬁrst bits of the sequence k1||k2|| . . . where

and

k1 = hash(K||H||string||session id)

ki+1 = hash(K||H||k1||···||ki ).

12.3 SSL: Secure Socket Layer

SSL is a famous communication protocol which was ﬁrst developed by Netscape. It
is used in the Internet world. The interface is fairly similar to the TCP/IP one in the
sense that applications which need to communicate securely open and close sockets in
a very similar way so that it is mostly transparent. The most popular versions are SSL
version 3.0 and its successor TLS version 1.0 which is the Internet standard RFC 2246
(Ref. [58]). Although SSL is a more popular name, we summarize TLS 1.0 here.
SSL/TLS is typically used by Internet browsers in order to communicate securely with
HTTP (Hypertext Transfer Protocol) servers. It can also be used by other applications
like an e-mail manager willing to connect to a mailbox server. SSL/TLS is designed to
be universal and does not rely on a speciﬁc cryptographic algorithm choice. The choice
of the algorithms, called cipher suite, is negotiated between the client and the server at
the beginning of a session. We present here a simpliﬁed version of SSL.

There are actually two layers of protocols. The lowest is the SSL Record Protocol
layer which is on top of TCP and below other high-level communication protocols like
HTTP. The other is the SSL Handshake Protocol which is on the same level as the
HTTP protocol. The SSL Handshake Protocol is used in order to initiate a session with
all the expensive cryptographic protocols such as asymmetric authentication and key
agreement. A session can launch several connections which are handled by the SSL
Record Protocol. This is really the secure channel which uses only symmetric (i.e. fast)
cryptography. As we have seen in the previous chapters, such a channel is possible as
long as the two parties previously exchange a symmetric key in an authenticated and
conﬁdential way.

From Cryptography to Communication Security

301

For each session, the SSL client and server keep an internal state which contains a
session identiﬁer, the peer certiﬁcate, the cipher suite choice, and a master secret (a 48-
byte symmetric key which is set up at the beginning of the session). It also contains a
compression algorithm choice. A session can include several connections. A connection
state includes a server and a client nonce, an initialization vector, a sequence number
which is incremented for each message, and a set of four secret symmetric keys for
MAC and encryption in the two ways: two for the client and two for the server.

The cipher suite speciﬁes the algorithm which is used for peer authentication and
key agreement in the handshake protocol, and the symmetric algorithms for encryption
and MAC in the record protocol. The latter algorithm pair is often called the “cipher
spec.” The cipher spec of a session can be changed through the SSL Change Cipher
Spec Protocol. It is typically run once at the beginning of the protocol in order to set
it up.

An extra protocol, the SSL Alert Protocol, is used to handle alert messages. They
can be simple warning alerts or fatal alerts. In the latter case the connection aborts.
Other connections in the session can continue, but no new connection can be launched
in the session.

12.3.1 Handshake

When a new session starts, the client and the server agree on a protocol version,
negotiate a cipher spec, optionally authenticate each other by using certiﬁcates, and
exchange a key as follows.

1. The client ﬁrst sends a ClientHello message which includes the session
identiﬁer, the set of all cipher suites that he can accept, and a nonce to the
server.

2. The server responds by sending a ServerHello message. This includes the
session identiﬁer, the cipher suite he selected in the set of the client, and a
nonce. He usually sends his certiﬁcate in order to be authenticated. The cer-
tiﬁcate may include material for a Difﬁe-Hellman key agreement as explained
in Section 12.3.6. Otherwise the server needs to send a speciﬁc key exchange
message. He may also send a certiﬁcate request if he wishes to authenticate the
client.

3. Based on this the client may send his certiﬁcate if requested by the server. He
may also send a key exchange message depending on which key algorithm was
selected in the cipher suite. This is the ClientKeyExchange message. The
client and the server can then compute four symmetric secret keys: two for
encryption in one way or another and two for MAC in one way or another.

4. The client sends a protected MAC of all previous handshake messages. This

ensures that no messages were lost, swapped, or replayed.

5. Similarly, the server responds by a protected MAC of all previous handshake

messages.

302

Client

pre master secret

check

ClientHello:acceptable cipher suites, nonceC

−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− select cipher suite
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ decrypt

ClientKeyExchange:RSA ENC(pre master secret)

ServerHello:cipher suite, certi cate , nonceS

Chapter 12

Server

(key derivation)

−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ check
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−

MAC(handshake messages)

MAC(handshake messages)

(open tunnel)

Figure 12.6. A typical SSL handshake.

The client and the server can then communicate through a protected channel.

A typical handshake session is illustrated in Fig. 12.6.

12.3.2 Cipher Suites

A cipher suite includes a key agreement algorithm, a symmetric cipher algorithm, and
a hash function. In Figs. 12.7 and 12.8 are a few standard cipher suite deﬁnitions for
TLS. In addition, an update of TLS integrates AES by specifying the cipher suites of
Fig. 12.9 (see Ref. [46]).

We notice that RC4 is the only stream cipher. It is used with two different key

lengths: 40 and 128 bits.

All block ciphers use blocks of 8 bytes, except AES which uses 16 bytes. They are

all used in CBC mode with an initial vector.

Hash
Key Exchange Cipher
CipherSuite
NULL
NULL
NULL
TLS NULL WITH NULL NULL
MD5
NULL
RSA
TLS RSA WITH NULL MD5
SHA-1
RSA
NULL
TLS RSA WITH NULL SHA
MD5
RC4 40
RSA
TLS RSA EXPORT WITH RC4 40 MD5
MD5
RC4 128
RSA
TLS RSA WITH RC4 128 MD5
SHA-1
RC4 128
RSA
TLS RSA WITH RC4 128 SHA
MD5
RC2 40
TLS RSA EXPORT WITH RC2 CBC 40 MD5 RSA
SHA-1
RSA
IDEA
TLS RSA WITH IDEA CBC SHA
SHA-1
DES40
RSA
TLS RSA EXPORT WITH DES40 CBC SHA
DES
RSA
SHA-1
TLS RSA WITH DES CBC SHA
3DES EDE SHA-1
RSA
TLS RSA WITH 3DES EDE CBC SHA

Figure 12.7. Standard TLS cipher suites with NULL or RSA key exchange.

From Cryptography to Communication Security

303

Hash
Key Exchange Cipher
CipherSuite
SHA-1
DES40
DH DSS
TLS DH DSS EXPORT WITH DES40 CBC SHA
DES
DH DSS
SHA-1
TLS DH DSS WITH DES CBC SHA
3DES EDE SHA-1
DH DSS
TLS DH DSS WITH 3DES EDE CBC SHA
SHA-1
DES40
DH RSA
TLS DH RSA EXPORT WITH DES40 CBC SHA
DES
DH RSA
SHA-1
TLS DH RSA WITH DES CBC SHA
3DES EDE SHA-1
DH RSA
TLS DH RSA WITH 3DES EDE CBC SHA
TLS DHE DSS EXPORT WITH DES40 CBC SHA DHE DSS
DES40
SHA-1
DES
DHE DSS
SHA-1
TLS DHE DSS WITH DES CBC SHA
3DES EDE SHA-1
DHE DSS
TLS DHE DSS WITH 3DES EDE CBC SHA
SHA-1
DES40
TLS DHE RSA EXPORT WITH DES40 CBC SHA DHE RSA
DHE RSA
DES
SHA-1
TLS DHE RSA WITH DES CBC SHA
3DES EDE SHA-1
DHE RSA
TLS DHE RSA WITH 3DES EDE CBC SHA
RC4 40
DH anon
MD5
TLS DH anon EXPORT WITH RC4 40 MD5
RC4 128
DH anon
MD5
TLS DH anon WITH RC4 128 MD5
DES40
TLS DH anon EXPORT WITH DES40 CBC SHA DH anon
SHA-1
DH anon
DES
SHA-1
TLS DH anon WITH DES CBC SHA
3DES EDE SHA-1
DH anon
TLS DH anon WITH 3DES EDE CBC SHA

Figure 12.8. Standard TLS cipher suites with Difﬁe-Hellman key agreement.

DES is used in three variants: a variant limited to 40-bit keys DES40, the regular
DES, and triple DES with three keys 3DES EDE. AES is used with two different key
lengths: 128 and 256 bits. RC2 40 is another block cipher with a key of 40 bits.

At the time SSL was developed, the US export restrictions were quite drastic since
it required that secret keys were computable by anyone within an effort comparable to
an exhaustive search on 40 bits. Corresponding cipher suites are identiﬁed by the word
“EXPORT” in their name. Actually the algorithms use a secret key which is derived
from a 40-bit key and the nonces which prevent dictionary attacks. Corresponding
cipher suites still exist because of compatibility reasons.

Key Exchange Cipher
CipherSuite
RSA
TLS RSA WITH AES 128 CBC SHA
DH DSS
TLS DH DSS WITH AES 128 CBC SHA
DH RSA
TLS DH RSA WITH AES 128 CBC SHA
TLS DHE DSS WITH AES 128 CBC SHA DHE DSS
TLS DHE RSA WITH AES 128 CBC SHA DHE RSA
TLS DH anon WITH AES 128 CBC SHA DH anon
RSA
TLS RSA WITH AES 256 CBC SHA
DH DSS
TLS DH DSS WITH AES 256 CBC SHA
DH RSA
TLS DH RSA WITH AES 256 CBC SHA
TLS DHE DSS WITH AES 256 CBC SHA DHE DSS
TLS DHE RSA WITH AES 256 CBC SHA DHE RSA
TLS DH anon WITH AES 256 CBC SHA DH anon

AES 128
AES 128
AES 128
AES 128
AES 128
AES 128
AES 256
AES 256
AES 256
AES 256
AES 256
AES 256

Hash
SHA-1
SHA-1
SHA-1
SHA-1
SHA-1
SHA-1
SHA-1
SHA-1
SHA-1
SHA-1
SHA-1
SHA-1

Figure 12.9. Standard TLS cipher suites with AES.

⎛
⎜⎜⎝ seq num

⎞
⎟⎟⎠

304

12.3.3 Record Protocol

Chapter 12

When a party needs to send a message (called application data) to the other party, it is
ﬁrst split into fragments of length at most 214 bytes. Each fragment is treated separately.
A fragment is compressed using the compression algorithm of the session (if any). Then
we append a MAC to the compressed fragment and we obtain the plaintext. Next, it is
encrypted, and the ciphertext is ﬁnally sent with an SSL record header.

Upon reception of a record, the header is extracted, the ciphertext is decrypted, the
MAC is checked, then extracted, and the remaining is decompressed in order to get the
fragment.

When the hash algorithm of the cipher spec is NULL, no MAC is computed, i.e.
the MAC length is null. Otherwise, the MAC is simply an HMAC algorithm with the
speciﬁed hash function. More precisely the MAC of a fragment is computed as

HMACMAC write secret

TLSCompressed.type, TLSCompressed.version,
TLSCompressed.length
TLSCompressed.fragment

where MAC write secret is the MAC key of the sender, seq num is the sequence
number of the fragment, and remaining ﬁelds are the compressed fragment with its
actual length and some additional information about the TLS protocol (namely, the
compression algorithm) that is being used.

12.3.4 Stream Cipher

The RC4 stream cipher is used as a key-stream generator with one-time pad. The
internal state of the generator is kept in the connection state so that the RC4 automaton
continuously generates keystreams in order to encrypt the sequence of fragments.

12.3.5 Block Cipher

Since block ciphers are used in CBC mode, the plaintext must be converted into an
integral sequence of blocks. For this we append a padding to the plaintext and a padding
length of 1 byte. The padding length must be equal to all bytes of the padding, and the
total length (the plaintext, the padding, and the padding length) must be a multiple of
the block size. When the ciphertext is decrypted, the last byte speciﬁes the length of the
padding to be removed. The padding structure is also checked and an error is issued if
it is not valid.

Note that the padding does not need to be the shortest one. It can actually be longer

in order to hide the real size of the plaintext to a potential adversary.

From Cryptography to Communication Security

305

Client

Server

ClientHello:acceptable cipher suites, nonceC

−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− select TLS RSA cipher hash
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ decrypt

ServerHello:TLS RSA cipher hash, certificate, nonceS

ClientKeyExchange:RSA ENC(pre master secret)

pick pre master secret

Figure 12.10. TLS key exchange using RSA.

The initial vector (IV) which is used in the CBC mode is a secret pseudorandom
value. The IV value for the next record is simply the last ciphertext block so that, like
in the stream cipher mode, we can view the sequence of all (compressed and MACed)
fragments as a unique plaintext to be encrypted in CBC mode. The very ﬁrst IV value of
a connection is generated together with the secret keys from the nonces and a master se-
cret. (For export cipher suites, the master secret is not used so that IV is not secret at all.)

12.3.6 Master Key Exchange

The key exchange protocol which is speciﬁed in the current cipher suite is used in order
to set up a pre-master secret. As we have seen in the cipher suites, there are six possible
protocols.

RSA: The client chooses the secret and encrypts it using the RSA public key of the
server (see Fig. 12.10). This public key must be authenticated in a certiﬁcate.
Encryption follows the PKCS#1v1.5 standard.

DH DSS and DH RSA: These are “ﬁxed Difﬁe-Hellman” algorithms in which
long-term Difﬁe-Hellman parameters are used. The-Difﬁe-Hellman parame-
ters p and g are put in the certiﬁcate of the server, as well as the Difﬁe-Hellman
public key gx mod p of the server. The certiﬁcate is signed using either DSS
or RSA to authenticate the keys. So the client can just take the authenticated
Difﬁe-Hellman parameters from the certiﬁcate, pick his Difﬁe-Hellman pub-
lic value g y mod p, and send it to the server in the ClientKeyExchange
message (see Fig. 12.11).

DHE DSS and DHE RSA: These are “ephemeral Difﬁe-Hellman” algorithms in
which the Difﬁe-Hellman parameters are randomly selected by the client and
the server. The server certiﬁcate contains either a DSS or a RSA public key.
The server can select his chosen Difﬁe-Hellman parameters and public value
( p, g, gx mod p), hash them with the selected hash function, sign them with

Client

pick y

Server

ClientHello:acceptable cipher suites, nonceC

−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− select TLS DH sig cipher hash
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ pre master secret = gxy mod p

ServerHello:TLS DH sig cipher hash, certificate, nonceS

ClientKeyExchange:gy mod p

Figure 12.11. TLS key exchange using DH DSS or DH RSA.

306

Client

pick y

Chapter 12

Server

ClientHello:acceptable cipher suites, nonceC

−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− select TLS DHE sig cipher hash
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− select p,g, pick x
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ pre master secret = gxy mod p

ServerHello:TLS DHE sig cipher hash, certificate, nonceS
ServerKeyExchange:p,g,gx mod p,sig(hash(p,g,gx mod p))

ClientKeyExchange:gy mod p

Figure 12.12. TLS key exchange using DHE DSS or DHE RSA.

his key and the selected authentication algorithm. The signed parameters are
sent in an additional ServerKeyExchange message which immediately
follows the ServerHello message. So the client can just take the signed
Difﬁe-Hellman parameters, check the signature by using the authenticated
public key from the certiﬁcate, pick his Difﬁe-Hellman public value g y mod
p, and send it to the server in the ClientKeyExchange message (see
Fig. 12.12).

DH anon: This is a particular case of the previous Difﬁe-Hellman protocols in
which the parameters are not authenticated. It does not require any certiﬁ-
cate, but it is vulnerable to a man-in-the-middle attack. Like in the ephemeral
Difﬁe-Hellman algorithms, the server selects his chosen Difﬁe-Hellman pa-
rameters and public value ( p, g, gx mod p) and sends them in an additional
ServerKeyExchange message. So the client can just take the Difﬁe-
Hellman parameters, pick his Difﬁe-Hellman public value g y mod p, and
send it to the server in the ClientKeyExchange message (see Fig. 12.13).

12.3.7 Key Derivation

A hash algorithm hash deﬁnes a pseudorandom generator P hash. Given a secret
secret and a seed seed, we deﬁne a sequence P hash(secret, seed) by

P hash(secret, seed) = r1, r2, r3, . . .

where ri = HMAChash(secret, ai , seed), ai = HMAChash(secret, ai−1), and a0 =
seed.

Client

pick y

Server

ClientHello:acceptable cipher suites, nonceC

ServerHello:TLS DH anon cipher hash, nonceS

−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− select TLS DH anon cipher hash
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− select p,g, pick x
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ pre master secret = gxy mod p

ServerKeyExchange:p,g,gx mod p

ClientKeyExchange:gy mod p

Figure 12.13. TLS key exchange using DH anon.

From Cryptography to Communication Security

307

?

?

nonceC
nonceS

?

?

pre master secret -

PRF

- master secret -

PRF

-
-
-
-
-
-

Aut. C → S
Aut. S → C
Enc. C → S
Enc. S → C
IV C → S
IV S → C

Figure 12.14. Key derivation in SSL.

Given secret secret, a seed seed, and a string label we further deﬁne a sequence

PRF(secret, label, seed) by
PRF(secret, label, seed)= P MD5(S1, label||seed) ⊕ P SHA1(S2, label||seed)

where S1 and S2 are the two halves of secret. (If secret has an odd length, its middle
byte is both the last byte of S1 and the ﬁrst byte of S2.)

PRF is used in order to compute the master secret. For this we just take the ﬁrst

48 bytes of
master secret = PRF(pre master secret, “master secret”, nonceC||nonceS).

As illustrated in Fig. 12.14, PRF is also used in order to generate a key block from the
master secret as follows.

key block = PRF(master secret, “key expansion”, nonceS||nonceC )

This key block is the concatenation of the four secret keys and the two initial vectors
which are used in the cipher spec.

PRF is also used to compute the two 12-byte MAC of the handshake (one MACC

from the client, one MACS from the server) by

h handshake = MD5(handshake)||SHA1(handshake)

MACC = PRF(master secret, “client finished”, h handshake)
MACS = PRF(master secret, “server finished”, h handshake)

where handshake is the concatenation of all handshake messages.

12.4 PGP: Pretty Good Privacy

Unlike SSL which is dedicated to security of on-line communication, PGP brings
security in an off-line way: signature and encryption of e-mails, archives, etc. PGP was

308

Chapter 12

ﬁrst designed by Phil Zimmermann, in the United States, in the nineties. At that time, this
country (and some others) looked like a dictatorship from the regulation of cryptography
viewpoint. It was basically illegal to prevent authorities from having access to any
cleartext by the means of strong cryptography. Several people, like Zimmermann,
fought against it by developing appropriate (illegal) software in order to provide access
to strong privacy for any individual. Today, rules are more liberal and we can use PGP
without betraying the US law.2

PGP is available in a commercial and a freeware version. Since there are still strong
restrictions for export of cryptographic applications, there is one version to be used
within the United States, and another one called PGPi for international usage.3

There is also a Gnu version of PGP called GPG as for GnuPGP which is available

as the RFC 2440 (Ref. [40]).

12.4.1 Security for Individuals

Due to its origins, PGP is easy to set up without any corporate help. For this reason,
certiﬁcates do not rely on any authority, we do not use any public parameter, and anyone
can freely generate his own key and choose his cryptographic algorithm.

PGP can be used in order to encrypt, decrypt, hash, sign, or verify digital ﬁles.
These can be archives or just e-mails. Popular algorithms in PGP are IDEA symmet-
ric encryption, RSA encryption or signature, and MD5 hash function. Many other
algorithms are available as well.

PGP has a nice feature which enables protecting unreadable ﬁles (like ciphertexts,
signatures, hashed values, or even cryptographic keys) by encoding them into a readable
form. This uses the Radix-64 code, which is also called base64 in the Multipurpose
Internet Mail Extensions (MIME) standard (see Ref. [70]). This feature is called the
ASCII armor in PGP. Files in the ASCII armor format have the .asc ﬁle extension.

When PGP is used for e-mails, human beings can usually see which version of
PGP, which algorithm, and which key length are used.4 The PGP software recognizes it
by itself, so this is more for transparency and education of users than for making their
life complicated. As an example, here is a signed message.

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

PGP makes cryptographic messages readable for human beings.

2 See, Ref. [116] for more information about historical details.
3 Ref. [73] is a reference book for PGP. One can refer to it for more details about using PGP.
4 Human beings actually could see it, but this feature seems to have disappeared nowadays in versions of

PGP, so the user cannot see it by default unless he really asks for it.

From Cryptography to Communication Security

309

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.4 (GNU/Linux)

iD8DBQFBA4c1/LSQdhvwJ58RAjzEAKCXHnwQHNGbX2Bzjo3AMZHABWTW5wCgkx
VLrq22vPs5vlR6RZOf1zEDSF4=
=cVzf
-----END PGP SIGNATURE-----

One can see that the message hashed value (using SHA1) was signed by GnuPGP
version 1.2.4. If the user asks for the signature veriﬁcation, the following message is
returned.

gpg: Signature made Sun 25 Jul 2004 12:11:01 PM CEST using DSA
key ID 1BF0279F
gpg: Good signature from "Serge Vaudenay <serge.vaudenay@epfl
.ch>"

So one realizes that the signature is a DSA signature and that the public key is identiﬁed
in the key ring as belonging to the named person.

When doing a cryptographic operation, PGP may need cryptographic keys which
are hardly manageable for a human user. Symmetric keys can be prompted to the user.
They are usually derived from a pass phrase which is freely chosen by the user by using
a hash function.

Asymmetric keys are more problematic since they are in a speciﬁc mathematical
format. For instance an RSA key is a pair of a modulus and an exponent. We cannot
derive them from a human pass phrase. For this, PGP retrieves the key from a key ring.
The user just needs to provide an identiﬁer for the key to be used. When the key is a
secret one, it may be encrypted by a symmetric algorithm. Therefore, PGP needs to
ask for the symmetric key by prompting for the corresponding pass phrase.

As an example, here is a listing of a public key ring.

vaudenay@lasecpc7:~> gpg --list-public-keys
/home/vaudenay/.gnupg/pubring.gpg
---------------------------------
pub 1024D/1BF0279F 2004-07-25 Serge Vaudenay
<serge.vaudenay@epfl.ch>

pub 1024D/8EB9124A 2004-07-25 Student <student@epfl.ch>

pub 1536R/27295F6B 2004-07-25 Colleague <colleague@epfl.ch>

It may look quite cryptic. The second ﬁeld tells the bit length and the scheme. For
instance, 1024D means a 1024-bit key for DSA, 1024g means a 1024-bit key for

310

Chapter 12

ElGamal, and 1536R means a 1536-bit key for RSA. The date of creation as well as
the name (e-mail address) of the owner is provided.

PGP makes extensive usage of checksums and cryptographic digests so that bad

pass phrases or modiﬁed ﬁles are easily detected.

12.4.2 Public-Key Management

A user can create his asymmetric keys. He just needs to select the algorithm and key
size, and to provide enough randomness for PGP to generate it in a random way. For this,
PGP analyzes key strokes on the keyboard (people calls this an “entropy collector”) and
makes random bits from time intervals between key strokes by using a pseudorandom
generator. Once an asymmetric key pair is created, the secret part is encrypted with
a symmetric key which is derived from a pass phrase. Both the public key and the
encrypted secret key are stored in the corresponding key ring.

PGP provides commands in order to manage key rings: extracting, adding, chang-

ing keys, etc. Those commands are as secure and user friendly as possible.

When a user is given a public key from another one, he can insert it in his key
ring. At the same time, he determines the level of trust he can attribute to the validity
of that key. For instance, if the key was given hand to hand, he can fairly trust that the
key is valid. If the key was taken from a Web site through insecure connection, he may
give a low conﬁdence to the validity. Additionally, the public key can be certiﬁed by
a third party that the user trusts, or partially trusts. The trust path can of course have
more than one intermediate parties. There can be several trust paths to the public key.
The public key can be inserted in the key ring with this extra kind of certiﬁcate. The
user must therefore permanently manage keys with different trust levels and be aware
of potential weak trusts.

PGP users have deﬁned the notion of web of trust in which keys are vertices and
oriented edges means that one key certiﬁed the other one. Trust paths are simply paths
in this graph. The shorter the paths the higher the trust conﬁdence.

12.4.3 Security Weaknesses

Since PGP was made in order to be fully controlled by the user, security issues occur
when it is not well used. For this a minimal education on cryptography is required in
order to securely use it. People must be educated on how to choose and use a pass
phrase, on what a key size means, on how the public-key management works, and on
how sensitive a key ring can be.

Among one of the weakest point, the key infrastructure heavily relies on trust. The
authentication of public keys is not controlled by any central authority (PGP was made

From Cryptography to Communication Security

311

in order to avoid it), so depending on how secure an application should be, users really
need to authenticate their public keys accordingly.

Also, a key may eventually get compromised (for instance if a pass phrase is
intercepted when typing it), and key revocation is ad hoc based. When one wants to
revoke his key, he must broadcast a revocation message to any potential user of his
public key. Indeed, no central revocation list is available for the same reasons. Of
course, one can set up an authority for certiﬁcation and revocation of PGP keys, but
PGP was not done for that and careless users will naturally bypass this feature.

For these reasons PGP is mostly used for small ﬁxed communities in an ad hoc

way.

12.5 Exercises

Exercise 12.1. RC5-CBC-PAD is speciﬁed in the informative Internet document
RFC 2040. It describes how to pad digital messages (represented as a sequence of
bytes) in order to be encrypted via block cipher RC5 in CBC mode. Here is how it

works.r Take the message x1, . . . , x(cid:9) as a sequence of (cid:9) bytes.

r Take an integer p such that (cid:9) + p is a multiple of 8 and that 1 ≤ p ≤ 8.
r Let xi = p for i = (cid:9) + 1, . . . , (cid:9) + p.
r Take the byte sequence x1, . . . , x(cid:9)+ p and rewrite it as a block sequence
r Encrypt the block sequence via RC5 in CBC mode, and obtain the encrypted

B1, . . . , B (cid:9)+ p

.

8

message C1, . . . , C (cid:9)+ p
1. Show that p is essentially unique and express its value with a mathematical

.

8

formula.

2. Explain how the Ci are computed.
3. We assume that the receiver of the encrypted message ﬁrst decrypts in CBC
mode, then checks if the padding is correct, and ﬁnally extracts the clear text.
Carefully explain how all this is performed (for instance by writing a computer
program).
4. Given a ciphertext y = (C1, . . . , Cn), let O(y) be equal to 1 if the padding
check is correct after the RC5-CBC decryption or equal to 0 otherwise. By
using subroutine calls to the O oracle, write a program which given a block
C computes RC5
Hint: Submit ciphertexts with the form (R, C) for a carefully chosen block R.
5. By using the previous question show how to decrypt any message by having
access to O only.

−1(C).

6. In order to ﬁx the scheme, we decide to encrypt twice with RC5 in CBC
mode. Namely, we add an extra step in the previous scheme by re-encrypting
C1, . . . , C (cid:9)+ p
. Make a picture of
the encryption scheme. Show that a similar attack holds: we can still decrypt

in CBC mode and obtaining C

, . . . , C

(cid:8)
(cid:9)+ p
8

(cid:8)
1

8

312

Chapter 12

any message by having access to an oracle which says whether or not the
decrypted message is correctly padded.

7. Recall how block ciphers are used in order to encrypt SSL fragments. Deduce

an attack against SSL.

Conclude about how to integrate cryptographic schemes.5

5 This exercise was inspired by Refs. [21, 41, 182].

Further Readings

The reader familiar with the content of the present textbook may want to have a deeper look at
some selected (or missing) topics here. We suggest the following references.

C. Boyd, A. Mathuria. Protocols for Authentication and Key Establishment. Springer-Verlag,

NY, 2003.
An excellent textbook on peer authentication and key establishment protocols.

R. Crandall, C. Pomerance. Prime Numbers: A Computational Perspective. Springer, NY, 2001.

This book tells much more on primality tests and factoring algorithms.

M.R. Garey, D.S. Johnson. Computers and Intractability—A Guide to the Theory of NP-

Completeness. Freeman, NY, 1979.
This is another fundamental reference about complexity theory. It contains a nice
catalog of NP-complete problems.

O. Goldreich. Modern Cryptography, Probabilistic Proofs and Pseudorandomness. Algorithms

and Combinatorics, vol. 17. Springer-Verlag, NY, 1999.
A survey on the fascinating area of randomness and the power of interaction. It is more
oriented to complexity theory, but it is hard to formally talk about randomness without
it.

D. Gollman. Computer Security. John Wiley & Sons, NY, 1999.

An excellent textbook on computer and network security. It is more oriented to the
integration and the theory behind. It addresses aspects of security related to hardware,
operation systems, networks and distributed systems, and management.

J.E. Hopcroft, J.D. Ullman. Introduction to Automata Theory, Languages, and Computation.

Addison Wesley, London, 1979.
This is an outstanding reference on the foundations of computer sciences. It con-
tains necessary information about complexity theory (Turing machines, complexity,
intractability).

S. Katzenbeisser. Recent Advances in RSA Cryptography. Advances in Information Security,

vol. 3. Kluwer Academic Publishers, Boston, 2001.
This is a complete survey on RSA cryptography.

W. Mao. Modern Cryptography—Theory and Practice. Prentice Hall PTR, Upper Saddle River,

NJ, 2003.
An excellent textbook about issues related to what is called “textbook cryptography,” or
how theoretical cryptography may fail to implement security. It is somewhat a textbook
on nontextbook cryptography.

314

Further Readings

A.J. Menezes, P.C. van Oorschot, S.A. Vanston. Handbook of Applied Cryptography. CRC, NY,

1997.
This is a reference textbook. This 780-page book is not meant to be read from the
beginning to the end. It is an excellent handbook for ﬁnding references, deﬁnitions,
related topics, etc.

M.A. Nielsen, I.L. Chuang. Quantum Computation and Quantum Information. Cambridge Uni-

versity Press, Cambridge, 2000.
A reference textbook about current research on quantum computing. It notably in-
cludes quantum cryptography and the Shor polynomial time factorization algorithm.
If quantum computers become a reality, this might become a classical lecture in every
computer science curriculum.

J. Stern. La Science du Secret. Odile Jacob, Paris, 1998.

A philosophical dissertation (in French) on cryptography by Jacques Stern. Interesting
for people who want to understand the rise of modern cryptography in theoretical
computer sciences and the real technical advances which led to modern cryptography.

Bibliography

[1] Advanced Encryption Standard (AES). Federal Information Processing Standards Publica-
tion #197. U.S. Department of Commerce, National Institute of Standards and Technology,
2001.

[2] ANSI X9.17. American National Standard Institute. Financial Institution Key Management

(Wholesale). ASC X9 Secretariat, American Bankers Association, 1986.

[3] ANSI X9.62. Public Key Cryptography for the Financial Services Industry: The Ellip-
tic Curve Digital Signature Algorithm (ECDSA). American National Standard Institute.
American Bankers Association, 1998.

[4] Bluetooth. Speciﬁcation of the Bluetooth System. Version 1.2, November 2003. Available

at http://www.bluetooth.org.

[5] Data Encryption Standard (DES). Federal Information Processing Standard publication
#46–3. U.S. Department of Commerce, National Institute of Standards and Technology,
1999.

[6] DES Modes of Operation. Federal Information Processing Standard publication #81. U.S.
Department of Commerce—National Bureau of Standards, National Technical Information
Service, Springﬁeld, VA, 1980.

[7] Digital Signature Standard. Federal Information Processing Standard publication #186.

U.S. Department of Commerce, National Institute of Standards and Technology, 1994.

[8] Digital Signature Standard (DSS). Federal Information Processing Standards publication
#186-2. U.S. Department of Commerce, National Institute of Standards and Technology,
2000.

[9] ETSI. Universal Mobile Telecommunications System (UMTS); Speciﬁcation of the 3GPP
Conﬁdentiality and Integrity Algorithms. Document 2: Kasumi Algorithm speciﬁcation
(3GPP TS 35.202 Version 3.1.2 Release 1999). Available at http://www.etsi.org/.

[10] ISO/IEC 9796.

Information Technology—Security Techniques—Digital Signature
Scheme Giving Message Recovery. International Organization for Standardization,
Geneva, 1991.

[11] ISO/IEC 9797. Data Cryptographic Techniques—Data Integrity Mechanism Using a Cryp-
tographic Check Function Employing a Block Cipher Algorithm. International Organiza-
tion for Standardization, Geneva, 1989.

316

Bibliography

[12] ISO/IEC 14888. Information Technology—Security Techniques—Digital Signature with

Appendix. International Organization for Standardization, Geneva, 1998.

[13] PKCS#1v1: RSA Cryptography Standard. An RSA Laboratories Technical Report Version

1.5. RSA Laboratories, 1993.

[14] PKCS#1v2.1: RSA Cryptography Standard. RSA Security Inc. Public-Key Cryptography

Standards. RSA Laboratories, 2002.

[15] Secure Hash Standard. Federal Information Processing Standard publication #180. U.S.

Department of Commerce, National Institute of Standards and Technology, 1993.

[16] Secure Hash Standard. Federal Information Processing Standard publication #180-1. U.S.

Department of Commerce, National Institute of Standards and Technology, 1995.

[17] Secure Hash Standard. Federal Information Processing Standard publication #180-2. U.S.

Department of Commerce, National Institute of Standards and Technology, 2002.

[18] Speciﬁcation of the Bluetooth System. Vol. 2: Core System Package. Bluetooth Speciﬁ-

cation Version 1.2, 2003.

[19] A.V. Aho, J.E. Hopcroft, J.D. Ullman. The Design and Analysis of Computer Algorithms.

Addison-Wesley, London, 1974.

[20] J.L. Balc´azar, J. D´ıaz, J. Gabarr´o. Structural Complexity I. EATC. Springer, NY, 1988.

[21] R. Baldwin, R. Rivest. The RC5, RC5-CBC, RC5-CBC-Pad, and RC5-CTS Algorithms.

RFC 2040, 1996.

[22] J. Bamford. The Puzzle Palace. Penguin Books, NY, 1983.

[23] D. Bayer, S. Haber, W.S. Stornetta. Improving the Efﬁciency and Reliability of Digital
Time-Stamping. In Sequences II: Methods in Communication, Security, and Computer
Science. Springer-Verlag, NY, 1993, pp. 329–334

[24] M. Bellare, R. Canetti, H. Krawczyk. Keyed Hash Functions and Message Authentication.
In Advances in Cryptology CRYPTO’96, Santa Barbara, CA, Lecture Notes in Computer
Science 1109. Springer-Verlag, NY, 1996, pp. 1–15.

[25] M. Bellare, A. Desai, D. Pointcheval, P. Rogaway. Relations among Notions of Security for
Public-Key Encryption Schemes. In Advances in Cryptology CRYPTO’98, Santa Barbara,
CA, Lecture Notes in Computer Science 1462. Springer-Verlag, NY, 1998, pp. 26–45.

[26] M. Bellare, P. Rogaway. Optimal Asymmetric Encryption—How to Encrypt with RSA.
In Advances in Cryptology EUROCRYPT’94, Perugia, Italy, Lecture Notes in Computer
Science 950. Springer-Verlag, NY, 1995, pp. 92–111.

[27] J. Benaloh, J. Leichter. Generalized Secret Sharing and Monotone Functions. In Advances
in Cryptology CRYPTO’88, Santa Barbara, CA, Lecture Notes in Computer Science 403.
Springer-Verlag, NY, 1990, pp. 27–35.

Bibliography

317

[28] E. Biham, A. Shamir. Differential Cryptanalysis of DES-like Cryptosystems. In Advances
in Cryptology CRYPTO’90, Santa Barbara, CA, Lecture Notes in Computer Science 537.
Springer-Verlag, NY, 1991, pp. 2–21.

[29] E. Biham, A. Shamir. Differential Cryptanalysis of DES-Like Cryptosystems. Journal of

Cryptology, vol. 4, pp. 3–72, 1991.

[30] E. Biham, A. Shamir. Differential Cryptanalysis of the Full 16-Round DES. In Advances
in Cryptology CRYPTO’92, Santa Barbara, CA, Lecture Notes in Computer Science 740.
Springer-Verlag, NY, 1993, pp. 487–496.

[31] E. Biham, A. Shamir. Differential Cryptanalysis of the Data Encryption Standard.

Springer-Verlag, NY, 1993.

[32] A. Biryukov, A. Shamir, D. Wagner. Real Time Cryptanalysis of A5/1 on a PC. In Fast
Software Encryption’00, New York, Lecture Notes in Computer Science 1978. Springer-
Verlag, NY, 2001, pp. 1–18.

[33] D. Bleichenbacher. Generating ElGamal Signatures without Knowing the Secret Key. In
Advances in Cryptology EUROCRYPT’96, Zaragoza, Spain, Lecture Notes in Computer
Science 1070. Springer-Verlag, NY, 1996, pp. 10–18.

[34] D. Bleichenbacher. Chosen Ciphertext Attack against Protocols Based on the RSA En-
cryption Standard PKCS#1. In Advances in Cryptology CRYPTO’98, Santa Barbara, CA,
Lecture Notes in Computer Science 1462. Springer-Verlag, NY, 1998, pp. 1–12.

[35] D. Boneh, R.A. DeMillo, R.J. Lipton. On the Importance of Checking Cryptographic
Protocols for Faults. In Advances in Cryptology EUROCRYPT’97, Konstanz, Germany,
Lecture Notes in Computer Science 1233. Springer-Verlag, NY, 1997, pp. 37–51.

[36] J. Boyar. Inferring Sequences Produced by Pseudorandom Number Generators. Journal of

the ACM, vol. 36, pp. 129–144, 1989.

[37] C. Boyd, A. Mathuria. Protocols for Authentication and Key Establishment. Springer, NY,

2003.

[38] E. Brickell, D. Pointcheval, S. Vaudenay, M. Yung. Design Validations for Discrete
Logarithm Based Signature Schemes. In Public Key Cryptography’00, Melbourne,
Australia, Lecture Notes in Computer Science 1751. Springer-Verlag, NY, 2000, pp. 276–
292.

[39] C. Cachin. Entropy Measures and Unconditional Security in Cryptography. Hartung–

Gorre, Konstang, ETH Series in Information Security and Cryptography, vol. 1, 1997.

[40] J. Callas, L. Donnerhacke, H. Finney, R. Thayer. OpenPGP Message Format. Internet

Standard. RFC 2440, The Internet Society, 1998.

[41] B. Canvel, A. Hiltgen, S. Vaudenay, M. Vuagnoux. Password Interception in a SSL/TLS
Channel. In Advances in Cryptology CRYPTO’03, Santa Barbara, CA, Lecture Notes in
Computer Science 2729. Springer-Verlag, NY, 2003, pp. 583–599.

318

Bibliography

[42] J.L. Carter, M.N. Wegman. Universal Classes of Hash Functions. Journal of Computer

and System Sciences, vol. 18, pp. 143–154, 1979.

[43] F. Chabaud, A. Joux. Differential Collisions in SHA-0.

In Advances in Cryptology
CRYPTO’98, Santa Barbara, CA, Lecture Notes in Computer Science 1462. Springer-
Verlag, NY, 1998, pp. 56–71.

[44] F. Chabaud, S. Vaudenay. Links between Differential and Linear Cryptanalysis. In Advances
in Cryptology EUROCRYPT’94, Perugia, Italy, Lecture Notes in Computer Science 950.
Springer-Verlag, NY, 1995, pp. 356–365.

[45] D. Chaum, H. van Antwerpen. Undeniable Signatures.

In Advances in Cryptology
CRYPTO’89, Santa Barbara, CA, Lecture Notes in Computer Science 435. Springer-
Verlag, NY, 1990, pp. 212–217.

[46] P. Chown. Advanced Encryption Standard (AES) Ciphersuites for Transport Layer Security

(TLS). Internet Standard. RFC 3268, The Internet Society, 2002.

[47] S.A. Cook. The Complexity of Theorem-Proving Procedures. In Proceedings of the 3rd
ACM Symposium on Theory of Computing, Atlanta, GA. ACM Press, NY, 1971, pp. 151–
158.

[48] D. Coppersmith. The Data Encryption Standard (DES) and Its Strength against Attacks.

IBM Journal of Research and Development, vol. 38, pp. 243–250, 1994.

[49] D. Coppersmith. Finding a Small Root of a Univariate Modular Equation. In Advances in
Cryptology EUROCRYPT’96, Zaragoza, Spain, Lecture Notes in Computer Science 1070.
Springer-Verlag, NY, 1996, pp. 155–165.

[50] D. Coppersmith, M. Franklin, J. Patarin, M. Reiter. Low-Exponent RSA with Related
Messages. In Advances in Cryptology EUROCRYPT’96, Zaragoza, Spain, Lecture Notes
in Computer Science 1070. Springer-Verlag, NY, 1996, pp. 1–9.

[51] J.-S. Coron, D. Naccache, J.P. Stern. On the Security of RSA Padding. In Advances in
Cryptology CRYPTO’99, Santa Barbara, CA, Lecture Notes in Computer Science 1666.
Springer-Verlag, NY, 1999, pp. 1–18.

[52] T. Cover, J. Thomas. Elements of Information Theory. John Wiley & Sons, NY, 1991.

[53] R. Crandall, C. Pomerance. Prime Numbers: A Computational Perspective. Springer, NY,

2001.

[54] J. Daemen, V. Rijmen. The Design of Rijndael. Information Security and Cryptography,

Springer, NY, 2002.

[55] J. Daemen, L. Knudsen, V. Rijmen. The Block Cipher SQUARE. In Fast Software Encryp-
tion’97, Haifa, Israel, Lecture Notes in Computer Science 1267. Springer-Verlag, NY,
1997, pp. 149–165.

Bibliography

319

[56] I.B. Damg˚ard. A Design Principle for Hash Functions.

In Advances in Cryptology
CRYPTO’89, Santa Barbara, CA, Lecture Notes in Computer Science 435. Springer-
Verlag, NY, 1990, pp. 416–427.

[57] D.W. Davies, G.I.P. Parkin. The Average Cycle Size of the Key Stream in Output Feedback
Encipherment. In Advances in Cryptology, Proceedings of CRYPTO’82, 1982, pp. 97–98.
Re-edited by Springer, NY, LNCS vol. 1440, 1998.

[58] T. Dierks, C. Allen. The TLS Protocol Version 1.0. Internet Standard. RFC 2246, The

Internet Society, 1999.

[59] W. Difﬁe, M.E. Hellman. New Directions in Cryptography. IEEE Transactions on Infor-

mation Theory, vol. IT-22, pp. 644–654, 1976.

[60] W. Difﬁe, S. Landau. Privacy on the Line—The Politics of Wiretapping and Encryption.

The MIT Press, Cambridge, MA, 1998.

[61] H. Dobbertin. The First Two Rounds of MD4 are Not One-Way. In Fast Software Encryp-
tion’98, Paris, France, Lecture Notes in Computer Science 1372. Springer-Verlag, NY,
1998, pp. 284–292.

[62] H. Dobbertin. Cryptanalysis of MD4. Journal of Cryptology, vol. 11, pp. 253–271,

1998.

[63] T. ElGamal. Cryptography and Logarithms over Finite Fields. PhD Thesis, Stanford

University, 1984.

[64] T. ElGamal. A Public-Key Cryptosystem and a Signature Scheme Based on Discrete
Logarithms. In Advances in Cryptology CRYPTO’84, Santa Barbara, CA, Lecture Notes
in Computer Science 196. Springer-Verlag, NY, 1985, pp. 10–18.

[65] T. ElGamal. A Public-Key Cryptosystem and a Signature Scheme Based on Discrete

Logarithms. IEEE Transactions on Information Theory, vol. IT-31, pp. 469–472, 1985.

[66] U. Feige, A. Fiat, A. Shamir. Zero-Knowledge Proofs of Identity. Journal of Cryptology,

vol. 1, pp. 77–94, 1988.

[67] N. Ferguson, B. Schneier. Practical Cryptography. John Wiley & Sons, NY, 2003.

[68] A. Fiat, A. Shamir. How to Prove Yourself: Practical Solutions to Identiﬁcation
and Signature Problems.
In Advances in Cryptology CRYPTO’86, Santa Barbara,
CA, Lecture Notes in Computer Science 263. Springer-Verlag, NY, 1987, pp. 186–
194.

[69] J. Franks, P. Hallam-Baker, J. Hostetler, S. Lawrence, P. Leach, A. Luotonen, L. Stew-
art. HTTP Authentication: Basic and Digest Access Authentication. Internet Standard.
RFC 2617, The Internet Society, 1999.

320

Bibliography

[70] N. Freed, N. Borenstein. Multipurpose Internet Mail Extensions (MIME) Part One: Format

of Internet Message Bodies. Internet Standard. RFC 2045, 1996.

[71] E. Fujisaki, T. Okamoto, D. Pointcheval, J. Stern. RSA-OAEP is Secure under the RSA
Assumption. In Advances in Cryptology CRYPTO’01, Santa Barbara, CA, Lecture Notes
in Computer Science 2139. Springer-Verlag, NY, 2001, pp. 260–274.

[72] M.R. Garey, D.S. Johnson. Computers and Intractability— A Guide to the Theory of NP-

Completeness. Freeman, NY, 1979.

[73] S. Garﬁnkel. PGP—Pretty Good Privacy. O’Reilly, Cambridge, 1995.

[74] H. Gilbert. Cryptanalyse Statistique des Algorithmes de Chiffrement et S´ecurit´e des

Sch´emas d’Authentiﬁcation. Th`ese de Doctorat de l’Universit´e de Paris 11, 1997.

[75] H. Gilbert, G. Chass´e. A Statistical Attack of the FEAL-8 Cryptosystem. In Advances
in Cryptology CRYPTO’90, Santa Barbara, CA, Lecture Notes in Computer Science 537.
Springer-Verlag, NY, 1991, pp. 22–33.

[76] O. Goldreich. Modern Cryptography, Probabilistic Proofs and Pseudorandomness. Algo-

rithms and Combinatorics vol. 17. Springer-Verlag, NY, 1999.

[77] S. Goldwasser, S. Micali. Probabilistic Encryption. Journal of Computer and System Sci-

ences, vol. 28, pp. 270–299, 1984.

[78] S. Goldwasser, S. Micali, C. Rackoff. The Knowledge Complexity of Interactive Proof

Systems. SIAM Journal on Computing, vol. 18, pp. 186–208, 1989.

[79] S. Goldwasser, S. Micali, R.L. Rivest. A “Paradoxical” Solution to the Signature Problem.
In Advances in Cryptology CRYPTO’84, Santa Barbara, CA, Lecture Notes in Computer
Science 196. Springer-Verlag, NY, 1985, p. 467.

[80] S. Goldwasser, S. Micali, R.L. Rivest. A Digital Signature Scheme Secure against Adap-
tive Chosen-Message Attacks. SIAM Journal on Computing, vol. 17, pp. 281–308,
1988.

[81] F. Grieu. A Chosen Messages Attack on the ISO/IEC 9796-1 Signature Scheme.

In
Advances in Cryptology CRYPTO’00, Santa Barbara, CA, Lecture Notes in Computer
Science 1880. Springer-Verlag, NY, 2000, pp. 70–80.

[82] L. Guillou, J.-J. Quisquater. A “Paradoxical” Identity-Based Signature Scheme Result-
ing from Zero-Knowledge.
In Advances in Cryptology CRYPTO’88, Santa Barbara,
CA, Lecture Notes in Computer Science 403. Springer-Verlag, NY, 1990, pp. 216–
231.

[83] S.C. Kleene. Representation of Events in Nerve Nets. In Automata Studies, C.E. Shannon
and M. McCarthy (Eds.), Annals of Mathematical Studies, vol. 34. Princeton University
Press, Princeton, 1956, pp. 3–41.

Bibliography

321

[84] S. Haber, W.S. Stornetta. How to Time-Stamp a Digital Document. Journal of Cryptology,

vol. 3, pp. 99–111, 1991.

[85] N. Haller. The S/KEY One-Time Password System. RFC 1760, 1995.

[86] N. Haller, C. Metz, P. Nesser, M. Straw. A One-Time Password System. Internet Standard.

RFC 2289, The Internet Society, 1998.

[87] J. H˚astad. Solving Simultaneous Modular Equations of Low Degree. SIAM Journal on

Computing, vol. 17, pp. 376–404, 1988.

[88] M.E. Hellman. A Cryptanalysis Time–Memory Trade-Off. IEEE Transactions on Infor-

mation Theory, vol. IT-26, pp. 401–406, 1980.

[89] M.E. Hellman, R. Merkle, R. Schroeppel, L. Washington, W. Difﬁe, S. Pohlig,
P. Schweitzer. Results of an Initial Attempt to Cryptanalyze the NBS Data Encryption
Standard. Stanford University, September 1976.

[90] H.M. Heys. A Tutorial on Linear and Differential Cryptanalysis. Technical Report
CORR 2001–17, Centre for Applied Cryptographic Research, Department of Combi-
natorics and Optimization, University of Waterloo, 2001. (Also appears in Cryptologia,
vol. 26, pp. 189–221, 2002.)

[91] F.H. Hinsley, A. Stripp. Code Breakers. Oxford University Press, NY, 1993.

[92] J.E. Hopcroft, J.D. Ullman. Introduction to Automata Theory, Languages, and Computa-

tion. Addison Wesley, London, 1979.

[93] R. Housley, W. Ford, W. Polk, D. Solo. Internet X.509 Public Key Infrastructure Certiﬁcate

and CRL Proﬁle. Internet Standard. RFC 2459, The Internet Society, 1999.

[94] T. Iwata, K. Kurosawa. OMAC: One-Key CBC MAC. In Fast Software Encryption’03,
Lund, Sweden, Lecture Notes in Computer Science 2887. Springer-Verlag, NY, 2003,
pp. 137–161.

[95] T. Jakobsen, L.R. Knudsen. The Interpolation Attack on Block Ciphers. In Fast Software
Encryption’97, Haifa, Israel, Lecture Notes in Computer Science 1267. Springer-Verlag,
NY, 1997, pp. 28–40.

[96] P.

Junod, S. Vaudenay. FOX Speciﬁcations Version 1.1. Technical Report

EPFL/IC/2004/75, EPFL, 2004.

[97] P. Junod, S. Vaudenay. FOX: A New Family of Block Ciphers. In Selected Areas in Cryp-
tography’04, Watterloo, ON, Canada, Lecture Notes in Computer Science 3357. Springer-
Verlag, NY, 2005, pp. 114–129.

[98] D. Kahn. The Codebreakers. Scriber, 1996.

[99] R.M. Karp. Reducibility among Combinatorial Problems. In Complexity of Computer

Computations, Plenum Press, New York, 1972. pp. 85–103.

322

Bibliography

[100] S. Katzenbeisser. Recent Advances in RSA Cryptography. Advances in Information Secu-

rity vol. 3. Kluwer Academic Publishers, Boston, 2001.

[101] J. Kelsey, B. Schneier, N. Ferguson. Yarrow-160: Notes on the Design and Analysis of the
Yarrow Cryptographic Pseudorandom Number Generator. In Selected Areas in Cryptogra-
phy’99, Kingston, ON, Canada, Lecture Notes in Computer Science 1758. Springer-Verlag,
NY, 2000, pp. 13–33.

[102] N. Koblitz. A Course in Number Theory and Cryptography. Graduate Texts in Mathematics

114. Springer-Verlag, NY, 1994.

[103] P. Kocher. Timing Attacks on Implementations of Difﬁe-Hellman, RSA, DSS, and Other
Systems. In Advances in Cryptology CRYPTO’96, Santa Barbara, CA, Lecture Notes in
Computer Science 1109. Springer-Verlag, NY, 1996, pp. 104–113.

[104] P. Kocher. Differential Power Analysis. In Advances in Cryptology CRYPTO’99, Santa Bar-
bara, CA, Lecture Notes in Computer Science 1666. Springer-Verlag, NY, 1999, pp. 388–
397.

[105] J. Kohl, C. Neuman. The Kerberos Network Authentication Service (V5). Internet Stan-

dard. RFC 1510, 1993.

[106] H. Krawczyk. How to Predict Congruential Generators.

In Advances in Cryptology
CRYPTO’89, Santa Barbara, CA, Lecture Notes in Computer Science 435. Springer-
Verlag, NY, 1990, pp. 138–153.

[107] H. Krawczyk. How to Predict Congruential Generators. Journal of Algorithms, vol. 13,

pp. 527–545, 1992.

[108] H. Krawczyk. LFSR-Based Hashing and Authentication.

In Advances in Cryptology
CRYPTO’94, Santa Barbara, CA, Lecture Notes in Computer Science 839. Springer-
Verlag, NY, 1994, pp. 129–139.

[109] H. Krawczyk, M. Bellare, R. Canetti. HMAC: Keyed-Hashing for Message Authentication.

RFC 2104, 1997.

[110] X. Lai. On the Design and Security of Block Ciphers. ETH Series in Information Process-

ing, vol. 1. Hartung-Gorre Verlag Konstanz, 1992.

[111] X. Lai, J.L. Massey, S. Murphy. Markov Ciphers and Differential Cryptanalysis. In Ad-
vances in Cryptology EUROCRYPT’91, Brighton, UK, Lecture Notes in Computer Science
547. Springer-Verlag, NY, 1991, pp. 17–38.

[112] G. Lam´e. Mentioned in J.O. Shallit, Historia Mathematica, vol. 21, pp. 401–419, 1994.

[113] L. Lamport. Constructing Digital Signatures from a One Way Function, Technical Report

CSL-98, SRI Intl., 1979.

[114] H.W. Lenstra. Factoring Integers with Elliptic Curves. Annals of Mathematics, vol. 126,

pp. 649–673, 1987.

Bibliography

323

[115] A.K. Lenstra, H.W. Lenstra. The Development of the Number Field Sieve. Springer-Verlag,

NY, 1993.

[116] S. Levy. Crypto. Penguin Books, NY, 2001.

[117] R. Lidl, H. Niederreiter. Introduction to Finite Fields and Their Applications. Cambridge

University Press, Cambridge, 1994.

[118] B. Lloyd, W. Simpson. PPP Authentication Protocols. Internet Standard. RFC 1334,

1992.

[119] M. Luby, C. Rackoff. How to Construct Pseudorandom Permutations from Pseudorandom

Functions. SIAM Journal on Computing, vol. 17, pp. 373–386, 1988.

[120] W. Mao. Modern Cryptography—Theory and Practice. Prentice Hall PTR, Upper Saddle

River, NJ, 2003.

[121] J.L. Massey. SAFER K-64: A Byte-Oriented Block-Ciphering Algorithm. In Fast Software
Encryption’94, Cambridge, UK, Lecture Notes in Computer Science 809. Springer-Verlag,
NY, 1994, 1–17.

[122] J.L. Massey. SAFER K-64: One Year Later. In Fast Software Encryption’95, Leuven,
Belgium, Lecture Notes in Computer Science 1008. Springer-Verlag, NY, 1995, pp. 212–
241.

[123] J.L. Massey. Guessing and Entropy. In IEEE International Symposium on Information

Theory, Tronheim, Norway, 1994, pp. 204.

[124] M. Matsui. Linear Cryptanalysis Methods for DES Cipher. In Advances in Cryptology
EUROCRYPT’93, Lofthus, Norway, Lecture Notes in Computer Science 765. Springer-
Verlag, NY, 1994, pp. 386–397.

[125] M. Matsui. The First Experimental Cryptanalysis of the Data Encryption Standard.

In
Advances in Cryptology CRYPTO’94, Santa Barbara, CA, Lecture Notes in Computer
Science 839. Springer-Verlag, NY, 1994, pp. 1–11.

[126] M. Matsui. New Structure of Block Ciphers with Provable Security against Differential
and Linear Cryptanalysis. In Fast Software Encryption’96, Cambridge, UK, Lecture Notes
in Computer Science 1039. Springer-Verlag, NY, 1996, pp. 205–218.

[127] M. Matsui. New Block Encryption Algorithm MISTY. In Fast Software Encryption’97,
Haifa, Israel, Lecture Notes in Computer Science 1267. Springer-Verlag, NY, 1997, pp. 54–
68.

[128] A.J. Menezes, P.C. van Oorschot, S.A. Vanston. Handbook of Applied Cryptography. CRC,

NY, 1997.

[129] R.C. Merkle. Secure Communications over Insecure Channels. Communications of the

ACM, vol. 21, pp. 294–299, 1978.

324

Bibliography

[130] R.C. Merkle. One Way Hash Functions and DES. In Advances in Cryptology CRYPTO’89,
Santa Barbara, CA, Lecture Notes in Computer Science 435. Springer-Verlag, NY, 1990,
pp. 416–427.

[131] R.C. Merkle. A Certiﬁed Digital Signature.

In Advances in Cryptology CRYPTO’89,
Santa Barbara, CA, Lecture Notes in Computer Science 435. Springer-Verlag, NY, 1990,
218–238.

[132] R.C. Merkle, M. Hellman. Hiding Information and Signatures in Trapdoor Knapsacks.

IEEE Transactions on Information Theory, vol. IT-24, pp. 525–530, 1978.

[133] R.C. Merkle, M. Hellman. On the Security of Multiple Encryption. Communications of

the ACM, vol. 24, pp. 465–467, 1981.

[134] G. Miller. Riemann’s Hypothesis and Tests for Primality. Journal of Computer and System

Sciences, vol. 13, pp. 300–317, 1976.

[135] J. Monnerat, S. Vaudenay. Undeniable Signatures Based on Characters: How to Sign with
One Bit. In Public Key Cryptography’04, Singapore, Lecture Notes in Computer Science
2947. Springer-Verlag, NY, 2004, pp. 69–85.

[136] J. Monnerat, S. Vaudenay. Generic Homomorphic Undeniable Signatures. In Advances in
Cryptology ASIACRYPT’04, Jeju Island, Korea, Lecture Notes in Computer Science 3329.
Springer-Verlag, NY, 2004, pp. 354–371.

[137] M. Naor, S. Shamir. Visual Cryptography. In Advances in Cryptology EUROCRYPT’94,
Perugia, Italy, Lecture Notes in Computer Science 950. Springer-Verlag, NY, 1995, pp. 1–
12.

[138] R.M. Needham, M.D. Schroeder. Using Encryption for Authentication in Large Networks

of Computers. Communications of the ACM, vol. 21, pp. 993–999, 1978.

[139] P.Q. Nguyen, J. Stern. The Two Faces of Lattices in Cryptology. In Proceedings of the
International Conference on Cryptography and Lattices (CaLC’01), Providence, Rhode
Island, Lecture Notes in Computer Science 2146. Springer-Verlag, NY, 2001, pp. 146–180.

[140] M.A. Nielsen, I.L. Chuang. Quantum Computation and Quantum Information. Cambridge

University Press, Cambridge, 2000.

[141] K.Nyberg, L.R. Knudsen. Provable Security against a Differential Cryptanalysis. Journal
of Cryptology, Journal version of a paper presented at CRYPTO’92, Santa Barbara, CA,
vol. 8, pp. 27–37, 1995.

[142] P. Oechslin. Making a Faster Cryptanalytic Time–Memory Trade-Off.

In Advances in
Cryptology CRYPTO’03, Santa Barbara, CA, Lecture Notes in Computer Science 2729.
Springer-Verlag, NY, 2003, pp. 617–630.

[143] T. Okamoto, S. Uchiyama. A New Public-Key Cryptosystem as Secure as Factoring. In
Advances in Cryptology EUROCRYPT’98, Espoo, Finland, Lecture Notes in Computer
Science 1403. Springer-Verlag, NY, 1998, pp. 308–318.

Bibliography

325

[144] H. Ong, C.P. Schnorr, A. Shamir. An Efﬁcient Signature Scheme Based on Quadratic Equa-
tions. In Proceedings of the 16th ACM Symposium on Theory of Computing, Washington,
D.C. ACM Press, NY, 1984, pp. 208–216.

[145] S. Pohlig, M. Hellman. An Improved Algorithm for Computing Logarithms over GF(q)
and Its Cryptographic Signiﬁcance. IEEE Transactions on Information Theory, vol. IT-24,
pp. 106–110, 1978.

[146] D. Pointcheval, J. Stern. Security Proofs for Signature Schemes. In Advances in Cryptology
EUROCRYPT’96, Zaragoza, Spain, Lecture Notes in Computer Science 1070. Springer-
Verlag, NY, 1996, pp. 387–398.

[147] D. Pointcheval, S. Vaudenay. On Provable Security for Digital Signature Algorithms. Tech-

nical Report LIENS 96-17, Ecole Normale Sup´erieure, 1996.

[148] J.M. Pollard. Theorems on Factorization and Primality Testing. Mathematical Proceedings

of the Cambridge Philosophical Society, vol. 76, pp. 521–528, 1974.

[149] J.M. Pollard. A Monte Carlo Method for Factorization. Nordisk Tidskrift for Informations-

behandlung (BIT), vol. 15, pp. 331–334, 1975.

[150] J.M. Pollard. Monte Carlo Methods for Index Computation mod p. Mathematics of Com-

putation, vol. 32, pp. 918–924, 1978.

[151] J.M. Pollard, C.P. Schnorr. An Efﬁcient Solution of the Congruence x 2 + ky2 = m

(mod n). IEEE Transactions on Information Theory, vol. IT-33, pp. 702–709, 1987.

[152] C. Pomerance. Fast, Rigorous Factorization and Discrete Logarithm Algorithms. In Dis-
crete Algorithms and Complexity, D.S. Johnson, T. Nishizeki, A. Nozaki, and H.S. Wilf
(Eds.). Academic Press, NY, 1987, pp. 119–143.

[153] M.O. Rabin. Digitalized Signatures and Public-Key Functions as Intractable as Factor-
ization. Technical Report MIT/LCS/TR–212, MIT Laboratory for Computer Science,
1979.

[154] M.O. Rabin. Probabilistic Algorithm for Testing Primality. Journal of Number Theory,

vol. 12, pp. 128–138, 1980.

[155] A. R´enyi. Probability Theory. Elsevier, NY, 1970.

[156] R.L. Rivest. The MD4 Message Digest Algorithm.

In Advances in Cryptology
CRYPTO’90, Santa Barbara, CA, Lecture Notes in Computer Science 537. Springer-
Verlag, NY, 1991, pp. 303–311.

[157] R.L. Rivest. The MD5 Message Digest Algorithm. RFC 1321, 1992.

[158] R.L. Rivest, A. Shamir, L.M. Adleman. A Method for Obtaining Digital Signatures and

Public-Key Cryptosystem. Communications of the ACM, vol. 21, pp. 120–126, 1978.

326

Bibliography

[159] B. Schneier. Applied Cryptography. John Wiley & Sons, NY, 1996. French version: Cryp-

tographie Appliqu´ee. Vuibert, Paris, 1996.

[160] C.P. Schnorr. Efﬁcient Identiﬁcation and Signature for Smart Cards.

In Advances in
Cryptology CRYPTO’89, Santa Barbara, CA, Lecture Notes in Computer Science 435.
Springer-Verlag, NY, 1990, 235–251.

[161] C.P. Schnorr. Efﬁcient Identiﬁcation and Signature for Smart Cards. Journal of Cryptology,

vol. 4, pp. 161–174, 1991.

[162] C.P. Schnorr, S. Vaudenay. Black Box Cryptanalysis of Hash Networks Based on Multi-
permutations. In Advances in Cryptology EUROCRYPT’94, Perugia, Italy, Lecture Notes
in Computer Science 950. Springer-Verlag, NY, 1995, pp. 47–57.

[163] A. Shamir. How to Share a Secret. Communications of the ACM, vol. 22, pp. 612–613,

1979.

[164] A. Shamir. A Polynomial Time Algorithm for Breaking the Basic Merkle–Hellman Cryp-
tosystem. In Proceedings of the 23rd IEEE Symposium on Foundations of Computer Sci-
ence, Chicago, IL. IEEE, 1982, pp. 145–152.

[165] A. Shamir. On the Security of DES.

In Advances in Cryptology CRYPTO’85, Santa
Barbara, CA, Lecture Notes in Computer Science 218. Springer-Verlag, NY, 1986, pp. 280–
281.

[166] A. Shamir. IP=PSPACE. In Proceedings of the 22nd ACM Symposium on Theory of Com-

puting, Baltimore, MD. ACM Press, NY, 1990, pp. 11–15.

[167] C.E. Shannon. Communication Theory of Secrecy Systems. Bell System Technical Journal,
vol. 28, pp. 656–715, 1969. Re-edited in Claude Elwood Shannon—Collected Papers. IEEE
Press, New York, 1993.

[168] R. Shirey. Internet Security Glossary. RFC 2828, The Internet Society, 2000.

[169] V. Shoup. OAEP Reconsidered. In Advances in Cryptology CRYPTO’01, Santa Barbara,
CA, Lecture Notes in Computer Science 2139. Springer-Verlag, NY, 2001, pp. 239–259.

[170] V. Shoup. A Computational Introduction to Number Theory and Algebra. Online textbook,

2004. Available at http://shoup.net/ntb.

[171] J.H. Silverman. The Arithmetic of Elliptic Curves. Graduate Texts in Mathematics 106.

Springer-Verlag, NY, 1986.

[172] G.J. Simmons. A “Weak” Privacy Protocol Using the RSA Crypto Algorithm. Cryptologia,

vol. 7, pp. 180–182, 1983.

[173] S. Singh. The Code Book. Fourth Estate, London, 1999.

[174] R. Solovay, V. Strassen. A Fast Monte-Carlo Test for Primality. SIAM Journal on Comput-

ing, vol. 6, pp. 84–86, 1977.

Bibliography

327

[175] J. Stern. La Science du Secret. Odile Jacob, Paris, 1998.

[176] J. Stern, S. Vaudenay. CS-Cipher. In Fast Software Encryption’98, Paris, France, Lecture

Notes in Computer Science 1372. Springer-Verlag, NY, 1998, pp. 189–205.

[177] D.R. Stinson. Cryptography, Theory and Practice (2nd Edition). CRC, NY, 2002. French

version: Cryptographie, Th´eorie et Pratique, Vuibert, Paris, 2003.

[178] A. Tardy-Corfdir, H. Gilbert. A Known Plaintext Attack of FEAL-4 and FEAL-6.

In
Advances in Cryptology CRYPTO’91, Santa Barbara, CA, Lecture Notes in Computer
Science 576. Springer-Verlag, NY, 1992, pp. 172–181.

[179] S. Vaudenay. On the Need for Multipermutations: Cryptanalysis of MD4 and SAFER. In
Fast Software Encryption’95, Leuven, Belgium, Lecture Notes in Computer Science 1008.
Springer-Verlag, NY, 1995, pp. 286–297.

[180] S. Vaudenay. Hidden Collisions on DSS. In Advances in Cryptology CRYPTO’96, Santa
Barbara, CA, Lecture Notes in Computer Science 1109. Springer-Verlag, NY, 1996, pp.
83–88.

[181] S. Vaudenay. On the Security of CS-Cipher. In Fast Software Encryption’99, Roma, Italy,

Lecture Notes in Computer Science 1636. Springer-Verlag, NY, 1999, 260–274.

[182] S. Vaudenay. Security Flaws Induced by CBC Padding—Applications to SSL, IPSEC,
WTLS. In Advances in Cryptology EUROCRYPT’02, Amsterdam, Netherlands, Lecture
Notes in Computer Science 2332. Springer-Verlag, NY, 2002, pp. 534–545.

[183] S. Vaudenay. Decorrelation: A Theory for Block Cipher Security. Journal of Cryptology,

vol. 16, pp. 249–286, 2003.

[184] M.N. Wegman, J.L. Carter. New Hash Functions and Their Use in Authentication and Set

Equality. Journal of Computer and System Sciences, vol. 22, pp. 265–279, 1981.

[185] M.J. Wiener. Cryptanalysis of Short RSA Secret Exponents. IEEE Transactions on Infor-

mation Theory, vol. IT-36, pp. 553–558, 1990.

Index

3DES, see triple DES, 31

baby step–giant step algorithm, see discrete

A3, see MAC, 149
A5/1, see stream cipher, 48
A8, see pseudorandom generator, 149
ability frontiers, 220–221
access structure, see secret sharing, 282
Adleman, Leonard, 236
Advanced Encryption Standard, see AES, 42
AES, see block cipher, 42
alphabet, 215, 216, 218, 220
analysis (cryptographic), see cryptanalysis, 3
anonymity, 11, 148, 292, 295
ASCII armor, 308
associativity, 32, 33, 156, 157, 159, 160, 175, 215
asymmetric cryptography, see public-key

cryptography, 229

attack

brute force, 51–61, 70–74, 83, 97
chosen ciphertext, 12, 239, 244–246, 251
chosen message, 79, 87, 256
chosen plaintext, 12, 53, 55, 64, 97, 102, 103,

115, 124, 246
ciphertext-only, 12
codebook, 54
dictionary, 53, 62, 303
generic, 51–59, 70–74
known message, 79
known plaintext, 12, 20, 60, 64, 92, 103, 111,

120, 178

man-in-the-middle, 233, 306
meet-in-the-middle, 30, 59–60, 95
preimage, 64, 65, 70, 74
second preimage, 64, 70, 74, 124

authenticated mode of operation, 90–91

CCM, 90–91

authentication, 10, 15, 63, 79, 90, 135, 143, 148,

150, 151, 153, 231, 232, 295, 296, 299–301,
310

authentication tree, 145–148, 253
automaton

deterministic, 217
ﬁnite, 47, 48, 50, 88, 216–220, 226, 304
nondeterministic, 217, 220

logarithm, 204

basic access control in HTTP, see password access

control, 136

Baudot code, 7
BEAR, see block cipher, 32
Bellare, Mihir, 89, 246
Bezout relationship, 164, 166, 168
Biham, Eli, 97, 102
birthday paradox, 70–74, 76, 80, 95, 124, 191
Bleichenbacher, Daniel, 243, 262, 263, 270
blind signature, 292
block cipher, 22, 46

AES, 42–46, 75, 90, 299, 302, 303
BEAR, 32
BLOWFISH, 32, 53
CS-CIPHER, 40–42, 75
DES, 22–25, 303
FOX, 34, 37–40
IDEA, 33, 61, 300
KASUMI, 123
LION, 32
LUCIFER, 22
MARS, 32
MISTY, 123
PURE, 122
RC2, 303
Rijndael, 43
SAFER+, 150
SAFER K-64, 36, 150
triple DES, 31, 299, 303

BLOWFISH, see block cipher, 32
Bluetooth, 50–51, 150–153, 295
bombes, 10
Boyar, Joan, 92
broadcast encryption, 241, 293
brute force attack, see attack, 51

Caesar, Julius, 4, 5, 36, 178
Canetti, Ran, 89
Carmichael function, 201, 226
Carmichael number, 182–184, 187, 189
Cartesian product, 156
CBC, see mode of operation, 25
CBC-MAC, see MAC, 80

330

Index

CCM, see authenticated mode of operation, 90
certiﬁcate, 267, 280, 293, 296
CFB, see mode of operation, 25
Chabaud, Florent, 67
challenge, 267, 278, 281
challenge-response protocol, 137–140

CHAP, 140
HTTP digest, 138–139

CHAP, see challenge-response protocol, 140
characteristic, see ﬁeld, 172
Chaum, David, 288
chinese remainder theorem, 167–169, 182, 183,

191, 201, 202, 206, 209, 237, 241, 242, 261,
289

chosen ciphertext attack, see attack, 12
chosen message attack, see attack, 79
chosen plaintext attack, see attack, 12
Church hypothesis, 220
cipher, 3
cipher block chaining, see CBC, 26
cipher feedback, see CFB, 29
ciphertext, 3
ciphertext-only attack, see attack, 12
cleartext, 3
cliper chip, 13
closure, 157, 160, 216
code, 2
codebook attack, see attack, 54
codeword, 2, 146
coding theory, 1, 2, 15
collision, 26, 63, 64, 66, 67, 70–72, 74, 76–78, 81,

83–86, 90, 95, 124

collision-free, 64
collision-resistant, 64, 66, 89, 146, 256
commitment, 64, 65, 267, 269, 278–282, 289–291
commutativity, 32, 33, 157, 160, 175
COMP128, 149
completeness, 278, 279, 281, 289–291
complexity, 14, 21, 52, 54, 62, 124, 215, 224, 229

asymptotic, 222–223
classes, 223–224
polynomial, 222
reduction, 222

compression function, 65–70
computability, 218, 220–221, 223
concatenation, 25–27, 29, 30, 80, 91, 151, 215,

216, 263, 307

conﬁdentiality, 2, 10, 15, 26, 31, 63, 135, 137, 139,

148, 150, 152, 153, 231, 232, 295

conventional cryptography, 16, 63, 74, 75, 97, 123,
124, 135, 142, 145, 150, 155, 231, 232, 253

Cook, Stephen, 224, 225
Coppersmith, Don, 102, 241
Coron, Jean-S´ebastien, 259
counter mode, see mode of operation, 30

CRC, 63
cryptanalysis, 3, 10

differential, 97–103, 111–120, 122, 130
interpolation, 122
linear, 103–111, 114–120, 122, 130

crypto, see, fun, xv
cryptoanalysis, see cryptanalysis, 3
cryptogram, 3, 12, 19
cryptographic analysis, see cryptanalysis, 3
cryptographic system, 3
cryptography, 21, 1–21
cryptology, 3
cryptosystem, 3, 11, 12, 200, 229–231, 234, 235,

253–255, 296

ElGamal, 248–250, 310
Merkle–Hellman, 235–236
Okamoto–Uchiyama, 179
Rabin, 251, 257
RSA, 236–248, 250, 296, 299, 305,

308–310

CS-CIPHER, see block cipher, 40
CTR, see mode of operation, 30
cyclical redundancy check, see CRC, 63

Daemen, Joan, 43
Damg˚ard, Ivan, 65–67
Data Encryption Standard, see DES, 22
Davies-Meyer scheme, 66, 67, 95
decidability, 221–222
decipherment, 3
decisional problem, 221, 234, 235
decode, 2
decorrelation, 126–131, 133
decryption, 3
DES, see block cipher, 22
designated conﬁrmer, 292
dictionary attack, see attack, 53
Difﬁe–Hellman

key agreement protocol, 234, 231–

306

problem, 234, 249

Difﬁe, Whitﬁeld, 13, 229, 231, 234
digest access control in HTTP, see

challenge–response protocol, 138

digital cash, 292
digital signature scheme, 253

DSA, 264–266, 299
ECDSA, 264–266
ElGamal, 260–262, 274, 275
Merkle, 145–146
MOVA, 289–291
Ong–Schnorr–Shamir, 275
Pointcheval–Vaudenay, 266, 270–274
RSA, 255–260, 305, 308
Schnorr, 263, 266–270, 274, 275

Index

digital signature standard, see DSA, 264
digram, 4, 19
discrete logarithm, 203–213

baby step–giant step, 204–205
factor base, 210–211
index calculus, 210–211
Pohlig–Hellman, 205–209, 233, 262
Pollard rho, 204
problem, 203

DLKOFP, 203–205
DLKOP, 203, 204, 210, 249
DLP, 203, 204, 234, 249

distinguisher, 114, 122–125

adaptive, 124, 129, 130
differential, 115
linear, 116, 119
nonadaptive, 124

distributivity, 160
Dobbertin, Hans, 77
double mode, 30, 59
DSA, see digital signature scheme, 264
DSS, see DSA, 264

E0, see stream cipher, 50
E1, see MAC, 150
ECB, see mode of operation, 25
ECDSA, see digital signature scheme, 264
Echelon, 13
ECM, see factorization, 194
electronic code book, see ECB, 25
ElGamal

decryption problem, 248, 249
encryption, see cryptosystem, 248
key recovery problem, 249
signature, see digital signature scheme, 260

ElGamal, Taher, 248, 260
elliptic curve, 173–178, 194, 213, 250, 264,

275

anomalous, 178
characteristic 2, 176–177
characteristic p > 3, 173–176
discriminant, 174, 177, 178
Hasse Theorem, 177
j-invariant, 174, 175, 177
method, see factorization, 194
singular, 178
supersingular, 177, 178
trace of Frobenius, 177, 178

EMAC, see MAC, 80
encipherment, 3
encode, 2
encryption, 3
Enigma, 8–11, 15
entropy, 17–18, 21, 62, 94, 284, 285, 310
ETSI, 48, 148

331

Euclid algorithm, 164, 166, 168, 184, 196, 238
Euclidean division, 158, 165, 178
Euler Totient Function, 166
exhaustive search, 10, 12, 32, 52–54, 59, 102, 124,

145, 152, 303

existential forgery, 254, 256, 270, 288
extractor, 267, 278, 279, 282

factor base discrete logarithm, see discrete

logarithm, 197

factor base factorization, see factorization, 197
factorization, 190–200

elliptic curves method (ECM), 194–196, 198
factor base, 197
Fermat, 196–197, 240
number ﬁeld sieve, 199
Pollard p − 1, 192–194
Pollard rho, 190–192, 204
quadratic sieve, 197–199, 213
quantum, 200–201
trial division, 181, 190

fail-stop signature, 292
Feige–Fiat-Shamir protocol, 280–282
Feistel, Horst, 22
Feistel scheme, 22–23, 32–34, 36, 42, 67, 69, 121,

122, 125, 133, 134, 247

Ferguson, Niels, 94
Fermat factorization algorithm, see factorization,

196

Fiat, Amos, 278
Fiat–Shamir protocol, 269, 278–280, 293
ﬁeld, 160, 166

characteristic, 172, 178
ﬁnite, 95, 122, 172–173, 175–177
Galois, 172
GF(2n), 39, 45, 82, 95, 122, 172, 176, 177,

265

Z p, 169–172, 189, 265

ﬁngerprint, 63, 298
forward secrecy, 233
FOX, see block cipher, 37
Franklin, Matthew, 103
fun, see cryptoxv, see crypto, 1
function, 155

gcd, 164–166, 169, 191–193
generator, 158, 170, 179, 248, 261
generic attack, see attack, 51
Gilbert, Henri, 103
Goldwasser, Shaﬁ, 244, 254, 277, 278
greatest common divisor, see gcd, 164
group, 157

Abelian, 157, 159–161, 175, 177
cyclic, 170, 201, 203
exponent, 183, 201–202

332

group (cont.)

isomorphism, 158–160, 169, 175, 179
law, 32–34, 38, 157, 158, 232
morphism, 158
order, 160, 166, 183, 194, 201–203
product, 159
quotient, 159
symmetric, 158
theory, 155–160
trivial, 157
Zn, 158

group signature, 291
GSM, 48, 49, 148–150, 152
Guillou, Louis, 269, 293

Haber, Stuart, 147
halting problem, 221
hash function, 63–71, 74, 78, 79, 86, 88, 135, 138,
145, 148, 150, 246, 248, 256, 260, 270, 299,
302, 309

MD4, 32, 66, 74–78, 95, 141
MD5, 65–67, 71, 74, 95, 138, 139, 141, 260,

300, 307, 308

SHA, 67–70
SHA-1, 67–70, 89, 141, 264, 265, 300

H˚astad, Johan, 241
Hellman, Martin, 55, 60, 229, 231, 234, 235
Heys, Howard, 97
Hill Cipher, 20, 178
HMAC, see MAC, 88
HTTP, 136, 139, 300

IDEA, see block cipher, 33
ideal, 161, 172
image, 155
index calculus, see discrete logarithm, 210
index of coincidence, 6, 19
indistinguishability, 89, 123–125, 245, 246, 269
integrity, 10, 15, 26, 27, 63, 65, 79, 90, 135, 139,

141, 150, 153, 232, 253, 295

interactive proof, 266–270, 277–279, 281, 288
intractability, 14, 64, 181, 224–226, 229, 230
invertibility, 157, 160, 166, 169, 170
invisible signature, 288, 292
involution, 4, 8, 10, 41, 61

Jacobi symbol, 184, 186, 244, 280, 281, 290
Joux, Antoine, 67
Junod, Pascal, 37

Karp, Richard, 224, 226
Karp Theorem, 235
Kasiski, Friedrich, 19
Kasiski Test, 6, 19
KASUMI, see block cipher, 123

Index

Kelsey, John, 94
Kerberos, 143–144, 153
Kerckhoffs, Auguste, 12
Kerckhoffs Principle, 11
key agreement protocol, 142, 231, 233, 295, 296,

299–302, 305, 306
key distribution, 142–145
key exchange protocol, 231, 305
key schedule, 23–25, 37–39, 42, 43, 53, 70
Kleene Theorem, 218
knapsack, 235–236
known message attack, see attack, 79
known plaintext attack, see attack, 12
Knudsen, Lars, 43, 121, 122, 133
Kocher, Paul, 242
Krawczyk, Hugo, 87–89, 92

Lagrange Theorem, 160, 166, 169
Lai–Massey scheme, 33–38
Lai, Xuejia, 33, 66, 112
Lamport, Leslie, 140
Lamport scheme, 140–141, 229, 253
Landau, Susan, 13
language, 215
empty, 216
null, 216
recursively enumerable, 219
regular, 216
universal, 221
Latin square, 20
Legendre symbol, 184, 186, 290
Lenstra, Arjen, 199
Lenstra, Hendrik, 194, 199
LFSR, 39, 48, 50, 88
linear feedback shift register, see LFSR, 39
LION, see block cipher, 32
Little Fermat Theorem, 181
Luby, Michael, 125
Luby–Rackoff result, 125–126
LUCIFER, see block cipher, 22

MAC, 78–91

A3, 148–149
CBC-MAC, 80–86, 90–91
E1, 150
EMAC, 80, 82
HMAC, 88–90, 300, 304
OMAC, 81–82
RMAC, 81
TMAC, 81
XCBC, 81

malleability, 26, 245–246, 256
man-in-the-middle attack, see attack, 233
manipulation detection codes, see MDC, 64
Maple, 155

Index

333

Markov chain, 112, 113
Markov cipher, 133
Markov ciphers, 112–114
MARS, see block cipher, 32
Massey, James, 33, 36, 112
Matsui, Mitsuru, 103, 123
MD4, see hash function, 141
MD5, see hash function, 66
MDC, 64
MediaCrypt, 33, 37
meet-in-the-middle attack, see attack, 30
mental poker, 292
Merkle-Damg˚ard scheme, 65–67
Merkle–Hellman cryptosystem, see cryptosystem,

235

Merkle puzzles, 145
Merkle, Ralph, 60, 65–67, 145, 146, 229, 235, 253
Merkle tree, 145–146
message authentication codes, see MAC, 78
message digest, 63, 70, 135, 260, 262, 263
message recovery, 255, 256, 258
Micali, Silvio, 244, 254, 277, 278
MISTY, see block cipher, 123
mode of operation

authenticated, see authenticated mode of

operation, 90
CBC, 25–27, 302
CCM, see authenticated mode of operation, 90
CFB, 25, 29
CTR, 30, 49, 93
ECB, 25–26
OFB, 25, 27–28, 93

Monnerat, Jean, 289
Moore Law, 12
MOVA, see digital signature scheme, 289
multigram, 19
multiparty computation, 293
multipermutation, 45, 75, 95
Murphy Law, 12
Murphy, Sean, 112

Naccache, David, 259
Naor, Moni, 7
Needham–Schroeder authentication protocol, 143,

144

Needham–Schroeder authentication protocol, 142
Netscape, 300
neutral element, 157, 158, 160, 175, 177, 215
NIST, 22, 264, 265
nonce, 30, 90, 91, 142, 301, 303, 305
non-repudiation, 11, 254
NP-complete, 14, 224–227, 229, 235, 236
NP-hard, 224
NSA, 7, 13, 22
number ﬁeld sieve, see factorization, 199

Nyberg, Kaisa, 121, 122, 133
Nyberg–Knudsen Theorem, 121, 123

OAEP, 246–248, 260
Oechslin, Philippe, 58
OFB, see mode of operation, 25
Okamoto–Uchiyama cryptosystem, see

cryptosystem, 179

OMAC, see MAC, 81
one-time pad, see stream cipher, 7
one-time passwords, 140–142
one-way function, 14, 32, 64, 136, 140, 141, 145,

146, 229, 246, 266

Ong–Schnorr–Shamir signature, see digital

signature scheme, 275

opposite, 157, 158, 175
oracle, 52, 62, 82–84, 114–116, 123, 124, 129, 130,
222, 225, 226, 230, 239, 243, 245, 249, 254,
270, 311, 312

orthomorphism, 34, 38
output feedback, see OFB, 27

pair, 156
PAP, see password access control, 137
partition, 156
password access control, 135–142

HTTP basic, 136–137
PAP, 137, 140

PGP, 14, 307–311
piling-up Lemma, 105, 106
PKCS, 240, 243, 246–248, 260, 305
plaintext, 3
Pohlig–Hellman algorithm, see discrete logarithm,

205

Pointcheval, David, 266, 270
Pointcheval–Vaudenay signature, see digital

signature scheme, 266

point to point protocol, see PPP, 137
Pollard rho discrete logarithm, see discrete

logarithm, 190

Pollard rho factorization, see factorization, 190
polynomially equivalent, 224, 251
Pomerance, Carl, 198
PPP, 137

authentication protocol PAP, see password access

control, 137

challenge-handshake authentication protocol
CHAP, see challenge-response protocol,
140

preimage, 155
preimage attack, see attack, 64
Pretty Good Privacy, see PGP, 307
primality, 181–190

Fermat test, 181–182
Miller-Rabin test, 187–189

334

primality (cont.)

Solovay–Strassen test, 184–187
trial division, 181

prime number

generation, 189–190
strong, 211
theorem, 189

principal, 161, 172
privacy, 1, 2, 13, 14, 148, 149, 292, 307,

308

problem reduction, 222, 238, 243
product cipher, 19
prover, 277
pseudorandom generator, 92–94

A5, 150
A8, 148–149
congruential, 92–93
CTR, 93
OFB, 28, 93
Yarrow, 94

PSS, 260
public key, 145, 230, 234
public-key cryptography, 14, 145, 155, 181,

229–231, 253

Public Key Cryptography Standards, see PKCS,

260

PURE, see block cipher, 122
puzzle, 145

quadratic residue, 170–172, 175, 176, 184, 187,

189, 198, 201, 239, 251, 262, 279, 280, 289,
290

quadratic sieve, see factorization, 197
quantum computing, 200–201
Quisquater, Jean-Jacques, 269, 293

Rabin cryptosystem, see cryptosystem,

251

Rackoff, Charles, 125, 277, 278
randomness, 63, 92, 94, 114, 125, 310
RC2, see block cipher, 303
RC4, see stream cipher, 46
red telephone, 7, 13, 21
regular expression, 215–216
regular operation, 216
relation, 156

equivalence, 156
reﬂexive, 156
symmetric, 156
transitive, 156

remainder, 158
response, 267, 278, 281
right pair, 101
Rijmen, Vincent, 43
Rijndael, see block cipher, 43

Index

ring, 160

multiplicative group, 160
Zn, 169

ring signature, 291
Rivest, Ronald, 46, 66, 236, 254
RMAC, see MAC, 81
Rogaway, Phillip, 246
RSA

cryptosystem, see cryptosystem, 236
problem

RSADP, 238
RSAEMP, 238
RSAFP, 238
RSAKRP, 238
RSAOP, 238

signature, see digital signature scheme, 255

RSA155, 200
SAFER+, see block cipher, 150
SAFER K-64, see block cipher, 36
SAGE group, 48
SAT, 225
S-box, 24, 32, 36, 39, 44, 45, 61, 98, 100–106
Schneier, Bruce, 94
Schnorr, Claus, 75, 263, 267, 275
Schnorr signature, see digital signature scheme, 263
scytale, 4
second preimage attack, see attack, 64
secrecy, 2, 7, 11, 15–21, 133, 178, 215, 285
secret key, 230
secret sharing, 282–287

access structure, 282, 285–286
Benaloh-Leichter, 286–287
perfect, 284–285
Shamir threshold scheme, 283–284

secure hash algorithm, see SHA, 67
secure shell, see SSH, 297
Secure Socket Layer, see SSL, 300
semantic security, 244–246
set, 155
set theory, 155–157
SHA, see hash function, 67
SHA-1, see hash function, 67
shadow function, 257
Shamir, Adi, 7, 97, 102, 103, 236, 277, 278, 283
Shannon, Claude, 1, 2, 7, 16–18, 21, 36, 215, 230,

284
share, 282
Shor algorithm, 201
signature forgery, 253, 255, 256, 261, 270, 272,

288, 292

simple substitution, 4
simulator, 271, 272, 278, 280, 282
S/Key, 141
smooth, 193, 194, 197, 198, 204, 206, 213, 233, 262

Index

socket, 300
soundness, 278, 282, 289–291
SSH, 297–300
SSL, 47, 137, 300–307, 312
cipher suite, 300, 302–303
handshake protocol, 301–302
master key exchange, 305–306
master secret, 307
record protocol, 304

state, 216
steganography, 3
Stern, Jacques, 40, 270, 314
Stern, Julien, 259
Stochastic equivalence, 113
stream cipher, 46–50

A5/1, 48–50, 148–150
E0, 50–51, 150
one-time pad, 7, 13
RC4, 46–48, 299, 302, 304
Vernam Cipher, 7, 19

subgroup, 159
subset sum decision problem, 235
substitution, 4
substitution box, see S-box, 24
substitution-permutation networks, 36–46

tape, 218
TCP/IP, 300
ticket, 142, 143
time-memory tradeoff, 54–59
timestamp, 147–148
TLS, see SSL, 300
TMAC, see MAC, 81
Tonelli algorithm, 171, 178, 289
trace, 176
transition function, 216, 218, 225
transposition, 4, 24, 36
trapdoor permutation, 229, 235
trial division, see factorization, 181

335

trigram, 4
triple DES, see block cipher, 31
triple mode, 30–31
trusted authority, 267, 296
trust path, 310
Turing

equivalent, 226
machine, 10, 123, 218–220, 222, 223, 277
reduction, 225
test, 114

Turing, Alan, 10, 229

UMTS, 123
undeniable signature, 288–291
universal hashing, 86
universal veriﬁability, 288
UNIX passwords, 31–32, 136

van Antwerpen, Hans, 288
Vaudenay, Serge, xviii, 4
veriﬁer, 277
Vernam Cipher, see stream cipher, 7
Vernam, Gilbert, 7
Vigen`ere, Blaise de, 5
Vigen`ere Cipher, 5, 20
visual cryptography, 7–8
vote (electronic), 11, 14, 292

web of trust, 310
Wegman–Carter theorem, 87
Wiener, Michael, 241
word, 215
wrong pair, 101

XCBC, see MAC, 81

zero-knowledge, 11, 269, 277–282, 289–291, 293,

294

Zimmermann, Phil, 308

