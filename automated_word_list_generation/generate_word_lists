#!/usr/bin/env python

import sys
import fileinput
import json
import string

from nltk        import word_tokenize
from collections import Counter


def get_word_lists(file_name):
    """
    Given a sequence of words from stdin, return a dictionary
    with words as keys and word frequencies as values
    """
    with open(file_name, 'r') as f:
        text = f.read()
        # Remove all punctuation from text
        text = text.translate(None, string.punctuation)
        words = word_tokenize(text)
        # Set comprehension ignores duplicates: 
        word_set = {word.lower() for word in words}
    return word_set


def remove_stop_words(word_set, stop_words_file = "../stop-words.txt"):
    """
    Removes all of the stop words from the set of words.
    """
    with open(stop_words_file, 'r') as f:
        stop_words = word_tokenize(f.read())
        for word in stop_words:
            try:
                word_set.remove(stop_word)
            except KeyError:
                pass
        return word_set

def main(): 
    for filename in sys.argv[1:]: 
        print "Generating word frequency list..."
        wordList = get_word_lists(filename)
        print "Removing stop words..."
        wordList = remove_stop_words(word_list)
        word_list_file_name = 'word-list-' + text_file[:-4] + '.txt'
        with open(word_list_file_name, 'wb') as word_list_file:
            wordListFile.write('\n'.join(word_list))
    
if __name__ == "__main__":
    main()
